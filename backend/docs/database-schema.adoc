=  ğŸ“Š ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆ
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 4
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:version: 1.0.0
:author: KanshiChan Development Team
:email: team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

== ğŸ“– æ¦‚è¦

ç›£è¦–ã¡ã‚ƒã‚“ï¼ˆKanshiChanï¼‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆè©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚
Redisä¸­å¿ƒã®ãƒ‡ãƒ¼ã‚¿ç®¡ç†æˆ¦ç•¥ã€ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆã€ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¾ã™ã€‚

[NOTE]
====
ğŸ“‹ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±**

* **å¯¾è±¡èª­è€…**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆè€…ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…ã€ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
* **å‰æçŸ¥è­˜**: Redisã€NoSQLã€ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆåŸºç¤
* **ãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆ¦ç•¥**: Redisä¸­å¿ƒã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆæˆ¦ç•¥
* **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™**: æ¤œå‡ºçµæœä¿å­˜ < 1msã€åˆ†æã‚¯ã‚¨ãƒª < 10ms
* **æœ€çµ‚æ›´æ–°**: {docdate}

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<system-design>>, <<data-management>>, <<performance-optimization>>
====

== ğŸ¯ ãƒ‡ãƒ¼ã‚¿ç®¡ç†æˆ¦ç•¥

=== ğŸ“Š å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph TB
    subgraph "ğŸŒ Application Layer"
        APP1[KanshiChan App Instance 1]
        APP2[KanshiChan App Instance 2]
        APP3[KanshiChan App Instance N]
        WORKER[Background Workers]
    end
    
    subgraph "ğŸ’¾ Primary Data Storage"
        REDIS_PRIMARY[Redis Primary<br/>Main Data Store]
        REDIS_REPLICA[Redis Replica<br/>Read Scaling]
    end
    
    subgraph "ğŸ“ Persistent Storage"
        FILE_SYSTEM[File System<br/>Model Files/Logs]
        BACKUP_STORAGE[Backup Storage<br/>S3/GCS Compatible]
    end
    
    subgraph "ğŸ“Š Data Types"
        REALTIME[Real-time Data<br/>Session, Cache, Metrics]
        ANALYTICAL[Analytical Data<br/>Detection Logs, Behavior]
        CONFIG[Configuration Data<br/>Settings, Schedules]
        STATIC[Static Data<br/>Sound Files, Models]
    end
    
    subgraph "ğŸ”„ Data Flow"
        INGESTION[Data Ingestion]
        PROCESSING[Real-time Processing]
        STORAGE[Storage Distribution]
        RETRIEVAL[Data Retrieval]
        ANALYTICS[Analytics Processing]
    end
    
    %% Application to Storage
    APP1 --> REDIS_PRIMARY
    APP2 --> REDIS_PRIMARY
    APP3 --> REDIS_PRIMARY
    WORKER --> REDIS_PRIMARY
    
    APP1 --> REDIS_REPLICA
    APP2 --> REDIS_REPLICA
    APP3 --> REDIS_REPLICA
    
    %% Storage relationships
    REDIS_PRIMARY --> REDIS_REPLICA
    REDIS_PRIMARY --> BACKUP_STORAGE
    FILE_SYSTEM --> BACKUP_STORAGE
    
    %% Data type mapping
    REALTIME --> REDIS_PRIMARY
    ANALYTICAL --> REDIS_PRIMARY
    CONFIG --> REDIS_PRIMARY
    STATIC --> FILE_SYSTEM
    
    %% Data flow
    INGESTION --> PROCESSING
    PROCESSING --> STORAGE
    STORAGE --> RETRIEVAL
    STORAGE --> ANALYTICS
    
    classDef app fill:#e3f2fd
    classDef storage fill:#e8f5e8
    classDef persistent fill:#fff3e0
    classDef datatype fill:#f3e5f5
    classDef flow fill:#fce4ec
    
    class APP1,APP2,APP3,WORKER app
    class REDIS_PRIMARY,REDIS_REPLICA storage
    class FILE_SYSTEM,BACKUP_STORAGE persistent
    class REALTIME,ANALYTICAL,CONFIG,STATIC datatype
    class INGESTION,PROCESSING,STORAGE,RETRIEVAL,ANALYTICS flow
....

=== ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿åˆ†é¡ã¨ä¿å­˜æˆ¦ç•¥

[cols="2,2,2,2,2", options="header"]
|===
|ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— |ç‰¹æ€§ |ä¿å­˜å…ˆ |TTL |ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
|**ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿** |é«˜é »åº¦èª­ã¿æ›¸ã |Redis Primary |24æ™‚é–“ |Redis Replica
|**æ¤œå‡ºçµæœ** |å¤§é‡ãƒ‡ãƒ¼ã‚¿ |Redis Primary |7æ—¥é–“ |Redis Replica + Backup
|**è¡Œå‹•ãƒ­ã‚°** |æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ |Redis Primary |30æ—¥é–“ |Redis Replica + Backup
|**è¨­å®šãƒ‡ãƒ¼ã‚¿** |ä½é »åº¦æ›´æ–° |Redis Primary |æ°¸ç¶š |Redis Replica + Backup
|**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** |ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  |Redis Primary |1æ™‚é–“ |Redis Replica
|**éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«** |é™çš„ãƒ‡ãƒ¼ã‚¿ |File System |æ°¸ç¶š |Backup Storage
|**AIãƒ¢ãƒ‡ãƒ«** |é™çš„ãƒ‡ãƒ¼ã‚¿ |File System |æ°¸ç¶š |Backup Storage
|===

== ğŸ—ï¸ Redis ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆ

=== ğŸ“‹ ã‚­ãƒ¼å‘½åè¦å‰‡

```yaml
# Key Naming Convention
PREFIX_PATTERNS:
  # Session Management
  session: "session:{session_id}"
  user: "user:{user_id}"
  
  # Detection Data
  detection: "detection:{timestamp}:{frame_id}"
  detection_summary: "detection:summary:{date}"
  detection_stats: "detection:stats:{hour}"
  
  # Behavior Analysis
  behavior: "behavior:{session_id}:{behavior_type}"
  behavior_timeline: "behavior:timeline:{date}"
  behavior_stats: "behavior:stats:{user_id}:{period}"
  
  # Configuration
  config: "config:{component}"
  schedule: "schedule:{schedule_id}"
  alert_rule: "alert:rule:{rule_id}"
  
  # Performance Monitoring
  performance: "perf:{instance_id}:{metric_type}"
  metrics: "metrics:{timestamp}"
  health: "health:{component}"
  
  # Cache Management
  cache: "cache:{cache_type}:{key}"
  temp: "temp:{process_id}:{temp_id}"
  
  # TTS and Audio
  tts: "tts:{voice_id}:{text_hash}"
  audio: "audio:{audio_id}"
  
  # AI/ML Model Data
  model: "model:{model_type}:{version}"
  inference: "inference:{model_id}:{frame_id}"
```

=== ğŸ“Š ä¸»è¦ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è©³ç´°å®šç¾©

==== ğŸ” æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ

[mermaid]
....
erDiagram
    DETECTION_LOG {
        string detection_id PK
        datetime timestamp
        string session_id FK
        string frame_id
        boolean person_detected
        boolean smartphone_detected
        float person_confidence
        float smartphone_confidence
        json person_bbox
        json smartphone_bbox
        float processing_time
        string device_used
        json model_metadata
        json detection_metadata
    }
    
    DETECTION_SUMMARY {
        string summary_id PK
        date summary_date
        string session_id FK
        int total_detections
        int person_detections
        int smartphone_detections
        float avg_confidence
        float total_processing_time
        json hourly_breakdown
        json performance_stats
    }
    
    DETECTION_STATS {
        string stats_id PK
        datetime hour_timestamp
        int detection_count
        float avg_fps
        float avg_processing_time
        float cpu_usage_avg
        float memory_usage_avg
        float gpu_usage_avg
        json model_performance
    }
    
    DETECTION_LOG ||--o{ DETECTION_SUMMARY : aggregates
    DETECTION_LOG ||--o{ DETECTION_STATS : contributes_to
....

```python
# Detection Log Data Structure
DETECTION_LOG_SCHEMA = {
    "detection_id": str,  # UUID
    "timestamp": datetime,  # ISO 8601 format
    "session_id": str,  # Session identifier
    "frame_id": str,  # Frame identifier
    "person_detected": bool,
    "smartphone_detected": bool,
    "person_confidence": float,  # 0.0 - 1.0
    "smartphone_confidence": float,  # 0.0 - 1.0
    "person_bbox": {
        "x": float,
        "y": float, 
        "width": float,
        "height": float
    },
    "smartphone_bbox": {
        "x": float,
        "y": float,
        "width": float, 
        "height": float
    },
    "processing_time": float,  # milliseconds
    "device_used": str,  # "cpu", "cuda:0", etc.
    "model_metadata": {
        "yolo_version": str,
        "mediapipe_version": str,
        "input_resolution": tuple,
        "confidence_threshold": float
    },
    "detection_metadata": {
        "frame_size": tuple,
        "preprocessing_time": float,
        "inference_time": float,
        "postprocessing_time": float
    }
}
```

==== ğŸ‘¤ è¡Œå‹•åˆ†æãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ

[mermaid]
....
erDiagram
    BEHAVIOR_LOG {
        string behavior_id PK
        datetime timestamp
        string session_id FK
        string behavior_type
        string current_state
        string previous_state
        int duration_seconds
        float confidence_score
        string trigger_event
        json context_data
        json analysis_result
    }
    
    BEHAVIOR_TIMELINE {
        string timeline_id PK
        date timeline_date
        string session_id FK
        json timeline_events
        json state_transitions
        json duration_summary
        json pattern_analysis
    }
    
    BEHAVIOR_STATS {
        string stats_id PK
        string user_id FK
        string period_type
        datetime period_start
        datetime period_end
        json absence_stats
        json smartphone_stats
        json productivity_metrics
        json trend_analysis
    }
    
    BEHAVIOR_LOG ||--o{ BEHAVIOR_TIMELINE : builds
    BEHAVIOR_LOG ||--o{ BEHAVIOR_STATS : aggregates
....

```python
# Behavior Analysis Data Structure
BEHAVIOR_LOG_SCHEMA = {
    "behavior_id": str,  # UUID
    "timestamp": datetime,
    "session_id": str,
    "behavior_type": str,  # "presence", "absence", "smartphone_usage"
    "current_state": str,  # "PRESENT", "ABSENT", "SMARTPHONE_DETECTED" 
    "previous_state": str,
    "duration_seconds": int,
    "confidence_score": float,  # 0.0 - 1.0
    "trigger_event": str,  # "detection_result", "timer_expiry", "manual"
    "context_data": {
        "recent_detections": list,
        "environmental_factors": dict,
        "system_state": dict
    },
    "analysis_result": {
        "pattern_match": bool,
        "anomaly_score": float,
        "prediction_confidence": float,
        "recommendations": list
    }
}

BEHAVIOR_TIMELINE_SCHEMA = {
    "timeline_id": str,
    "timeline_date": date,
    "session_id": str,
    "timeline_events": [
        {
            "timestamp": datetime,
            "event_type": str,
            "duration": int,
            "details": dict
        }
    ],
    "state_transitions": [
        {
            "from_state": str,
            "to_state": str,
            "timestamp": datetime,
            "confidence": float
        }
    ],
    "duration_summary": {
        "total_present_time": int,
        "total_absent_time": int,
        "smartphone_usage_time": int,
        "productive_time": int
    },
    "pattern_analysis": {
        "frequent_patterns": list,
        "anomalies": list,
        "productivity_score": float
    }
}
```

==== âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ

[mermaid]
....
erDiagram
    CONFIGURATION {
        string config_id PK
        string component_name
        string config_key
        json config_value
        string data_type
        datetime created_at
        datetime updated_at
        string updated_by
        json validation_rules
        boolean is_sensitive
    }
    
    SCHEDULE {
        string schedule_id PK
        string name
        time start_time
        time end_time
        json days_of_week
        boolean alert_enabled
        boolean monitoring_enabled
        datetime created_at
        datetime updated_at
        json metadata
    }
    
    ALERT_RULE {
        string rule_id PK
        string rule_name
        string condition_type
        json condition_params
        string action_type
        json action_params
        int priority_level
        boolean is_active
        datetime created_at
        datetime last_triggered
        json statistics
    }
    
    CONFIGURATION ||--o{ SCHEDULE : configures
    CONFIGURATION ||--o{ ALERT_RULE : defines
....

```python
# Configuration Data Structure
CONFIGURATION_SCHEMA = {
    "config_id": str,
    "component_name": str,  # "ai_detector", "alert_manager", "tts_service"
    "config_key": str,
    "config_value": Any,  # JSON-serializable value
    "data_type": str,  # "string", "integer", "float", "boolean", "json"
    "created_at": datetime,
    "updated_at": datetime,
    "updated_by": str,
    "validation_rules": {
        "type": str,
        "min_value": float,
        "max_value": float,
        "allowed_values": list,
        "regex_pattern": str
    },
    "is_sensitive": bool  # Encryption flag
}

SCHEDULE_SCHEMA = {
    "schedule_id": str,
    "name": str,
    "start_time": time,
    "end_time": time,
    "days_of_week": list,  # [0-6], 0=Monday
    "alert_enabled": bool,
    "monitoring_enabled": bool,
    "created_at": datetime,
    "updated_at": datetime,
    "metadata": {
        "description": str,
        "tags": list,
        "created_by": str
    }
}
```

==== ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ

```python
# Performance Monitoring Data Structure
PERFORMANCE_METRICS_SCHEMA = {
    "metric_id": str,
    "timestamp": datetime,
    "instance_id": str,
    "metric_type": str,  # "system", "application", "ai_model"
    "metrics": {
        # System metrics
        "cpu_usage_percent": float,
        "memory_usage_percent": float,
        "gpu_usage_percent": float,
        "disk_usage_percent": float,
        
        # Application metrics
        "fps": float,
        "detection_latency_ms": float,
        "frame_processing_time_ms": float,
        "queue_length": int,
        
        # AI Model metrics
        "model_inference_time_ms": float,
        "model_memory_usage_mb": float,
        "batch_size": int,
        "throughput_fps": float
    },
    "health_status": str,  # "healthy", "warning", "critical"
    "alerts_triggered": list
}

HEALTH_CHECK_SCHEMA = {
    "health_id": str,
    "timestamp": datetime,
    "component": str,
    "status": str,  # "up", "down", "degraded"
    "response_time_ms": float,
    "details": {
        "checks": dict,
        "errors": list,
        "warnings": list
    }
}
```

=== ğŸ”— ãƒ‡ãƒ¼ã‚¿é–¢é€£æ€§ã¨æ•´åˆæ€§

[mermaid]
....
graph LR
    subgraph "ğŸ¯ Core Data Flow"
        DETECTION[Detection Data]
        BEHAVIOR[Behavior Analysis]
        SESSION[Session Management]
        PERFORMANCE[Performance Data]
    end
    
    subgraph "âš™ï¸ Configuration Data"
        CONFIG[System Config]
        SCHEDULE[Schedules]
        ALERTS[Alert Rules]
    end
    
    subgraph "ğŸµ Media Data"
        TTS[TTS Cache]
        AUDIO[Audio Files]
        MODELS[AI Models]
    end
    
    subgraph "ğŸ“Š Analytics Data"
        STATS[Statistics]
        TRENDS[Trend Analysis]
        REPORTS[Reports]
    end
    
    %% Core relationships
    DETECTION --> BEHAVIOR
    DETECTION --> SESSION
    BEHAVIOR --> SESSION
    PERFORMANCE --> SESSION
    
    %% Configuration relationships
    CONFIG --> DETECTION
    SCHEDULE --> BEHAVIOR
    ALERTS --> BEHAVIOR
    
    %% Media relationships
    TTS --> AUDIO
    MODELS --> DETECTION
    
    %% Analytics relationships
    DETECTION --> STATS
    BEHAVIOR --> TRENDS
    STATS --> REPORTS
    TRENDS --> REPORTS
    
    classDef core fill:#e3f2fd
    classDef config fill:#e8f5e8
    classDef media fill:#fff3e0
    classDef analytics fill:#f3e5f5
    
    class DETECTION,BEHAVIOR,SESSION,PERFORMANCE core
    class CONFIG,SCHEDULE,ALERTS config
    class TTS,AUDIO,MODELS media
    class STATS,TRENDS,REPORTS analytics
....

== ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

=== ğŸ“ˆ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

```python
# Redis Index Strategy
REDIS_INDEXES = {
    # Time-based indexes for time-series queries
    "detection_by_time": {
        "key_pattern": "detection:index:time:{YYYYMMDD}",
        "sorted_set": True,
        "score": "timestamp",
        "member": "detection_id"
    },
    
    # Session-based indexes
    "detection_by_session": {
        "key_pattern": "detection:index:session:{session_id}",
        "sorted_set": True,
        "score": "timestamp", 
        "member": "detection_id"
    },
    
    # Behavior pattern indexes
    "behavior_by_type": {
        "key_pattern": "behavior:index:type:{behavior_type}",
        "sorted_set": True,
        "score": "timestamp",
        "member": "behavior_id"
    },
    
    # Performance metrics indexes
    "metrics_by_instance": {
        "key_pattern": "metrics:index:instance:{instance_id}",
        "sorted_set": True,
        "score": "timestamp",
        "member": "metric_id"
    }
}
```

=== âš¡ ã‚¯ã‚¨ãƒªæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# Optimized Query Patterns
class OptimizedQueries:
    
    def get_recent_detections(self, session_id: str, limit: int = 100):
        """æœ€æ–°ã®æ¤œå‡ºçµæœã‚’åŠ¹ç‡çš„ã«å–å¾—"""
        # Use sorted set index for O(log(N)) retrieval
        detection_ids = redis.zrevrange(
            f"detection:index:session:{session_id}",
            0, limit-1
        )
        
        # Batch fetch detection data using pipeline
        pipe = redis.pipeline()
        for detection_id in detection_ids:
            pipe.hgetall(f"detection:{detection_id}")
        
        return pipe.execute()
    
    def get_behavior_timeline(self, date: str, session_id: str):
        """æŒ‡å®šæ—¥ã®è¡Œå‹•ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’å–å¾—"""
        timeline_key = f"behavior:timeline:{date}:{session_id}"
        
        # Check cache first
        cached_timeline = redis.get(timeline_key)
        if cached_timeline:
            return json.loads(cached_timeline)
        
        # Build timeline from behavior logs
        behavior_ids = redis.zrangebyscore(
            f"behavior:index:session:{session_id}",
            start_timestamp, end_timestamp
        )
        
        # Cache result for future queries
        timeline_data = self._build_timeline(behavior_ids)
        redis.setex(timeline_key, 3600, json.dumps(timeline_data))
        
        return timeline_data
    
    def get_performance_stats(self, instance_id: str, hours: int = 24):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—"""
        end_time = time.time()
        start_time = end_time - (hours * 3600)
        
        metric_ids = redis.zrangebyscore(
            f"metrics:index:instance:{instance_id}",
            start_time, end_time
        )
        
        # Use Lua script for server-side aggregation
        lua_script = """
        local stats = {}
        for i, metric_id in ipairs(ARGV) do
            local data = redis.call('HGETALL', 'metrics:' .. metric_id)
            -- Aggregate statistics server-side
        end
        return stats
        """
        
        return redis.eval(lua_script, 0, *metric_ids)
```

=== ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æˆ¦ç•¥

[mermaid]
....
graph TB
    subgraph "ğŸ“Š Data Lifecycle Management"
        HOT[Hot Data<br/>< 1 hour<br/>Memory: High Priority]
        WARM[Warm Data<br/>1-24 hours<br/>Memory: Medium Priority]
        COLD[Cold Data<br/>1-7 days<br/>Memory: Low Priority]
        ARCHIVE[Archive Data<br/>> 7 days<br/>Compressed/Backup]
    end
    
    subgraph "ğŸ’¾ Memory Optimization"
        COMPRESS[Data Compression<br/>JSON â†’ MessagePack]
        EXPIRE[TTL Management<br/>Automatic Expiry]
        PIPELINE[Pipeline Operations<br/>Batch Processing]
        LUA[Lua Scripts<br/>Server-side Logic]
    end
    
    subgraph "ğŸ”„ Cache Strategy"
        L1[L1 Cache<br/>Application Memory]
        L2[L2 Cache<br/>Redis Primary]
        L3[L3 Cache<br/>Redis Replica]
        PERSIST[Persistent Storage<br/>Backup Systems]
    end
    
    HOT --> WARM
    WARM --> COLD
    COLD --> ARCHIVE
    
    HOT --> COMPRESS
    WARM --> EXPIRE
    COLD --> PIPELINE
    ARCHIVE --> LUA
    
    L1 --> L2
    L2 --> L3
    L3 --> PERSIST
    
    classDef hot fill:#ffebee
    classDef warm fill:#fff3e0
    classDef cold fill:#e8f5e8
    classDef archive fill:#f3e5f5
    
    class HOT hot
    class WARM warm
    class COLD cold
    class ARCHIVE archive
....

```python
# Memory Optimization Configuration
MEMORY_OPTIMIZATION = {
    # Data compression settings
    "compression": {
        "enable": True,
        "algorithm": "lz4",  # Fast compression for real-time data
        "threshold_bytes": 1024,  # Compress data > 1KB
        "compression_level": 1  # Fast compression
    },
    
    # TTL management
    "ttl_policy": {
        "detection_data": 604800,  # 7 days
        "behavior_logs": 2592000,  # 30 days
        "performance_metrics": 3600,  # 1 hour
        "session_data": 86400,  # 24 hours
        "cache_data": 300,  # 5 minutes
        "temp_data": 60  # 1 minute
    },
    
    # Memory limits
    "memory_limits": {
        "max_memory_usage": "4gb",
        "eviction_policy": "allkeys-lru",
        "max_clients": 10000
    },
    
    # Batch processing
    "batch_settings": {
        "pipeline_size": 100,
        "batch_timeout_ms": 10,
        "max_batch_size": 1000
    }
}
```

== ğŸ”’ ãƒ‡ãƒ¼ã‚¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

=== ğŸ›¡ï¸ ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–æˆ¦ç•¥

[mermaid]
....
graph TB
    subgraph "ğŸ” Encryption Layers"
        APP_ENCRYPT[Application Layer<br/>Field-level Encryption]
        REDIS_ENCRYPT[Redis Layer<br/>TLS + AUTH]
        STORAGE_ENCRYPT[Storage Layer<br/>Disk Encryption]
        NETWORK_ENCRYPT[Network Layer<br/>End-to-End TLS]
    end
    
    subgraph "ğŸ—ï¸ Key Management"
        KEY_ROTATION[Key Rotation<br/>Automatic]
        KEY_VAULT[Key Vault<br/>Secure Storage]
        ACCESS_CONTROL[Access Control<br/>RBAC]
        AUDIT_LOG[Audit Logging<br/>Key Usage]
    end
    
    subgraph "ğŸ“Š Data Classification"
        PII[PII Data<br/>Personal Information]
        SENSITIVE[Sensitive Data<br/>Detection Details]
        INTERNAL[Internal Data<br/>System Metrics]
        PUBLIC[Public Data<br/>Configuration]
    end
    
    subgraph "ğŸ”’ Privacy Protection"
        ANONYMIZATION[Data Anonymization]
        PSEUDONYMIZATION[Pseudonymization]
        DATA_MASKING[Data Masking]
        RETENTION[Retention Policy]
    end
    
    APP_ENCRYPT --> REDIS_ENCRYPT
    REDIS_ENCRYPT --> STORAGE_ENCRYPT
    STORAGE_ENCRYPT --> NETWORK_ENCRYPT
    
    KEY_ROTATION --> KEY_VAULT
    KEY_VAULT --> ACCESS_CONTROL
    ACCESS_CONTROL --> AUDIT_LOG
    
    PII --> ANONYMIZATION
    SENSITIVE --> PSEUDONYMIZATION
    INTERNAL --> DATA_MASKING
    PUBLIC --> RETENTION
    
    classDef encryption fill:#ffebee
    classDef keymanagement fill:#e8f5e8
    classDef classification fill:#fff3e0
    classDef privacy fill:#f3e5f5
    
    class APP_ENCRYPT,REDIS_ENCRYPT,STORAGE_ENCRYPT,NETWORK_ENCRYPT encryption
    class KEY_ROTATION,KEY_VAULT,ACCESS_CONTROL,AUDIT_LOG keymanagement
    class PII,SENSITIVE,INTERNAL,PUBLIC classification
    class ANONYMIZATION,PSEUDONYMIZATION,DATA_MASKING,RETENTION privacy
....

```python
# Data Security Configuration
DATA_SECURITY = {
    # Field-level encryption for sensitive data
    "encrypted_fields": [
        "user_identity",
        "location_data", 
        "personal_metadata",
        "biometric_data"
    ],
    
    # Data classification levels
    "classification": {
        "public": {
            "encryption": False,
            "access_level": "all"
        },
        "internal": {
            "encryption": True,
            "access_level": "internal"
        },
        "sensitive": {
            "encryption": True,
            "access_level": "authorized"
        },
        "restricted": {
            "encryption": True,
            "access_level": "admin_only"
        }
    },
    
    # Privacy protection
    "privacy": {
        "anonymization_delay": 3600,  # 1 hour
        "data_retention_days": 90,
        "audit_retention_days": 365,
        "gdpr_compliance": True
    }
}
```

== ğŸ”§ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

=== ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

[mermaid]
....
graph TB
    subgraph "ğŸ”„ Backup Types"
        FULL[Full Backup<br/>Complete Data Snapshot]
        INCREMENTAL[Incremental Backup<br/>Changed Data Only]
        DIFFERENTIAL[Differential Backup<br/>Since Last Full]
        CONTINUOUS[Continuous Backup<br/>Real-time Replication]
    end
    
    subgraph "ğŸ“… Backup Schedule"
        DAILY[Daily Backup<br/>2:00 AM]
        WEEKLY[Weekly Backup<br/>Sunday 1:00 AM]
        MONTHLY[Monthly Backup<br/>1st Day 12:00 AM]
        REALTIME[Real-time Backup<br/>Continuous]
    end
    
    subgraph "ğŸ“ Backup Locations"
        LOCAL[Local Storage<br/>Same Region]
        REMOTE[Remote Storage<br/>Different Region]
        CLOUD[Cloud Storage<br/>Multi-Cloud]
        OFFLINE[Offline Storage<br/>Cold Storage]
    end
    
    subgraph "âœ… Backup Verification"
        INTEGRITY[Data Integrity<br/>Checksum Validation]
        RESTORE_TEST[Restore Testing<br/>Monthly Validation]
        MONITORING[Backup Monitoring<br/>Success/Failure Alerts]
        REPORTING[Backup Reporting<br/>Status Dashboard]
    end
    
    FULL --> DAILY
    INCREMENTAL --> DAILY
    DIFFERENTIAL --> WEEKLY
    CONTINUOUS --> REALTIME
    
    DAILY --> LOCAL
    WEEKLY --> REMOTE
    MONTHLY --> CLOUD
    REALTIME --> OFFLINE
    
    LOCAL --> INTEGRITY
    REMOTE --> RESTORE_TEST
    CLOUD --> MONITORING
    OFFLINE --> REPORTING
    
    classDef backuptype fill:#e3f2fd
    classDef schedule fill:#e8f5e8
    classDef location fill:#fff3e0
    classDef verification fill:#f3e5f5
    
    class FULL,INCREMENTAL,DIFFERENTIAL,CONTINUOUS backuptype
    class DAILY,WEEKLY,MONTHLY,REALTIME schedule
    class LOCAL,REMOTE,CLOUD,OFFLINE location
    class INTEGRITY,RESTORE_TEST,MONITORING,REPORTING verification
....

```python
# Backup Configuration
BACKUP_CONFIG = {
    "schedule": {
        "full_backup": {
            "frequency": "weekly",
            "time": "01:00",
            "day": "sunday",
            "retention_weeks": 12
        },
        "incremental_backup": {
            "frequency": "daily", 
            "time": "02:00",
            "retention_days": 30
        },
        "continuous_replication": {
            "enabled": True,
            "lag_threshold_seconds": 1,
            "replica_locations": ["us-west-2", "eu-west-1"]
        }
    },
    
    "storage": {
        "local": {
            "path": "/backup/local",
            "retention_days": 7,
            "compression": True
        },
        "remote": {
            "type": "s3",
            "bucket": "kanshichan-backups",
            "encryption": "AES-256",
            "retention_months": 12
        }
    },
    
    "verification": {
        "integrity_check": True,
        "restore_test_frequency": "monthly",
        "monitoring_enabled": True,
        "alert_on_failure": True
    }
}
```

=== ğŸ”„ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œæˆ¦ç•¥

```python
# Data Migration Strategy
class DataMigration:
    
    def migrate_schema_version(self, from_version: str, to_version: str):
        """ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ç§»è¡Œ"""
        migration_plan = self._get_migration_plan(from_version, to_version)
        
        for step in migration_plan:
            # Pre-migration validation
            self._validate_pre_migration(step)
            
            # Execute migration step
            self._execute_migration_step(step)
            
            # Post-migration validation
            self._validate_post_migration(step)
            
            # Update version metadata
            self._update_version_metadata(step.target_version)
    
    def migrate_data_format(self, data_type: str, new_format: dict):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç§»è¡Œ"""
        # Gradual migration pattern
        keys = redis.scan_iter(match=f"{data_type}:*")
        
        for key in keys:
            old_data = redis.get(key)
            
            # Transform data to new format
            new_data = self._transform_data(old_data, new_format)
            
            # Atomic replacement with backup
            pipe = redis.pipeline()
            pipe.set(f"{key}:backup", old_data, ex=86400)  # 24h backup
            pipe.set(key, new_data)
            pipe.execute()
    
    def _validate_migration(self, validation_rules: dict) -> bool:
        """ç§»è¡Œæ¤œè¨¼"""
        for rule in validation_rules:
            if not self._check_rule(rule):
                raise MigrationValidationError(f"Validation failed: {rule}")
        return True
```

== ğŸ“Š é‹ç”¨ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

=== ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹

[cols="2,2,2,2", options="header"]
|===
|ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†é¡ |ç›£è¦–é …ç›® |é–¾å€¤ |ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶
|**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** |ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ |< 5ms |> 10ms ã§ Warning
|**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** |ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ |> 1000 QPS |< 500 QPS ã§ Warning
|**å®¹é‡** |ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ |< 80% |> 90% ã§ Critical
|**å®¹é‡** |ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ |< 85% |> 95% ã§ Critical
|**å¯ç”¨æ€§** |æ¥ç¶šæˆåŠŸç‡ |> 99.9% |< 99% ã§ Critical
|**å¯ç”¨æ€§** |ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ |< 1ç§’ |> 5ç§’ ã§ Warning
|**ãƒ‡ãƒ¼ã‚¿å“è³ª** |ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ |100% |ã‚¨ãƒ©ãƒ¼æ¤œå‡ºã§ Critical
|**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** |èªè¨¼å¤±æ•—ç‡ |< 1% |> 5% ã§ Warning
|===

```python
# Monitoring Configuration
MONITORING_CONFIG = {
    "metrics": {
        "collection_interval": 15,  # seconds
        "retention_hours": 168,  # 7 days
        "aggregation_intervals": [60, 300, 3600]  # 1min, 5min, 1hour
    },
    
    "alerts": {
        "performance": {
            "response_time_ms": {
                "warning": 10,
                "critical": 50
            },
            "throughput_qps": {
                "warning": 500,
                "critical": 100
            }
        },
        "resource": {
            "memory_usage_percent": {
                "warning": 80,
                "critical": 90
            },
            "connection_count": {
                "warning": 800,
                "critical": 950
            }
        }
    },
    
    "notifications": {
        "channels": ["slack", "email", "webhook"],
        "escalation_time": 300,  # 5 minutes
        "max_alerts_per_hour": 10
    }
}
```

== ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

=== ğŸ“– å¿…é ˆå‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
* **<<system-design>>**: ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆè©³ç´°
* **<<data-management>>**: ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
* **<<performance-optimization>>**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
* **<<security-specifications>>**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä»•æ§˜

=== ğŸ› ï¸ é–‹ç™ºè€…å‘ã‘ãƒªã‚½ãƒ¼ã‚¹
* **<<backend-architecture>>**: ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
* **<<rest-api-reference>>**: REST APIä»•æ§˜
* **<<development-guide>>**: é–‹ç™ºã‚¬ã‚¤ãƒ‰
* **<<testing-strategy>>**: ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

=== ğŸ“Š é‹ç”¨ãƒ»ä¿å®ˆé–¢é€£
* **<<configuration-guide>>**: è¨­å®šç®¡ç†
* **<<operations-monitoring>>**: é‹ç”¨ç›£è¦–
* **<<maintenance-procedures>>**: ä¿å®ˆæ‰‹é †
* **<<troubleshooting-guide>>**: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

[NOTE]
====
ğŸ”„ **ç¶™ç¶šçš„æ”¹å–„**

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã¯é‹ç”¨å®Ÿç¸¾ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦
ç¶™ç¶šçš„ã«æœ€é©åŒ–ã•ã‚Œã¾ã™ã€‚

**å¤‰æ›´ç®¡ç†**: ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã¯å¿…ãšãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»ã¨å…±ã«å®Ÿæ–½ +
**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: å®šæœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¨æœ€é©åŒ– +
**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: å››åŠæœŸã”ã¨ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿæ–½

**ãƒ‡ãƒ¼ã‚¿å“è³ª**: è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ +
**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: æœˆæ¬¡ã®ãƒªã‚¹ãƒˆã‚¢ãƒ†ã‚¹ãƒˆå®Ÿæ–½
====

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber} 