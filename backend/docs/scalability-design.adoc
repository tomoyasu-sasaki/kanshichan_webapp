= ğŸ“ˆ ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¨­è¨ˆ
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 3
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:author: KanshiChan Development Team  
:email: team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

[NOTE]
====
ğŸ“‹ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±**

* **ä½œæˆè€…**: KanshiChan Development Team
* **æœ€çµ‚æ›´æ–°æ—¥**: {docdate}
* **å¯¾è±¡èª­è€…**: ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã€ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒªãƒ¼ãƒ‰ã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
* **å‰æçŸ¥è­˜**: ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
* **é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<backend-architecture.adoc>>, <<deployment-architecture.adoc>>, <<performance-optimization.adoc>>
====

== ğŸ“– æ¦‚è¦

ç›£è¦–ã¡ã‚ƒã‚“ï¼ˆKanshiChanï¼‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¨­è¨ˆã«ã¤ã„ã¦è©³è¿°ã—ã¾ã™ã€‚
AIç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®æ€§è³ªä¸Šã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã«å¯¾å¿œã™ã‚‹æ‹¡å¼µå¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¿…è¦ã§ã™ã€‚

=== ğŸ¯ è¨­è¨ˆç›®æ¨™

* **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: 1000åŒæ™‚æ¥ç¶šã§ã®å®‰å®šå‹•ä½œ
* **å¯ç”¨æ€§**: 99.9%ã®ã‚µãƒ¼ãƒ“ã‚¹å¯ç”¨æ€§ç¢ºä¿  
* **æ‹¡å¼µæ€§**: æ°´å¹³ãƒ»å‚ç›´ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œ
* **åŠ¹ç‡æ€§**: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡æœ€é©åŒ–

== ğŸ—ï¸ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

=== ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ§‹æˆ

[mermaid]
....
graph TB
    subgraph "Load Balancer Layer"
        LB[Load Balancer<br/>nginx/HAProxy]
    end
    
    subgraph "Application Layer (Auto Scaling)"
        APP1[KanshiChan Instance 1<br/>Flask + WebSocket]
        APP2[KanshiChan Instance 2<br/>Flask + WebSocket] 
        APP3[KanshiChan Instance N<br/>Flask + WebSocket]
    end
    
    subgraph "AI Processing Layer"
        AI1[AI Worker 1<br/>YOLO + MediaPipe]
        AI2[AI Worker 2<br/>YOLO + MediaPipe]
        AI3[AI Worker N<br/>YOLO + MediaPipe]
    end
    
    subgraph "Data Layer"
        REDIS[(Redis Cluster<br/>Session + Cache)]
        DB[(PostgreSQL<br/>Master-Slave)]
        QUEUE[Message Queue<br/>Redis/RabbitMQ]
    end
    
    subgraph "Storage Layer"
        FILES[File Storage<br/>S3/MinIO]
        LOGS[Log Storage<br/>ELK Stack]
    end
    
    LB --> APP1
    LB --> APP2  
    LB --> APP3
    
    APP1 --> AI1
    APP2 --> AI2
    APP3 --> AI3
    
    APP1 --> REDIS
    APP2 --> REDIS
    APP3 --> REDIS
    
    APP1 --> DB
    APP2 --> DB
    APP3 --> DB
    
    AI1 --> QUEUE
    AI2 --> QUEUE
    AI3 --> QUEUE
    
    APP1 --> FILES
    APP2 --> FILES
    APP3 --> FILES
    
    APP1 --> LOGS
    APP2 --> LOGS
    APP3 --> LOGS
....

=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

==== ğŸ”„ æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆHorizontal Scalingï¼‰

**ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤**
```yaml
# docker-compose.scale.yml
version: '3.8'
services:
  kanshichan-app:
    build: .
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
    environment:
      - REDIS_CLUSTER_NODES=redis-1:6379,redis-2:6379,redis-3:6379
      - DATABASE_URL=postgresql://user:pass@db-cluster/kanshichan
    networks:
      - kanshichan-network

  nginx-lb:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - kanshichan-app
```

**AIå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼**
```python
# src/services/ai_ml/distributed_ai_processor.py
from typing import Dict, List
import asyncio
from dataclasses import dataclass
from utils.logger import setup_logger

logger = setup_logger(__name__)

@dataclass
class AIWorkerNode:
    """AIå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æƒ…å ±"""
    node_id: str
    host: str
    port: int
    gpu_available: bool
    current_load: float
    max_capacity: int

class DistributedAIProcessor:
    """åˆ†æ•£AIå‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.worker_nodes: List[AIWorkerNode] = []
        self.load_balancer = AILoadBalancer()
        
    async def scale_workers(self, target_count: int):
        """AIå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
        current_count = len(self.worker_nodes)
        
        if target_count > current_count:
            # ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
            for i in range(current_count, target_count):
                await self._spawn_worker(f"ai-worker-{i}")
        elif target_count < current_count:
            # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³  
            for i in range(target_count, current_count):
                await self._terminate_worker(f"ai-worker-{i}")
                
        logger.info(f"AI workers scaled: {current_count} -> {target_count}")
        
    async def distribute_detection_task(self, frame_data: bytes) -> Dict:
        """æ¤œå‡ºã‚¿ã‚¹ã‚¯ã®åˆ†æ•£å‡¦ç†"""
        optimal_worker = self.load_balancer.select_worker(self.worker_nodes)
        
        if not optimal_worker:
            raise RuntimeError("No available AI workers")
            
        try:
            result = await self._send_detection_request(optimal_worker, frame_data)
            self._update_worker_metrics(optimal_worker, result)
            return result
        except Exception as e:
            logger.error(f"Detection task failed on {optimal_worker.node_id}: {e}")
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼å‡¦ç†
            return await self._handle_worker_failure(optimal_worker, frame_data)
```

==== â¬†ï¸ å‚ç›´ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆVertical Scalingï¼‰

**ãƒªã‚½ãƒ¼ã‚¹è‡ªå‹•èª¿æ•´**
```python
# src/core/resource_optimizer.py
import psutil
import torch
from typing import Dict, Tuple
from utils.logger import setup_logger

logger = setup_logger(__name__)

class ResourceOptimizer:
    """å‹•çš„ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.cpu_threshold = 0.8
        self.memory_threshold = 0.85
        self.gpu_threshold = 0.9
        
    def optimize_resources(self) -> Dict[str, any]:
        """ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã«åŸºã¥ãæœ€é©åŒ–"""
        metrics = self._collect_metrics()
        optimizations = {}
        
        # CPUæœ€é©åŒ–
        if metrics['cpu_usage'] > self.cpu_threshold:
            optimizations['cpu'] = self._optimize_cpu_usage()
            
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–  
        if metrics['memory_usage'] > self.memory_threshold:
            optimizations['memory'] = self._optimize_memory_usage()
            
        # GPUæœ€é©åŒ–
        if metrics['gpu_usage'] > self.gpu_threshold:
            optimizations['gpu'] = self._optimize_gpu_usage()
            
        return optimizations
        
    def _collect_metrics(self) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        metrics = {
            'cpu_usage': cpu_percent / 100.0,
            'memory_usage': memory.percent / 100.0,
            'memory_available_gb': memory.available / (1024**3)
        }
        
        # GPUä½¿ç”¨ç‡ï¼ˆCUDAåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_stats()
            allocated = gpu_memory.get('allocated_bytes.all.current', 0)
            reserved = gpu_memory.get('reserved_bytes.all.current', 0)
            metrics['gpu_usage'] = allocated / max(reserved, 1)
            metrics['gpu_memory_gb'] = allocated / (1024**3)
            
        return metrics
        
    def _optimize_cpu_usage(self) -> Dict:
        """CPUä½¿ç”¨ç‡æœ€é©åŒ–"""
        return {
            'action': 'reduce_concurrent_processing',
            'max_workers': max(1, psutil.cpu_count() // 2),
            'frame_skip_rate': min(5, self.config_manager.get('ai.frame_skip_rate', 2) + 1)
        }
        
    def _optimize_memory_usage(self) -> Dict:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"""
        return {
            'action': 'clear_caches',
            'cache_size_reduction': 0.3,
            'batch_size_reduction': 0.5
        }
        
    def _optimize_gpu_usage(self) -> Dict:
        """GPUä½¿ç”¨é‡æœ€é©åŒ–"""
        return {
            'action': 'reduce_model_precision',
            'use_fp16': True,
            'reduce_batch_size': True
        }
```

== ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

=== AIå‡¦ç†æœ€é©åŒ–

**ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–**
```python
# src/core/parallel_detection.py
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel
from typing import List, Dict, Any
import numpy as np

class ParallelDetectionEngine:
    """ä¸¦åˆ—AIæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model_path: str, device_ids: List[int] = None):
        self.device_ids = device_ids or list(range(torch.cuda.device_count()))
        self.model = self._load_and_parallelize_model(model_path)
        self.batch_processor = BatchProcessor()
        
    def _load_and_parallelize_model(self, model_path: str):
        """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨ä¸¦åˆ—åŒ–è¨­å®š"""
        model = torch.load(model_path)
        
        if len(self.device_ids) > 1:
            # ãƒãƒ«ãƒGPUä¸¦åˆ—å‡¦ç†
            model = DataParallel(model, device_ids=self.device_ids)
            logger.info(f"Model parallelized across GPUs: {self.device_ids}")
        
        return model.cuda() if torch.cuda.is_available() else model
        
    async def process_batch(self, frames: List[np.ndarray]) -> List[Dict]:
        """ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰å‡¦ç†ã¨ãƒãƒƒãƒåŒ–
        batch_tensor = self.batch_processor.prepare_batch(frames)
        
        # ä¸¦åˆ—æ¨è«–å®Ÿè¡Œ
        with torch.no_grad():
            results = self.model(batch_tensor)
            
        # çµæœã®å¾Œå‡¦ç†
        return self.batch_processor.process_results(results, frames)

class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    def __init__(self, max_batch_size: int = 8):
        self.max_batch_size = max_batch_size
        
    def prepare_batch(self, frames: List[np.ndarray]) -> torch.Tensor:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
        processed_frames = []
        
        for frame in frames:
            # ãƒªã‚µã‚¤ã‚ºã¨æ­£è¦åŒ–
            processed_frame = self._preprocess_frame(frame)
            processed_frames.append(processed_frame)
            
        return torch.stack(processed_frames)
        
    def _preprocess_frame(self, frame: np.ndarray) -> torch.Tensor:
        """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰å‡¦ç†"""
        # OpenCVãƒ•ãƒ¬ãƒ¼ãƒ (BGR) -> RGBå¤‰æ›
        frame_rgb = frame[:, :, ::-1]
        
        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ã¨æ­£è¦åŒ–
        tensor = torch.from_numpy(frame_rgb).float()
        tensor = tensor.permute(2, 0, 1) / 255.0
        
        return tensor
```

=== ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

**éšå±¤åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ **
```python
# src/utils/scalable_cache.py
import redis
import pickle
import hashlib
from typing import Any, Optional, Dict
from dataclasses import dataclass
import time

@dataclass
class CacheStats:
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    
    @property
    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

class ScalableCache:
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªéšå±¤åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    
    def __init__(self, redis_cluster_nodes: List[str]):
        # L1: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ (æœ€é«˜é€Ÿ)
        self.l1_cache: Dict[str, Any] = {}
        self.l1_max_size = 1000
        
        # L2: Redisåˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (ä¸­é€Ÿãƒ»æ°¸ç¶š)
        self.redis_cluster = redis.RedisCluster(
            startup_nodes=[{"host": node.split(':')[0], "port": int(node.split(':')[1])} 
                          for node in redis_cluster_nodes],
            decode_responses=False
        )
        
        self.stats = CacheStats()
        
    def get(self, key: str) -> Optional[Any]:
        """éšå±¤åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å–å¾—"""
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è©¦è¡Œ
        if key in self.l1_cache:
            self.stats.hits += 1
            return self.l1_cache[key]
            
        # L2 (Redis) ã‹ã‚‰è©¦è¡Œ
        try:
            data = self.redis_cluster.get(key)
            if data is not None:
                value = pickle.loads(data)
                # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ˜‡æ ¼
                self._set_l1(key, value)
                self.stats.hits += 1
                return value
        except Exception as e:
            logger.warning(f"Redis cache error: {e}")
            
        self.stats.misses += 1
        return None
        
    def set(self, key: str, value: Any, ttl: int = 3600):
        """éšå±¤åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®ä¿å­˜"""
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self._set_l1(key, value)
        
        # L2 (Redis) ã«ä¿å­˜
        try:
            data = pickle.dumps(value)
            self.redis_cluster.setex(key, ttl, data)
        except Exception as e:
            logger.warning(f"Redis cache set error: {e}")
            
    def _set_l1(self, key: str, value: Any):
        """L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
        if len(self.l1_cache) >= self.l1_max_size:
            # LRU eviction (ç°¡æ˜“å®Ÿè£…)
            oldest_key = next(iter(self.l1_cache))
            del self.l1_cache[oldest_key]
            self.stats.evictions += 1
            
        self.l1_cache[key] = value
        
    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±"""
        return {
            'l1_size': len(self.l1_cache),
            'l1_max_size': self.l1_max_size,
            'hit_rate': self.stats.hit_rate,
            'total_hits': self.stats.hits,
            'total_misses': self.stats.misses,
            'evictions': self.stats.evictions
        }
```

== ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

=== èª­ã¿å–ã‚Šåˆ†æ•£ï¼ˆRead Replicasï¼‰

**Master-Slaveæ§‹æˆ**
```python
# src/models/scalable_db.py
import sqlalchemy as sa
from sqlalchemy.orm import sessionmaker
from typing import Dict, List
import random

class DatabaseRouter:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿å–ã‚Šåˆ†æ•£ãƒ«ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: Dict[str, str]):
        self.master_url = config['master_url']
        self.slave_urls = config['slave_urls']
        
        # Master (æ›¸ãè¾¼ã¿ç”¨)
        self.master_engine = sa.create_engine(self.master_url, pool_size=20)
        self.MasterSession = sessionmaker(bind=self.master_engine)
        
        # Slaves (èª­ã¿å–ã‚Šç”¨)
        self.slave_engines = [
            sa.create_engine(url, pool_size=10) for url in self.slave_urls
        ]
        self.SlaveSessions = [
            sessionmaker(bind=engine) for engine in self.slave_engines
        ]
        
    def get_read_session(self):
        """èª­ã¿å–ã‚Šç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆè² è·åˆ†æ•£ï¼‰"""
        session_class = random.choice(self.SlaveSessions)
        return session_class()
        
    def get_write_session(self):
        """æ›¸ãè¾¼ã¿ç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆMasterï¼‰"""
        return self.MasterSession()
        
    def execute_read_query(self, query: str, params: Dict = None):
        """èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        session = self.get_read_session()
        try:
            result = session.execute(sa.text(query), params or {})
            return result.fetchall()
        finally:
            session.close()
            
    def execute_write_query(self, query: str, params: Dict = None):
        """æ›¸ãè¾¼ã¿ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        session = self.get_write_session()
        try:
            result = session.execute(sa.text(query), params or {})
            session.commit()
            return result
        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()
```

=== ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥

**ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**
```python
# src/models/sharding_manager.py
import hashlib
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class ShardInfo:
    shard_id: str
    database_url: str
    weight: float  # è² è·åˆ†æ•£ç”¨é‡ã¿

class ShardingManager:
    """ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç®¡ç†"""
    
    def __init__(self, shard_configs: List[Dict]):
        self.shards = [
            ShardInfo(
                shard_id=config['id'],
                database_url=config['url'], 
                weight=config.get('weight', 1.0)
            ) for config in shard_configs
        ]
        
    def get_shard_for_user(self, user_id: str) -> ShardInfo:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠ"""
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        shard_index = hash_value % len(self.shards)
        return self.shards[shard_index]
        
    def get_shard_for_time(self, timestamp: int) -> ShardInfo:
        """æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠï¼ˆè¡Œå‹•ãƒ­ã‚°ç”¨ï¼‰"""
        # æœˆå˜ä½ã§ã‚·ãƒ£ãƒ¼ãƒ‰åˆ†å‰²
        month_hash = timestamp // (30 * 24 * 3600)
        shard_index = month_hash % len(self.shards)
        return self.shards[shard_index]
        
    def execute_distributed_query(self, query: str, shard_filter=None) -> List[Any]:
        """åˆ†æ•£ã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        results = []
        target_shards = self.shards if shard_filter is None else shard_filter(self.shards)
        
        for shard in target_shards:
            shard_result = self._execute_on_shard(shard, query)
            results.extend(shard_result)
            
        return results
```

== ğŸ“ˆ ç›£è¦–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†**
```python
# src/services/monitoring/scalability_monitor.py
import asyncio
import time
from typing import Dict, List
from dataclasses import dataclass, asdict
import psutil
import prometheus_client

@dataclass
class ScalabilityMetrics:
    timestamp: float
    instance_id: str
    cpu_usage: float
    memory_usage: float
    gpu_usage: float
    active_connections: int
    ai_processing_queue_size: int
    average_response_time: float
    throughput_fps: float

class ScalabilityMonitor:
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, instance_id: str):
        self.instance_id = instance_id
        self.metrics_history: List[ScalabilityMetrics] = []
        self.max_history_size = 1000
        
        # Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.setup_prometheus_metrics()
        
    def setup_prometheus_metrics(self):
        """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆæœŸåŒ–"""
        self.cpu_gauge = prometheus_client.Gauge(
            'kanshichan_cpu_usage', 'CPUä½¿ç”¨ç‡', ['instance_id']
        )
        self.memory_gauge = prometheus_client.Gauge(
            'kanshichan_memory_usage', 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡', ['instance_id'] 
        )
        self.connections_gauge = prometheus_client.Gauge(
            'kanshichan_active_connections', 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šæ•°', ['instance_id']
        )
        self.response_time_histogram = prometheus_client.Histogram(
            'kanshichan_response_time', 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“', ['instance_id']
        )
        
    async def collect_metrics(self) -> ScalabilityMetrics:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        metrics = ScalabilityMetrics(
            timestamp=time.time(),
            instance_id=self.instance_id,
            cpu_usage=psutil.cpu_percent(),
            memory_usage=psutil.virtual_memory().percent,
            gpu_usage=self._get_gpu_usage(),
            active_connections=self._count_active_connections(),
            ai_processing_queue_size=self._get_queue_size(),
            average_response_time=self._calculate_avg_response_time(),
            throughput_fps=self._calculate_throughput()
        )
        
        # Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        self._update_prometheus_metrics(metrics)
        
        # å±¥æ­´ç®¡ç†
        self._add_to_history(metrics)
        
        return metrics
        
    def _update_prometheus_metrics(self, metrics: ScalabilityMetrics):
        """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        self.cpu_gauge.labels(instance_id=self.instance_id).set(metrics.cpu_usage)
        self.memory_gauge.labels(instance_id=self.instance_id).set(metrics.memory_usage)
        self.connections_gauge.labels(instance_id=self.instance_id).set(metrics.active_connections)
        
    def get_scaling_recommendation(self) -> Dict[str, Any]:
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        if len(self.metrics_history) < 10:
            return {'action': 'monitor', 'reason': 'insufficient_data'}
            
        recent_metrics = self.metrics_history[-10:]
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        
        recommendations = []
        
        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ¨å¥¨
        if avg_cpu > 80:
            recommendations.append({
                'action': 'scale_up',
                'target': 'cpu',
                'reason': f'High CPU usage: {avg_cpu:.1f}%'
            })
            
        if avg_memory > 85:
            recommendations.append({
                'action': 'scale_up', 
                'target': 'memory',
                'reason': f'High memory usage: {avg_memory:.1f}%'
            })
            
        # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æ¨å¥¨
        if avg_cpu < 30 and avg_memory < 50:
            recommendations.append({
                'action': 'scale_down',
                'reason': f'Low resource usage: CPU {avg_cpu:.1f}%, Memory {avg_memory:.1f}%'
            })
            
        return {
            'timestamp': time.time(),
            'instance_id': self.instance_id,
            'recommendations': recommendations
        }
```

== ğŸš€ è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè£…

=== Kubernetes Auto Scaling

**HPA (Horizontal Pod Autoscaler) è¨­å®š**
```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kanshichan-hpa
  namespace: kanshichan
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kanshichan-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: kanshichan_active_connections
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼**
```python
# src/services/scaling/auto_scaler.py
import asyncio
import kubernetes
from typing import Dict, List
import yaml

class KubernetesAutoScaler:
    """Kubernetesè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self, namespace: str = 'kanshichan'):
        self.namespace = namespace
        self.k8s_client = kubernetes.client.ApiClient()
        self.apps_v1 = kubernetes.client.AppsV1Api()
        self.custom_metrics = kubernetes.client.CustomObjectsApi()
        
    async def monitor_and_scale(self):
        """ç›£è¦–ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                metrics = await self._collect_cluster_metrics()
                scaling_decision = self._analyze_scaling_needs(metrics)
                
                if scaling_decision['action'] != 'none':
                    await self._execute_scaling(scaling_decision)
                    
                await asyncio.sleep(30)  # 30ç§’é–“éš”ã§ç›£è¦–
                
            except Exception as e:
                logger.error(f"Auto scaling error: {e}")
                await asyncio.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾…æ©Ÿ
                
    async def _collect_cluster_metrics(self) -> Dict:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        # Pod ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        pods = self.apps_v1.list_namespaced_deployment(self.namespace)
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        custom_metrics = await self._get_custom_metrics()
        
        return {
            'pod_count': len(pods.items),
            'average_cpu': custom_metrics.get('avg_cpu', 0),
            'average_memory': custom_metrics.get('avg_memory', 0),
            'total_connections': custom_metrics.get('total_connections', 0),
            'queue_depth': custom_metrics.get('queue_depth', 0)
        }
        
    def _analyze_scaling_needs(self, metrics: Dict) -> Dict:
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…è¦æ€§åˆ†æ"""
        current_pods = metrics['pod_count']
        
        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ¡ä»¶
        if (metrics['average_cpu'] > 70 or 
            metrics['average_memory'] > 80 or
            metrics['total_connections'] > current_pods * 100):
            return {
                'action': 'scale_up',
                'target_pods': min(10, current_pods + 2),
                'reason': 'High resource usage or connections'
            }
            
        # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æ¡ä»¶
        if (metrics['average_cpu'] < 30 and 
            metrics['average_memory'] < 50 and
            metrics['total_connections'] < current_pods * 50):
            return {
                'action': 'scale_down',
                'target_pods': max(2, current_pods - 1),
                'reason': 'Low resource usage'
            }
            
        return {'action': 'none'}
        
    async def _execute_scaling(self, decision: Dict):
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        deployment_name = 'kanshichan-app'
        target_replicas = decision['target_pods']
        
        try:
            # Deploymentæ›´æ–°
            deployment = self.apps_v1.read_namespaced_deployment(
                name=deployment_name, namespace=self.namespace
            )
            
            deployment.spec.replicas = target_replicas
            
            self.apps_v1.patch_namespaced_deployment(
                name=deployment_name,
                namespace=self.namespace,
                body=deployment
            )
            
            logger.info(f"Scaled {deployment_name} to {target_replicas} replicas: {decision['reason']}")
            
        except Exception as e:
            logger.error(f"Scaling execution failed: {e}")
```

== ğŸ“Š ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°

=== ãƒªã‚½ãƒ¼ã‚¹è¦‹ç©ã‚‚ã‚Š

**åŒæ™‚æ¥ç¶šæ•°åˆ¥ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶**
[cols="2,2,2,2,2,2", options="header"]
|===
|æ¥ç¶šæ•° |CPU ã‚³ã‚¢ |ãƒ¡ãƒ¢ãƒª (GB) |GPU ãƒ¡ãƒ¢ãƒª (GB) |ãƒ‡ã‚£ã‚¹ã‚¯ (GB) |ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (Mbps)
|100 |4 |8 |4 |50 |100
|500 |8 |16 |8 |100 |500  
|1000 |16 |32 |16 |200 |1000
|5000 |32 |64 |32 |500 |2000
|10000 |64 |128 |64 |1000 |5000
|===

**AIå‡¦ç†è² è·åˆ¥è¦ä»¶**
```python
# src/utils/capacity_calculator.py
from typing import Dict, NamedTuple
from dataclasses import dataclass

@dataclass
class ResourceRequirement:
    cpu_cores: int
    memory_gb: int
    gpu_memory_gb: int
    storage_gb: int
    network_mbps: int

class CapacityCalculator:
    """ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£è¨ˆç®—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¦ä»¶ï¼ˆ100æ¥ç¶šã‚ãŸã‚Šï¼‰
    BASE_REQUIREMENTS = ResourceRequirement(
        cpu_cores=4,
        memory_gb=8, 
        gpu_memory_gb=4,
        storage_gb=50,
        network_mbps=100
    )
    
    def calculate_requirements(self, 
                             concurrent_users: int,
                             ai_processing_intensity: str = 'medium') -> ResourceRequirement:
        """è¦ä»¶è¨ˆç®—"""
        
        # åŸºæœ¬ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
        scale_factor = concurrent_users / 100
        
        # AIå‡¦ç†å¼·åº¦ã«ã‚ˆã‚‹èª¿æ•´
        intensity_multipliers = {
            'low': 0.7,    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—å¤šç”¨
            'medium': 1.0, # æ¨™æº–è¨­å®š
            'high': 1.5,   # é«˜ç²¾åº¦ãƒ»ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
            'ultra': 2.0   # æœ€é«˜å“è³ª
        }
        
        ai_multiplier = intensity_multipliers.get(ai_processing_intensity, 1.0)
        
        return ResourceRequirement(
            cpu_cores=int(self.BASE_REQUIREMENTS.cpu_cores * scale_factor * ai_multiplier),
            memory_gb=int(self.BASE_REQUIREMENTS.memory_gb * scale_factor * ai_multiplier),
            gpu_memory_gb=int(self.BASE_REQUIREMENTS.gpu_memory_gb * scale_factor * ai_multiplier),
            storage_gb=int(self.BASE_REQUIREMENTS.storage_gb * scale_factor),
            network_mbps=int(self.BASE_REQUIREMENTS.network_mbps * scale_factor)
        )
        
    def estimate_cost(self, requirements: ResourceRequirement, region: str = 'us-east-1') -> Dict[str, float]:
        """ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šï¼ˆAWSæ–™é‡‘ãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # æ™‚é–“ã‚ãŸã‚Šæ–™é‡‘ï¼ˆUSDï¼‰
        pricing = {
            'cpu_per_core_hour': 0.0464,  # c5.largeç›¸å½“
            'memory_per_gb_hour': 0.0116, # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            'gpu_per_gb_hour': 0.90,      # p3.2xlargeç›¸å½“
            'storage_per_gb_month': 0.10, # EBS gp3
            'network_per_gb': 0.09        # ãƒ‡ãƒ¼ã‚¿è»¢é€
        }
        
        monthly_hours = 24 * 30
        
        return {
            'cpu_monthly': requirements.cpu_cores * pricing['cpu_per_core_hour'] * monthly_hours,
            'memory_monthly': requirements.memory_gb * pricing['memory_per_gb_hour'] * monthly_hours,
            'gpu_monthly': requirements.gpu_memory_gb * pricing['gpu_per_gb_hour'] * monthly_hours,
            'storage_monthly': requirements.storage_gb * pricing['storage_per_gb_month'],
            'network_monthly': (requirements.network_mbps * monthly_hours * 3600 / 8 / 1024) * pricing['network_per_gb']
        }
```

== ğŸ”§ å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

=== ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã‚³ãƒ¼ãƒ‰è¨­è¨ˆ

**è¨­è¨ˆåŸå‰‡**
1. **ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹è¨­è¨ˆ**: ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’Redisã§å¤–éƒ¨åŒ–
2. **éåŒæœŸå‡¦ç†**: asyncio/awaitã«ã‚ˆã‚‹ä¸¦è¡Œå‡¦ç†
3. **ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•**: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã«ã‚ˆã‚‹ç–çµåˆ
4. **ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«**: ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã§ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡åŒ–
5. **graceful degradation**: è² è·æ™‚ã®æ©Ÿèƒ½åˆ¶é™

**å®Ÿè£…ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**
```python
# src/core/scalable_service_template.py
import asyncio
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod
import aioredis
import aioboto3

class ScalableService(ABC):
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.redis_pool: Optional[aioredis.ConnectionPool] = None
        self.s3_client = None
        self.metrics_collector = MetricsCollector()
        
    async def initialize(self):
        """éåŒæœŸåˆæœŸåŒ–"""
        # Redisæ¥ç¶šãƒ—ãƒ¼ãƒ«
        self.redis_pool = aioredis.ConnectionPool.from_url(
            self.config_manager.get('redis.cluster_url'),
            max_connections=20
        )
        
        # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        session = aioboto3.Session()
        self.s3_client = await session.client('s3').__aenter__()
        
        await self._initialize_service()
        
    @abstractmethod
    async def _initialize_service(self):
        """ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®åˆæœŸåŒ–"""
        pass
        
    async def process_request(self, request_data: Dict) -> Dict[str, Any]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ä»˜ãï¼‰"""
        start_time = time.time()
        
        try:
            result = await self._process_request_impl(request_data)
            self.metrics_collector.record_success(time.time() - start_time)
            return result
        except Exception as e:
            self.metrics_collector.record_error(time.time() - start_time, str(e))
            raise
            
    @abstractmethod  
    async def _process_request_impl(self, request_data: Dict) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†"""
        pass
        
    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.redis_pool:
            await self.redis_pool.disconnect()
        if self.s3_client:
            await self.s3_client.__aexit__(None, None, None)
```

=== è¨­å®šç®¡ç†

**ç’°å¢ƒåˆ¥è¨­å®š**
```yaml
# config/scaling.yaml
development:
  scaling:
    enabled: false
    max_instances: 1
    
  cache:
    type: local
    redis_cluster: false
    
  database:
    read_replicas: 0
    sharding: false

production:
  scaling:
    enabled: true
    max_instances: 10
    min_instances: 2
    target_cpu_utilization: 70
    target_memory_utilization: 80
    
  cache:
    type: distributed
    redis_cluster: true
    cluster_nodes:
      - "redis-1.internal:6379"
      - "redis-2.internal:6379" 
      - "redis-3.internal:6379"
      
  database:
    read_replicas: 3
    sharding: true
    shard_configs:
      - id: "shard-1"
        url: "postgresql://user:pass@shard1.db:5432/kanshichan"
        weight: 1.0
      - id: "shard-2"
        url: "postgresql://user:pass@shard2.db:5432/kanshichan"
        weight: 1.0
```

== ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

=== Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
```json
{
  "dashboard": {
    "title": "KanshiChan Scalability Metrics",
    "panels": [
      {
        "title": "System Resources",
        "type": "stat",
        "targets": [
          {
            "expr": "avg(kanshichan_cpu_usage)",
            "legendFormat": "CPU Usage %" 
          },
          {
            "expr": "avg(kanshichan_memory_usage)", 
            "legendFormat": "Memory Usage %"
          }
        ]
      },
      {
        "title": "Active Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(kanshichan_active_connections)",
            "legendFormat": "Total Connections"
          }
        ]
      },
      {
        "title": "Response Time Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, kanshichan_response_time)",
            "legendFormat": "95th percentile"
          }
        ]
      }
    ]
  }
}
```

=== ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

**Prometheus ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«**
```yaml
# alerts/scalability.yaml
groups:
- name: kanshichan-scalability
  rules:
  - alert: HighCPUUsage
    expr: avg(kanshichan_cpu_usage) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is {{ $value }}% for 5 minutes"
      
  - alert: HighMemoryUsage 
    expr: avg(kanshichan_memory_usage) > 85
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is {{ $value }}% for 5 minutes"
      
  - alert: TooManyConnections
    expr: sum(kanshichan_active_connections) > 1000
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High connection count"
      description: "Active connections: {{ $value }}"
      
  - alert: SlowResponseTime
    expr: histogram_quantile(0.95, kanshichan_response_time) > 2
    for: 3m
    labels:
      severity: critical
    annotations:
      summary: "Slow response time"
      description: "95th percentile response time: {{ $value }}s"
```

== ğŸ¯ ã¾ã¨ã‚

KanshiChanã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¨­è¨ˆã¯ä»¥ä¸‹ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

=== ä¸»è¦å®Ÿè£…é …ç›®

* âœ… **æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: Docker Swarm/Kuberneteså¯¾å¿œ
* âœ… **å‚ç›´ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: å‹•çš„ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´  
* âœ… **AIå‡¦ç†åˆ†æ•£**: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ã‚ˆã‚‹è² è·åˆ†æ•£
* âœ… **éšå±¤åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: L1(ãƒ¡ãƒ¢ãƒª) + L2(Redis)å®Ÿè£…
* âœ… **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ•£**: Read Replica + Sharding
* âœ… **è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: Kubernetes HPA + ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼

=== æ€§èƒ½ç›®æ¨™

[cols="2,2,2", options="header"]
|===
|é …ç›® |ç›®æ¨™å€¤ |å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½
|åŒæ™‚æ¥ç¶šæ•° |1000+ |âœ… ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ + è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
|ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ |< 2ç§’ (95%ile) |âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° + AIæœ€é©åŒ–
|å¯ç”¨æ€§ |99.9% |âœ… å†—é•·æ§‹æˆ + ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
|ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ™‚é–“ |< 60ç§’ |âœ… ã‚³ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ + HPA
|===

=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: 1000åŒæ™‚æ¥ç¶šã§ã®æ€§èƒ½æ¤œè¨¼
2. **ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰**: Prometheus + Grafanaè¨­å®š
3. **ç½å®³å¾©æ—§è¨ˆç”»**: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ‰‹é †ç­–å®š
4. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–**: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã®ç¶™ç¶šçš„æœ€é©åŒ–

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber} 