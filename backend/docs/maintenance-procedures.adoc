= ğŸ”§ ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) ä¿å®ˆæ‰‹é †æ›¸
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 3
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:author: KanshiChan Development Team
:email: team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

[NOTE]
====
ğŸ“‹ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±**

* **ä½œæˆè€…**: KanshiChan Development Team
* **æœ€çµ‚æ›´æ–°æ—¥**: {docdate}
* **å¯¾è±¡èª­è€…**: é‹ç”¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ä¿å®ˆæ‹…å½“è€…ã€ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…
* **å‰æçŸ¥è­˜**: ã‚·ã‚¹ãƒ†ãƒ é‹ç”¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ‰‹é †
* **é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<operations-monitoring.adoc>>, <<troubleshooting-guide.adoc>>, <<configuration-guide.adoc>>
====

== ğŸ“– æ¦‚è¦

ç›£è¦–ã¡ã‚ƒã‚“ï¼ˆKanshiChanï¼‰ã‚·ã‚¹ãƒ†ãƒ ã®ä¿å®ˆæ¥­å‹™ã«ãŠã‘ã‚‹æ¨™æº–æ‰‹é †æ›¸ã§ã™ã€‚
å®šæœŸä¿å®ˆã€ç·Šæ€¥å¯¾å¿œã€ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã€ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®åŒ…æ‹¬çš„ãªæ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚

=== ğŸ¯ ä¿å®ˆç›®æ¨™

* **ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§**: 99.9%ã®ç¨¼åƒç‡ç¶­æŒ
* **ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§**: ãƒ‡ãƒ¼ã‚¿æå¤±ã‚¼ãƒ­
* **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆäºˆé˜²
* **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: æœ€é©ãªæ€§èƒ½ã®ç¶­æŒ

== ğŸ—ï¸ ä¿å®ˆä½“åˆ¶

=== ä¿å®ˆçµ„ç¹”å›³

[mermaid]
....
graph TB
    subgraph "ä¿å®ˆè²¬ä»»ä½“åˆ¶"
        MANAGER[ä¿å®ˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼<br/>å…¨ä½“çµ±æ‹¬ãƒ»æ„æ€æ±ºå®š]
        LEAD[ä¿å®ˆãƒªãƒ¼ãƒ€ãƒ¼<br/>æŠ€è¡“æŒ‡å°ãƒ»ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
        
        subgraph "ä¿å®ˆãƒãƒ¼ãƒ "
            SYS[ã‚·ã‚¹ãƒ†ãƒ ä¿å®ˆæ‹…å½“<br/>æ—¥å¸¸ä¿å®ˆãƒ»ç›£è¦–]
            DB[ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å®ˆæ‹…å½“<br/>DBæœ€é©åŒ–ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—]
            SEC[ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¿å®ˆæ‹…å½“<br/>ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ãƒ»å¯¾ç­–]
            AI[AIä¿å®ˆæ‹…å½“<br/>ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ»æ€§èƒ½èª¿æ•´]
        end
        
        subgraph "ã‚µãƒãƒ¼ãƒˆä½“åˆ¶"
            DEV[é–‹ç™ºãƒãƒ¼ãƒ <br/>æŠ€è¡“ã‚µãƒãƒ¼ãƒˆãƒ»ãƒã‚°ä¿®æ­£]
            INFRA[ã‚¤ãƒ³ãƒ•ãƒ©ãƒãƒ¼ãƒ <br/>ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ãƒ»æ‹¡å¼µ]
            VENDOR[ãƒ™ãƒ³ãƒ€ãƒ¼ã‚µãƒãƒ¼ãƒˆ<br/>å¤–éƒ¨æŠ€è¡“ã‚µãƒãƒ¼ãƒˆ]
        end
    end
    
    MANAGER --> LEAD
    LEAD --> SYS
    LEAD --> DB
    LEAD --> SEC
    LEAD --> AI
    
    SYS -.-> DEV
    DB -.-> INFRA
    SEC -.-> VENDOR
    AI -.-> DEV
....

=== ä¿å®ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

**å®šæœŸä¿å®ˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼**
[cols="2,3,2,1", options="header"]
|===
|é »åº¦ |ä¿å®ˆé …ç›® |æ‹…å½“è€… |æ‰€è¦æ™‚é–“
|**æ¯æ—¥** |ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ»ãƒ­ã‚°ç¢ºèª |ã‚·ã‚¹ãƒ†ãƒ ä¿å®ˆæ‹…å½“ |1æ™‚é–“
|**æ¯æ—¥** |ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèªãƒ»æ¤œè¨¼ |ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å®ˆæ‹…å½“ |30åˆ†
|**é€±æ¬¡** |ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ»æœ€é©åŒ– |ã‚·ã‚¹ãƒ†ãƒ ä¿å®ˆæ‹…å½“ |2æ™‚é–“
|**é€±æ¬¡** |ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ãƒ»åˆ†æ |ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¿å®ˆæ‹…å½“ |3æ™‚é–“
|**æœˆæ¬¡** |AI ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ãƒ»èª¿æ•´ |AIä¿å®ˆæ‹…å½“ |4æ™‚é–“
|**æœˆæ¬¡** |å®¹é‡è¨ˆç”»ãƒ»æ‹¡å¼µè¨ˆç”»è¦‹ç›´ã— |ä¿å®ˆãƒªãƒ¼ãƒ€ãƒ¼ |2æ™‚é–“
|**å››åŠæœŸ** |ç·åˆã‚·ã‚¹ãƒ†ãƒ ç›£æŸ» |å…¨ãƒãƒ¼ãƒ  |1æ—¥
|**å¹´æ¬¡** |ç½å®³å¾©æ—§è¨“ç·´ãƒ»ã‚·ã‚¹ãƒ†ãƒ æ›´æ–° |å…¨ãƒãƒ¼ãƒ  |2æ—¥
|===

== ğŸ“… å®šæœŸä¿å®ˆé …ç›®

=== æ—¥æ¬¡ä¿å®ˆæ‰‹é †

**1. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª**
```bash
#!/bin/bash
# daily_system_check.sh

echo "=== KanshiChan Daily System Check $(date) ==="

# 1. ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
echo "1. Service Status Check"
systemctl status kanshichan-app
systemctl status kanshichan-ai
systemctl status redis
systemctl status postgresql

# 2. ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
echo "2. Process Check"
ps aux | grep -E "(kanshichan|redis|postgres)" | grep -v grep

# 3. ãƒãƒ¼ãƒˆç¢ºèª
echo "3. Port Check"
netstat -tlnp | grep -E "(8000|6379|5432)"

# 4. ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª
echo "4. Disk Usage Check"
df -h | grep -E "(/|/var|/tmp)"

# 5. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
echo "5. Memory Usage Check"
free -h

# 6. ãƒ­ã‚°ã‚¨ãƒ©ãƒ¼ç¢ºèª
echo "6. Recent Error Check"
tail -100 /var/log/kanshichan/app.log | grep -i error | tail -10

# 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
echo "7. Performance Check"
curl -s http://localhost:8000/health | jq '.'

echo "=== Daily Check Completed ==="
```

**2. ãƒ­ã‚°ç¢ºèªãƒ»åˆ†æ**
```python
# scripts/daily_log_analysis.py
import json
import re
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import subprocess

class DailyLogAnalyzer:
    """æ—¥æ¬¡ãƒ­ã‚°åˆ†æ"""
    
    def __init__(self):
        self.log_file = "/var/log/kanshichan/app.log"
        self.error_patterns = [
            r"ERROR.*Exception",
            r"ERROR.*Failed",
            r"CRITICAL.*",
            r"WARNING.*timeout",
            r"WARNING.*memory"
        ]
        
    def analyze_daily_logs(self) -> dict:
        """æ—¥æ¬¡ãƒ­ã‚°åˆ†æå®Ÿè¡Œ"""
        yesterday = datetime.now() - timedelta(days=1)
        
        # ãƒ­ã‚°èª­ã¿è¾¼ã¿
        log_entries = self._read_daily_logs(yesterday)
        
        analysis = {
            'date': yesterday.strftime('%Y-%m-%d'),
            'total_entries': len(log_entries),
            'error_analysis': self._analyze_errors(log_entries),
            'performance_analysis': self._analyze_performance(log_entries),
            'user_activity': self._analyze_user_activity(log_entries),
            'recommendations': []
        }
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        return analysis
        
    def _analyze_errors(self, log_entries: list) -> dict:
        """ã‚¨ãƒ©ãƒ¼åˆ†æ"""
        errors = [entry for entry in log_entries if 'ERROR' in entry or 'CRITICAL' in entry]
        
        error_types = Counter()
        error_modules = Counter()
        
        for error in errors:
            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ†é¡
            if 'database' in error.lower():
                error_types['database'] += 1
            elif 'ai' in error.lower() or 'model' in error.lower():
                error_types['ai_processing'] += 1
            elif 'memory' in error.lower():
                error_types['memory'] += 1
            elif 'network' in error.lower():
                error_types['network'] += 1
            else:
                error_types['other'] += 1
                
            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æ
            module_match = re.search(r'"module":\s*"([^"]+)"', error)
            if module_match:
                error_modules[module_match.group(1)] += 1
                
        return {
            'total_errors': len(errors),
            'error_types': dict(error_types),
            'error_modules': dict(error_modules),
            'critical_errors': len([e for e in errors if 'CRITICAL' in e])
        }
        
    def generate_daily_report(self, analysis: dict) -> str:
        """æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
=== KanshiChan Daily Maintenance Report ===
Date: {analysis['date']}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

SUMMARY:
- Total log entries: {analysis['total_entries']:,}
- Total errors: {analysis['error_analysis']['total_errors']}
- Critical errors: {analysis['error_analysis']['critical_errors']}

ERROR ANALYSIS:
"""
        for error_type, count in analysis['error_analysis']['error_types'].items():
            report += f"- {error_type}: {count}\n"
            
        if analysis['recommendations']:
            report += "\nRECOMMENDATIONS:\n"
            for i, rec in enumerate(analysis['recommendations'], 1):
                report += f"{i}. {rec}\n"
                
        return report

def run_daily_maintenance():
    """æ—¥æ¬¡ä¿å®ˆå®Ÿè¡Œ"""
    analyzer = DailyLogAnalyzer()
    analysis = analyzer.analyze_daily_logs()
    report = analyzer.generate_daily_report(analysis)
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = f"/var/log/kanshichan/daily_reports/{analysis['date']}.txt"
    with open(report_file, 'w') as f:
        f.write(report)
        
    print(report)
    
    # é‡è¦ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    if analysis['error_analysis']['critical_errors'] > 0:
        send_alert("Critical errors detected", report)

if __name__ == "__main__":
    run_daily_maintenance()
```

=== é€±æ¬¡ä¿å®ˆæ‰‹é †

**1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
```python
# scripts/weekly_optimization.py
import psutil
import subprocess
import json
from datetime import datetime, timedelta

class WeeklyOptimizer:
    """é€±æ¬¡æœ€é©åŒ–"""
    
    def __init__(self):
        self.optimization_tasks = [
            self.optimize_database,
            self.cleanup_cache,
            self.optimize_ai_models,
            self.analyze_performance_trends
        ]
        
    def run_weekly_optimization(self):
        """é€±æ¬¡æœ€é©åŒ–å®Ÿè¡Œ"""
        results = []
        
        for task in self.optimization_tasks:
            try:
                result = task()
                results.append({
                    'task': task.__name__,
                    'status': 'success',
                    'result': result
                })
            except Exception as e:
                results.append({
                    'task': task.__name__,
                    'status': 'failed',
                    'error': str(e)
                })
                
        self._generate_optimization_report(results)
        return results
        
    def optimize_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–"""
        optimizations = []
        
        # çµ±è¨ˆæƒ…å ±æ›´æ–°
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", "ANALYZE;"
        ], check=True)
        optimizations.append("Updated database statistics")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", "REINDEX DATABASE kanshichan;"
        ], check=True)
        optimizations.append("Rebuilt database indexes")
        
        # ä¸è¦ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        week_ago = datetime.now() - timedelta(days=7)
        cleanup_query = f"""
        DELETE FROM behavior_logs 
        WHERE created_at < '{week_ago}' 
        AND archived = false;
        """
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", cleanup_query
        ], check=True)
        optimizations.append("Cleaned up old behavior logs")
        
        return optimizations
        
    def cleanup_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        cleanups = []
        
        # Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        subprocess.run(["redis-cli", "FLUSHDB"], check=True)
        cleanups.append("Flushed Redis cache")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cache_dirs = [
            "/tmp/kanshichan/",
            "/var/cache/kanshichan/",
            "/data/kanshichan/cache/"
        ]
        
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):
                subprocess.run(["find", cache_dir, "-type", "f", "-mtime", "+3", "-delete"])
                cleanups.append(f"Cleaned {cache_dir}")
                
        return cleanups
        
    def optimize_ai_models(self):
        """AI ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
        optimizations = []
        
        # ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        model_cache_path = "/data/kanshichan/models/cache/"
        if os.path.exists(model_cache_path):
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            subprocess.run([
                "find", model_cache_path, "-name", "*.cache", 
                "-mtime", "+7", "-delete"
            ])
            optimizations.append("Cleaned AI model cache")
            
        # GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        try:
            subprocess.run(["nvidia-smi", "--gpu-reset"], check=True)
            optimizations.append("Reset GPU memory")
        except subprocess.CalledProcessError:
            optimizations.append("GPU reset skipped (not available)")
            
        return optimizations
```

**2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»**
```bash
#!/bin/bash
# weekly_security_audit.sh

echo "=== Weekly Security Audit $(date) ==="

# 1. å¤±æ•—ã—ãŸãƒ­ã‚°ã‚¤ãƒ³è©¦è¡Œç¢ºèª
echo "1. Failed Login Attempts"
grep "authentication failed" /var/log/kanshichan/auth.log | tail -20

# 2. ç•°å¸¸ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
echo "2. Unusual Access Patterns"
awk '{print $1}' /var/log/kanshichan/access.log | sort | uniq -c | sort -nr | head -10

# 3. ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ç¢ºèª
echo "3. File Permissions Check"
find /opt/kanshichan -type f -perm /o+w -ls

# 4. ãƒãƒ¼ãƒˆã‚¹ã‚­ãƒ£ãƒ³çµæœ
echo "4. Open Ports Check"
nmap -sT localhost

# 5. SSL/TLS è¨¼æ˜æ›¸ç¢ºèª
echo "5. SSL Certificate Check"
openssl x509 -in /etc/ssl/certs/kanshichan.crt -text -noout | grep -A 2 "Validity"

# 6. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ›´æ–°ç¢ºèª
echo "6. Security Updates Check"
apt list --upgradable | grep -i security

echo "=== Security Audit Completed ==="
```

== ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ‰‹é †

=== ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—éšå±¤**
[mermaid]
....
graph TB
    subgraph "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¬ãƒ™ãƒ«"
        FULL[Full Backup<br/>å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—<br/>é€±æ¬¡å®Ÿè¡Œ]
        INCR[Incremental Backup<br/>å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—<br/>æ—¥æ¬¡å®Ÿè¡Œ]
        DIFF[Differential Backup<br/>å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—<br/>æ—¥æ¬¡å®Ÿè¡Œ]
    end
    
    subgraph "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡"
        DB[(Database<br/>PostgreSQL)]
        FILES[Application Files<br/>è¨­å®šãƒ»ãƒ­ã‚°]
        AI[AI Models<br/>å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«]
        USER[User Data<br/>è¡Œå‹•ãƒ­ã‚°ãƒ»è¨­å®š]
    end
    
    subgraph "ä¿å­˜å ´æ‰€"
        LOCAL[Local Storage<br/>é«˜é€Ÿå¾©æ—§ç”¨]
        REMOTE[Remote Storage<br/>S3/GCS]
        OFFSITE[Offsite Backup<br/>ç½å®³å¯¾ç­–ç”¨]
    end
    
    FULL --> DB
    FULL --> FILES
    FULL --> AI
    FULL --> USER
    
    INCR --> DB
    INCR --> USER
    
    DIFF --> FILES
    DIFF --> USER
    
    DB --> LOCAL
    FILES --> LOCAL
    AI --> REMOTE
    USER --> REMOTE
    
    LOCAL --> OFFSITE
    REMOTE --> OFFSITE
....

**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```bash
#!/bin/bash
# backup_kanshichan.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/kanshichan"
REMOTE_BACKUP="s3://kanshichan-backups"

echo "=== KanshiChan Backup Started: $BACKUP_DATE ==="

# 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "1. Database Backup"
pg_dump kanshichan | gzip > "$BACKUP_DIR/db_$BACKUP_DATE.sql.gz"

# 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "2. Application Files Backup"
tar -czf "$BACKUP_DIR/app_$BACKUP_DATE.tar.gz" \
    /opt/kanshichan/config/ \
    /opt/kanshichan/logs/ \
    /opt/kanshichan/data/

# 3. AI ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "3. AI Models Backup"
tar -czf "$BACKUP_DIR/models_$BACKUP_DATE.tar.gz" \
    /opt/kanshichan/models/

# 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "4. User Data Backup"
tar -czf "$BACKUP_DIR/userdata_$BACKUP_DATE.tar.gz" \
    /data/kanshichan/behavior_logs/ \
    /data/kanshichan/user_profiles/

# 5. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "5. Configuration Backup"
tar -czf "$BACKUP_DIR/config_$BACKUP_DATE.tar.gz" \
    /etc/kanshichan/ \
    /etc/nginx/sites-available/kanshichan \
    /etc/systemd/system/kanshichan*

# 6. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
echo "6. Backup Verification"
for file in "$BACKUP_DIR"/*_$BACKUP_DATE.*; do
    if [ -f "$file" ]; then
        echo "âœ“ $(basename $file): $(du -h $file | cut -f1)"
    else
        echo "âœ— Missing: $(basename $file)"
    fi
done

# 7. ãƒªãƒ¢ãƒ¼ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "7. Remote Backup Upload"
aws s3 sync "$BACKUP_DIR" "$REMOTE_BACKUP/$(date +%Y/%m/%d)/"

# 8. å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
echo "8. Cleanup Old Backups"
find "$BACKUP_DIR" -name "*_*.gz" -mtime +30 -delete
find "$BACKUP_DIR" -name "*_*.tar.gz" -mtime +30 -delete

echo "=== Backup Completed: $BACKUP_DATE ==="
```

=== å¾©æ—§æ‰‹é †

**ç½å®³å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹**
```python
# scripts/disaster_recovery.py
import subprocess
import os
import sys
from datetime import datetime
import argparse

class DisasterRecovery:
    """ç½å®³å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, backup_dir="/backup/kanshichan"):
        self.backup_dir = backup_dir
        self.recovery_log = []
        
    def full_system_recovery(self, backup_date: str):
        """å®Œå…¨ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§"""
        print(f"Starting full system recovery from {backup_date}")
        
        recovery_steps = [
            ("stop_services", "Stop all KanshiChan services"),
            ("restore_database", "Restore database"),
            ("restore_application", "Restore application files"),
            ("restore_ai_models", "Restore AI models"),
            ("restore_user_data", "Restore user data"),
            ("restore_configuration", "Restore configuration"),
            ("verify_restoration", "Verify restoration"),
            ("start_services", "Start services"),
            ("health_check", "System health check")
        ]
        
        for step_func, description in recovery_steps:
            try:
                print(f"Executing: {description}")
                result = getattr(self, step_func)(backup_date)
                self.recovery_log.append({
                    'step': step_func,
                    'status': 'success',
                    'result': result
                })
                print(f"âœ“ {description} completed")
            except Exception as e:
                error_msg = f"âœ— {description} failed: {str(e)}"
                print(error_msg)
                self.recovery_log.append({
                    'step': step_func,
                    'status': 'failed',
                    'error': str(e)
                })
                raise Exception(f"Recovery failed at step: {step_func}")
                
        self._generate_recovery_report()
        print("Full system recovery completed successfully")
        
    def stop_services(self, backup_date: str):
        """ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢"""
        services = [
            "kanshichan-app",
            "kanshichan-ai", 
            "nginx",
            "redis",
            "postgresql"
        ]
        
        for service in services:
            subprocess.run(["systemctl", "stop", service], check=True)
            
        return f"Stopped {len(services)} services"
        
    def restore_database(self, backup_date: str):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§"""
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        db_backup = f"{self.backup_dir}/db_{backup_date}.sql.gz"
        if not os.path.exists(db_backup):
            raise FileNotFoundError(f"Database backup not found: {db_backup}")
            
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§
        subprocess.run([
            "bash", "-c", 
            f"zcat {db_backup} | psql kanshichan"
        ], check=True)
        
        return f"Database restored from {db_backup}"
        
    def restore_application(self, backup_date: str):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©æ—§"""
        app_backup = f"{self.backup_dir}/app_{backup_date}.tar.gz"
        if not os.path.exists(app_backup):
            raise FileNotFoundError(f"Application backup not found: {app_backup}")
            
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§
        subprocess.run([
            "tar", "-xzf", app_backup, "-C", "/"
        ], check=True)
        
        return f"Application files restored from {app_backup}"
        
    def verify_restoration(self, backup_date: str):
        """å¾©æ—§æ¤œè¨¼"""
        verification_results = []
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
        try:
            subprocess.run([
                "psql", "-d", "kanshichan", "-c", "SELECT COUNT(*) FROM behavior_logs;"
            ], check=True, capture_output=True)
            verification_results.append("Database: OK")
        except:
            verification_results.append("Database: FAILED")
            
        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        critical_files = [
            "/opt/kanshichan/config/config.yaml",
            "/opt/kanshichan/models/yolo.pt",
            "/etc/kanshichan/app.conf"
        ]
        
        for file_path in critical_files:
            if os.path.exists(file_path):
                verification_results.append(f"File {file_path}: OK")
            else:
                verification_results.append(f"File {file_path}: MISSING")
                
        return verification_results
        
    def start_services(self, backup_date: str):
        """ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹"""
        services = [
            "postgresql",
            "redis", 
            "kanshichan-app",
            "kanshichan-ai",
            "nginx"
        ]
        
        for service in services:
            subprocess.run(["systemctl", "start", service], check=True)
            subprocess.run(["systemctl", "enable", service], check=True)
            
        return f"Started {len(services)} services"

def main():
    parser = argparse.ArgumentParser(description='KanshiChan Disaster Recovery')
    parser.add_argument('--backup-date', required=True, 
                       help='Backup date in YYYYMMDD_HHMMSS format')
    parser.add_argument('--backup-dir', default='/backup/kanshichan',
                       help='Backup directory path')
    
    args = parser.parse_args()
    
    recovery = DisasterRecovery(args.backup_dir)
    
    try:
        recovery.full_system_recovery(args.backup_date)
        print("Disaster recovery completed successfully")
        sys.exit(0)
    except Exception as e:
        print(f"Disaster recovery failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

== ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

=== ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç›£è¦–

**æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```python
# scripts/data_integrity_check.py
import psycopg2
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any

class DataIntegrityChecker:
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚«ãƒ¼"""
    
    def __init__(self, db_config: Dict[str, str]):
        self.db_config = db_config
        self.integrity_checks = [
            self.check_referential_integrity,
            self.check_data_consistency,
            self.check_orphaned_records,
            self.check_duplicate_data,
            self.check_data_corruption
        ]
        
    def run_full_integrity_check(self) -> Dict[str, Any]:
        """å®Œå…¨æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'checks': [],
            'summary': {
                'total_checks': len(self.integrity_checks),
                'passed': 0,
                'failed': 0,
                'warnings': 0
            }
        }
        
        with psycopg2.connect(**self.db_config) as conn:
            for check_func in self.integrity_checks:
                try:
                    check_result = check_func(conn)
                    check_result['status'] = 'passed' if not check_result.get('issues') else 'failed'
                    results['checks'].append(check_result)
                    
                    if check_result['status'] == 'passed':
                        results['summary']['passed'] += 1
                    else:
                        results['summary']['failed'] += 1
                        
                except Exception as e:
                    results['checks'].append({
                        'name': check_func.__name__,
                        'status': 'error',
                        'error': str(e)
                    })
                    results['summary']['failed'] += 1
                    
        return results
        
    def check_referential_integrity(self, conn) -> Dict[str, Any]:
        """å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        cursor = conn.cursor()
        issues = []
        
        # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•åãƒã‚§ãƒƒã‚¯
        integrity_queries = [
            ("behavior_logs_user_fk", 
             "SELECT COUNT(*) FROM behavior_logs bl WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = bl.user_id)"),
            ("analysis_results_user_fk",
             "SELECT COUNT(*) FROM analysis_results ar WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = ar.user_id)"),
            ("user_profiles_user_fk", 
             "SELECT COUNT(*) FROM user_profiles up WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = up.user_id)")
        ]
        
        for check_name, query in integrity_queries:
            cursor.execute(query)
            violation_count = cursor.fetchone()[0]
            
            if violation_count > 0:
                issues.append({
                    'type': 'referential_integrity',
                    'check': check_name,
                    'violation_count': violation_count
                })
                
        return {
            'name': 'referential_integrity',
            'issues': issues,
            'description': 'Foreign key constraint violations'
        }
        
    def check_data_consistency(self, conn) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯"""
        cursor = conn.cursor()
        issues = []
        
        # ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        consistency_checks = [
            ("negative_timestamps", 
             "SELECT COUNT(*) FROM behavior_logs WHERE created_at > NOW()"),
            ("invalid_detection_confidence",
             "SELECT COUNT(*) FROM behavior_logs WHERE confidence < 0 OR confidence > 1"),
            ("null_required_fields",
             "SELECT COUNT(*) FROM users WHERE username IS NULL OR email IS NULL"),
            ("inconsistent_session_times",
             "SELECT COUNT(*) FROM behavior_logs WHERE session_end_time < session_start_time")
        ]
        
        for check_name, query in consistency_checks:
            cursor.execute(query)
            inconsistent_count = cursor.fetchone()[0]
            
            if inconsistent_count > 0:
                issues.append({
                    'type': 'data_consistency',
                    'check': check_name,
                    'inconsistent_count': inconsistent_count
                })
                
        return {
            'name': 'data_consistency',
            'issues': issues,
            'description': 'Data consistency violations'
        }
        
    def check_data_corruption(self, conn) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ç ´æãƒã‚§ãƒƒã‚¯"""
        cursor = conn.cursor()
        issues = []
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
        cursor.execute("""
            SELECT table_name, 
                   COUNT(*) as record_count,
                   MD5(string_agg(CAST(ctid AS text), '' ORDER BY ctid)) as checksum
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            GROUP BY table_name
        """)
        
        table_checksums = cursor.fetchall()
        
        # å‰å›ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã¨æ¯”è¼ƒï¼ˆå®Ÿè£…è¦ï¼‰
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’ãƒã‚§ãƒƒã‚¯
        for table_name, record_count, checksum in table_checksums:
            if record_count == 0 and table_name in ['users', 'behavior_logs']:
                issues.append({
                    'type': 'data_corruption',
                    'table': table_name,
                    'issue': 'empty_critical_table'
                })
                
        return {
            'name': 'data_corruption',
            'issues': issues,
            'description': 'Data corruption detection'
        }

# å®šæœŸå®Ÿè¡Œç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def run_daily_integrity_check():
    """æ—¥æ¬¡æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
    db_config = {
        'host': 'localhost',
        'database': 'kanshichan',
        'user': 'kanshichan_user',
        'password': os.environ.get('DB_PASSWORD')
    }
    
    checker = DataIntegrityChecker(db_config)
    results = checker.run_full_integrity_check()
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_path = f"/var/log/kanshichan/integrity_checks/{datetime.now().strftime('%Y%m%d')}.json"
    with open(report_path, 'w') as f:
        json.dump(results, f, indent=2)
        
    # é‡è¦ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    if results['summary']['failed'] > 0:
        send_integrity_alert(results)
        
    return results

if __name__ == "__main__":
    run_daily_integrity_check()
```

== ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

=== è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¹ã‚¯

**ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼**
```python
# scripts/system_cleanup.py
import os
import shutil
import subprocess
from datetime import datetime, timedelta
import logging

class SystemCleanup:
    """ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    
    def __init__(self):
        self.cleanup_tasks = {
            'daily': [
                self.cleanup_temp_files,
                self.cleanup_log_files,
                self.cleanup_cache_files
            ],
            'weekly': [
                self.cleanup_old_backups,
                self.cleanup_archived_data,
                self.optimize_storage
            ],
            'monthly': [
                self.deep_cleanup,
                self.cleanup_unused_ai_models,
                self.compress_old_logs
            ]
        }
        
    def run_cleanup(self, frequency: str = 'daily'):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        tasks = self.cleanup_tasks.get(frequency, [])
        results = []
        
        for task in tasks:
            try:
                result = task()
                results.append({
                    'task': task.__name__,
                    'status': 'success',
                    'result': result
                })
                logging.info(f"Cleanup task completed: {task.__name__}")
            except Exception as e:
                results.append({
                    'task': task.__name__,
                    'status': 'failed',
                    'error': str(e)
                })
                logging.error(f"Cleanup task failed: {task.__name__}: {e}")
                
        return results
        
    def cleanup_temp_files(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        temp_dirs = [
            "/tmp/kanshichan/",
            "/var/tmp/kanshichan/",
            "/opt/kanshichan/temp/"
        ]
        
        cleaned_size = 0
        cleaned_files = 0
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                for root, dirs, files in os.walk(temp_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        if file_age > timedelta(hours=24):  # 24æ™‚é–“ä»¥ä¸ŠçµŒé
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_size += file_size
                            cleaned_files += 1
                            
        return {
            'cleaned_files': cleaned_files,
            'cleaned_size_mb': cleaned_size / (1024 * 1024)
        }
        
    def cleanup_log_files(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        log_dir = "/var/log/kanshichan/"
        retention_days = 30
        
        cleaned_files = []
        
        if os.path.exists(log_dir):
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            for file in os.listdir(log_dir):
                file_path = os.path.join(log_dir, file)
                
                if os.path.isfile(file_path):
                    file_date = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_date < cutoff_date and not file.endswith('.log'):  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ­ã‚°ã¯ä¿æŒ
                        os.remove(file_path)
                        cleaned_files.append(file)
                        
        return {
            'cleaned_log_files': len(cleaned_files),
            'retention_days': retention_days
        }
        
    def cleanup_old_backups(self):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        backup_dir = "/backup/kanshichan/"
        retention_days = 90  # 90æ—¥ä¿æŒ
        
        if not os.path.exists(backup_dir):
            return {'status': 'backup_dir_not_found'}
            
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        cleaned_backups = []
        
        for backup_file in os.listdir(backup_dir):
            backup_path = os.path.join(backup_dir, backup_file)
            
            if os.path.isfile(backup_path):
                backup_date = datetime.fromtimestamp(os.path.getmtime(backup_path))
                
                if backup_date < cutoff_date:
                    os.remove(backup_path)
                    cleaned_backups.append(backup_file)
                    
        return {
            'cleaned_backups': len(cleaned_backups),
            'retention_days': retention_days
        }

# å®šæœŸå®Ÿè¡Œç”¨è¨­å®š
def schedule_cleanup_tasks():
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
    import schedule
    
    cleanup = SystemCleanup()
    
    # æ—¥æ¬¡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆåˆå‰2æ™‚ï¼‰
    schedule.every().day.at("02:00").do(
        lambda: cleanup.run_cleanup('daily')
    )
    
    # é€±æ¬¡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæ—¥æ›œåˆå‰3æ™‚ï¼‰
    schedule.every().sunday.at("03:00").do(
        lambda: cleanup.run_cleanup('weekly')
    )
    
    # æœˆæ¬¡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæœˆåˆåˆå‰1æ™‚ï¼‰
    schedule.every().month.do(
        lambda: cleanup.run_cleanup('monthly')
    )
    
    return schedule

if __name__ == "__main__":
    cleanup = SystemCleanup()
    results = cleanup.run_cleanup('daily')
    print(f"Cleanup completed: {results}")
```

== ğŸ¯ ã¾ã¨ã‚

KanshiChanã®ä¿å®ˆæ‰‹é †ã¯ä»¥ä¸‹ã®ä½“ç³»çš„ãƒ—ãƒ­ã‚»ã‚¹ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

=== ä¸»è¦ä¿å®ˆãƒ—ãƒ­ã‚»ã‚¹

* âœ… **å®šæœŸä¿å®ˆ**: æ—¥æ¬¡/é€±æ¬¡/æœˆæ¬¡ã®è‡ªå‹•åŒ–ã•ã‚ŒãŸä¿å®ˆã‚¿ã‚¹ã‚¯
* âœ… **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§**: å¤šå±¤ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã¨ç½å®³å¾©æ—§æ‰‹é †
* âœ… **ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§**: åŒ…æ‹¬çš„ãªæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¨ä¿®å¾©æ‰‹é †
* âœ… **ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—**: è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¹ã‚¯
* âœ… **ç›£æŸ»ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ**: å®šæœŸçš„ãªä¿å®ˆçŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

=== ä¿å®ˆåŠ¹ç‡åŒ–

[cols="2,2,2", options="header"]
|===
|ä¿å®ˆé …ç›® |è‡ªå‹•åŒ–ãƒ¬ãƒ™ãƒ« |åŠ¹æœ
|æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯ |100% è‡ªå‹• |âœ… äººçš„ã‚¨ãƒ©ãƒ¼å‰Šæ¸›
|ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— |100% è‡ªå‹• |âœ… ç¢ºå®Ÿãªãƒ‡ãƒ¼ã‚¿ä¿è­·
|æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ |90% è‡ªå‹• |âœ… æ—©æœŸå•é¡Œç™ºè¦‹
|ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— |100% è‡ªå‹• |âœ… ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
|===

=== ä¿å®ˆå“è³ªç®¡ç†

1. **æ¨™æº–åŒ–**: å…¨ä¿å®ˆä½œæ¥­ã®æ‰‹é †æ¨™æº–åŒ–
2. **è‡ªå‹•åŒ–**: å¯èƒ½ãªé™ã‚Šã®ä½œæ¥­è‡ªå‹•åŒ–
3. **ç›£æŸ»**: ä¿å®ˆä½œæ¥­ã®å®Œå…¨ãªãƒ­ã‚°è¨˜éŒ²
4. **ç¶™ç¶šæ”¹å–„**: å®šæœŸçš„ãªæ‰‹é †è¦‹ç›´ã—ã¨æœ€é©åŒ–

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber}