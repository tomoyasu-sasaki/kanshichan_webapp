= 🔧 監視ちゃん(KanshiChan) 保守手順書
:toc: left
:toc-title: 目次
:toclevels: 3
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:author: KanshiChan Development Team
:email: team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

[NOTE]
====
📋 **ドキュメント情報**

* **作成者**: KanshiChan Development Team
* **最終更新日**: {docdate}
* **対象読者**: 運用エンジニア、保守担当者、システム管理者
* **前提知識**: システム運用、データベース管理、バックアップ・復旧手順
* **関連ドキュメント**: <<operations-monitoring.adoc>>, <<troubleshooting-guide.adoc>>, <<configuration-guide.adoc>>
====

== 📖 概要

監視ちゃん（KanshiChan）システムの保守業務における標準手順書です。
定期保守、緊急対応、データ管理、システム最適化の包括的な手順を提供します。

=== 🎯 保守目標

* **システム安定性**: 99.9%の稼働率維持
* **データ整合性**: データ損失ゼロ
* **セキュリティ**: セキュリティインシデント予防
* **パフォーマンス**: 最適な性能の維持

== 🏗️ 保守体制

=== 保守組織図

[mermaid]
....
graph TB
    subgraph "保守責任体制"
        MANAGER[保守マネージャー<br/>全体統括・意思決定]
        LEAD[保守リーダー<br/>技術指導・エスカレーション]
        
        subgraph "保守チーム"
            SYS[システム保守担当<br/>日常保守・監視]
            DB[データベース保守担当<br/>DB最適化・バックアップ]
            SEC[セキュリティ保守担当<br/>セキュリティ監査・対策]
            AI[AI保守担当<br/>モデル管理・性能調整]
        end
        
        subgraph "サポート体制"
            DEV[開発チーム<br/>技術サポート・バグ修正]
            INFRA[インフラチーム<br/>インフラ管理・拡張]
            VENDOR[ベンダーサポート<br/>外部技術サポート]
        end
    end
    
    MANAGER --> LEAD
    LEAD --> SYS
    LEAD --> DB
    LEAD --> SEC
    LEAD --> AI
    
    SYS -.-> DEV
    DB -.-> INFRA
    SEC -.-> VENDOR
    AI -.-> DEV
....

=== 保守スケジュール

**定期保守カレンダー**
[cols="2,3,2,1", options="header"]
|===
|頻度 |保守項目 |担当者 |所要時間
|**毎日** |システム監視・ログ確認 |システム保守担当 |1時間
|**毎日** |バックアップ確認・検証 |データベース保守担当 |30分
|**週次** |パフォーマンス分析・最適化 |システム保守担当 |2時間
|**週次** |セキュリティスキャン・分析 |セキュリティ保守担当 |3時間
|**月次** |AI モデル性能評価・調整 |AI保守担当 |4時間
|**月次** |容量計画・拡張計画見直し |保守リーダー |2時間
|**四半期** |総合システム監査 |全チーム |1日
|**年次** |災害復旧訓練・システム更新 |全チーム |2日
|===

== 📅 定期保守項目

=== 日次保守手順

**1. システム状態確認**
```bash
#!/bin/bash
# daily_system_check.sh

echo "=== KanshiChan Daily System Check $(date) ==="

# 1. サービス状態確認
echo "1. Service Status Check"
systemctl status kanshichan-app
systemctl status kanshichan-ai
systemctl status redis
systemctl status postgresql

# 2. プロセス確認
echo "2. Process Check"
ps aux | grep -E "(kanshichan|redis|postgres)" | grep -v grep

# 3. ポート確認
echo "3. Port Check"
netstat -tlnp | grep -E "(8000|6379|5432)"

# 4. ディスク容量確認
echo "4. Disk Usage Check"
df -h | grep -E "(/|/var|/tmp)"

# 5. メモリ使用量確認
echo "5. Memory Usage Check"
free -h

# 6. ログエラー確認
echo "6. Recent Error Check"
tail -100 /var/log/kanshichan/app.log | grep -i error | tail -10

# 7. パフォーマンス確認
echo "7. Performance Check"
curl -s http://localhost:8000/health | jq '.'

echo "=== Daily Check Completed ==="
```

**2. ログ確認・分析**
```python
# scripts/daily_log_analysis.py
import json
import re
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import subprocess

class DailyLogAnalyzer:
    """日次ログ分析"""
    
    def __init__(self):
        self.log_file = "/var/log/kanshichan/app.log"
        self.error_patterns = [
            r"ERROR.*Exception",
            r"ERROR.*Failed",
            r"CRITICAL.*",
            r"WARNING.*timeout",
            r"WARNING.*memory"
        ]
        
    def analyze_daily_logs(self) -> dict:
        """日次ログ分析実行"""
        yesterday = datetime.now() - timedelta(days=1)
        
        # ログ読み込み
        log_entries = self._read_daily_logs(yesterday)
        
        analysis = {
            'date': yesterday.strftime('%Y-%m-%d'),
            'total_entries': len(log_entries),
            'error_analysis': self._analyze_errors(log_entries),
            'performance_analysis': self._analyze_performance(log_entries),
            'user_activity': self._analyze_user_activity(log_entries),
            'recommendations': []
        }
        
        # 推奨事項生成
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        return analysis
        
    def _analyze_errors(self, log_entries: list) -> dict:
        """エラー分析"""
        errors = [entry for entry in log_entries if 'ERROR' in entry or 'CRITICAL' in entry]
        
        error_types = Counter()
        error_modules = Counter()
        
        for error in errors:
            # エラータイプ分類
            if 'database' in error.lower():
                error_types['database'] += 1
            elif 'ai' in error.lower() or 'model' in error.lower():
                error_types['ai_processing'] += 1
            elif 'memory' in error.lower():
                error_types['memory'] += 1
            elif 'network' in error.lower():
                error_types['network'] += 1
            else:
                error_types['other'] += 1
                
            # モジュール分析
            module_match = re.search(r'"module":\s*"([^"]+)"', error)
            if module_match:
                error_modules[module_match.group(1)] += 1
                
        return {
            'total_errors': len(errors),
            'error_types': dict(error_types),
            'error_modules': dict(error_modules),
            'critical_errors': len([e for e in errors if 'CRITICAL' in e])
        }
        
    def generate_daily_report(self, analysis: dict) -> str:
        """日次レポート生成"""
        report = f"""
=== KanshiChan Daily Maintenance Report ===
Date: {analysis['date']}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

SUMMARY:
- Total log entries: {analysis['total_entries']:,}
- Total errors: {analysis['error_analysis']['total_errors']}
- Critical errors: {analysis['error_analysis']['critical_errors']}

ERROR ANALYSIS:
"""
        for error_type, count in analysis['error_analysis']['error_types'].items():
            report += f"- {error_type}: {count}\n"
            
        if analysis['recommendations']:
            report += "\nRECOMMENDATIONS:\n"
            for i, rec in enumerate(analysis['recommendations'], 1):
                report += f"{i}. {rec}\n"
                
        return report

def run_daily_maintenance():
    """日次保守実行"""
    analyzer = DailyLogAnalyzer()
    analysis = analyzer.analyze_daily_logs()
    report = analyzer.generate_daily_report(analysis)
    
    # レポート保存
    report_file = f"/var/log/kanshichan/daily_reports/{analysis['date']}.txt"
    with open(report_file, 'w') as f:
        f.write(report)
        
    print(report)
    
    # 重要な問題がある場合はアラート送信
    if analysis['error_analysis']['critical_errors'] > 0:
        send_alert("Critical errors detected", report)

if __name__ == "__main__":
    run_daily_maintenance()
```

=== 週次保守手順

**1. パフォーマンス最適化**
```python
# scripts/weekly_optimization.py
import psutil
import subprocess
import json
from datetime import datetime, timedelta

class WeeklyOptimizer:
    """週次最適化"""
    
    def __init__(self):
        self.optimization_tasks = [
            self.optimize_database,
            self.cleanup_cache,
            self.optimize_ai_models,
            self.analyze_performance_trends
        ]
        
    def run_weekly_optimization(self):
        """週次最適化実行"""
        results = []
        
        for task in self.optimization_tasks:
            try:
                result = task()
                results.append({
                    'task': task.__name__,
                    'status': 'success',
                    'result': result
                })
            except Exception as e:
                results.append({
                    'task': task.__name__,
                    'status': 'failed',
                    'error': str(e)
                })
                
        self._generate_optimization_report(results)
        return results
        
    def optimize_database(self):
        """データベース最適化"""
        optimizations = []
        
        # 統計情報更新
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", "ANALYZE;"
        ], check=True)
        optimizations.append("Updated database statistics")
        
        # インデックス再構築
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", "REINDEX DATABASE kanshichan;"
        ], check=True)
        optimizations.append("Rebuilt database indexes")
        
        # 不要データクリーンアップ
        week_ago = datetime.now() - timedelta(days=7)
        cleanup_query = f"""
        DELETE FROM behavior_logs 
        WHERE created_at < '{week_ago}' 
        AND archived = false;
        """
        subprocess.run([
            "psql", "-d", "kanshichan", "-c", cleanup_query
        ], check=True)
        optimizations.append("Cleaned up old behavior logs")
        
        return optimizations
        
    def cleanup_cache(self):
        """キャッシュクリーンアップ"""
        cleanups = []
        
        # Redis キャッシュ最適化
        subprocess.run(["redis-cli", "FLUSHDB"], check=True)
        cleanups.append("Flushed Redis cache")
        
        # ファイルシステムキャッシュクリーンアップ
        cache_dirs = [
            "/tmp/kanshichan/",
            "/var/cache/kanshichan/",
            "/data/kanshichan/cache/"
        ]
        
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):
                subprocess.run(["find", cache_dir, "-type", "f", "-mtime", "+3", "-delete"])
                cleanups.append(f"Cleaned {cache_dir}")
                
        return cleanups
        
    def optimize_ai_models(self):
        """AI モデル最適化"""
        optimizations = []
        
        # モデルキャッシュ最適化
        model_cache_path = "/data/kanshichan/models/cache/"
        if os.path.exists(model_cache_path):
            # 古いキャッシュファイル削除
            subprocess.run([
                "find", model_cache_path, "-name", "*.cache", 
                "-mtime", "+7", "-delete"
            ])
            optimizations.append("Cleaned AI model cache")
            
        # GPU メモリ最適化
        try:
            subprocess.run(["nvidia-smi", "--gpu-reset"], check=True)
            optimizations.append("Reset GPU memory")
        except subprocess.CalledProcessError:
            optimizations.append("GPU reset skipped (not available)")
            
        return optimizations
```

**2. セキュリティ監査**
```bash
#!/bin/bash
# weekly_security_audit.sh

echo "=== Weekly Security Audit $(date) ==="

# 1. 失敗したログイン試行確認
echo "1. Failed Login Attempts"
grep "authentication failed" /var/log/kanshichan/auth.log | tail -20

# 2. 異常なアクセスパターン確認
echo "2. Unusual Access Patterns"
awk '{print $1}' /var/log/kanshichan/access.log | sort | uniq -c | sort -nr | head -10

# 3. ファイル権限確認
echo "3. File Permissions Check"
find /opt/kanshichan -type f -perm /o+w -ls

# 4. ポートスキャン結果
echo "4. Open Ports Check"
nmap -sT localhost

# 5. SSL/TLS 証明書確認
echo "5. SSL Certificate Check"
openssl x509 -in /etc/ssl/certs/kanshichan.crt -text -noout | grep -A 2 "Validity"

# 6. セキュリティ更新確認
echo "6. Security Updates Check"
apt list --upgradable | grep -i security

echo "=== Security Audit Completed ==="
```

== 💾 バックアップ・復旧手順

=== バックアップ戦略

**バックアップ階層**
[mermaid]
....
graph TB
    subgraph "バックアップレベル"
        FULL[Full Backup<br/>完全バックアップ<br/>週次実行]
        INCR[Incremental Backup<br/>増分バックアップ<br/>日次実行]
        DIFF[Differential Backup<br/>差分バックアップ<br/>日次実行]
    end
    
    subgraph "バックアップ対象"
        DB[(Database<br/>PostgreSQL)]
        FILES[Application Files<br/>設定・ログ]
        AI[AI Models<br/>学習済みモデル]
        USER[User Data<br/>行動ログ・設定]
    end
    
    subgraph "保存場所"
        LOCAL[Local Storage<br/>高速復旧用]
        REMOTE[Remote Storage<br/>S3/GCS]
        OFFSITE[Offsite Backup<br/>災害対策用]
    end
    
    FULL --> DB
    FULL --> FILES
    FULL --> AI
    FULL --> USER
    
    INCR --> DB
    INCR --> USER
    
    DIFF --> FILES
    DIFF --> USER
    
    DB --> LOCAL
    FILES --> LOCAL
    AI --> REMOTE
    USER --> REMOTE
    
    LOCAL --> OFFSITE
    REMOTE --> OFFSITE
....

**バックアップスクリプト**
```bash
#!/bin/bash
# backup_kanshichan.sh

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/kanshichan"
REMOTE_BACKUP="s3://kanshichan-backups"

echo "=== KanshiChan Backup Started: $BACKUP_DATE ==="

# 1. データベースバックアップ
echo "1. Database Backup"
pg_dump kanshichan | gzip > "$BACKUP_DIR/db_$BACKUP_DATE.sql.gz"

# 2. アプリケーションファイルバックアップ
echo "2. Application Files Backup"
tar -czf "$BACKUP_DIR/app_$BACKUP_DATE.tar.gz" \
    /opt/kanshichan/config/ \
    /opt/kanshichan/logs/ \
    /opt/kanshichan/data/

# 3. AI モデルバックアップ
echo "3. AI Models Backup"
tar -czf "$BACKUP_DIR/models_$BACKUP_DATE.tar.gz" \
    /opt/kanshichan/models/

# 4. ユーザーデータバックアップ
echo "4. User Data Backup"
tar -czf "$BACKUP_DIR/userdata_$BACKUP_DATE.tar.gz" \
    /data/kanshichan/behavior_logs/ \
    /data/kanshichan/user_profiles/

# 5. 設定ファイルバックアップ
echo "5. Configuration Backup"
tar -czf "$BACKUP_DIR/config_$BACKUP_DATE.tar.gz" \
    /etc/kanshichan/ \
    /etc/nginx/sites-available/kanshichan \
    /etc/systemd/system/kanshichan*

# 6. バックアップ検証
echo "6. Backup Verification"
for file in "$BACKUP_DIR"/*_$BACKUP_DATE.*; do
    if [ -f "$file" ]; then
        echo "✓ $(basename $file): $(du -h $file | cut -f1)"
    else
        echo "✗ Missing: $(basename $file)"
    fi
done

# 7. リモートバックアップ
echo "7. Remote Backup Upload"
aws s3 sync "$BACKUP_DIR" "$REMOTE_BACKUP/$(date +%Y/%m/%d)/"

# 8. 古いバックアップクリーンアップ
echo "8. Cleanup Old Backups"
find "$BACKUP_DIR" -name "*_*.gz" -mtime +30 -delete
find "$BACKUP_DIR" -name "*_*.tar.gz" -mtime +30 -delete

echo "=== Backup Completed: $BACKUP_DATE ==="
```

=== 復旧手順

**災害復旧プロセス**
```python
# scripts/disaster_recovery.py
import subprocess
import os
import sys
from datetime import datetime
import argparse

class DisasterRecovery:
    """災害復旧システム"""
    
    def __init__(self, backup_dir="/backup/kanshichan"):
        self.backup_dir = backup_dir
        self.recovery_log = []
        
    def full_system_recovery(self, backup_date: str):
        """完全システム復旧"""
        print(f"Starting full system recovery from {backup_date}")
        
        recovery_steps = [
            ("stop_services", "Stop all KanshiChan services"),
            ("restore_database", "Restore database"),
            ("restore_application", "Restore application files"),
            ("restore_ai_models", "Restore AI models"),
            ("restore_user_data", "Restore user data"),
            ("restore_configuration", "Restore configuration"),
            ("verify_restoration", "Verify restoration"),
            ("start_services", "Start services"),
            ("health_check", "System health check")
        ]
        
        for step_func, description in recovery_steps:
            try:
                print(f"Executing: {description}")
                result = getattr(self, step_func)(backup_date)
                self.recovery_log.append({
                    'step': step_func,
                    'status': 'success',
                    'result': result
                })
                print(f"✓ {description} completed")
            except Exception as e:
                error_msg = f"✗ {description} failed: {str(e)}"
                print(error_msg)
                self.recovery_log.append({
                    'step': step_func,
                    'status': 'failed',
                    'error': str(e)
                })
                raise Exception(f"Recovery failed at step: {step_func}")
                
        self._generate_recovery_report()
        print("Full system recovery completed successfully")
        
    def stop_services(self, backup_date: str):
        """サービス停止"""
        services = [
            "kanshichan-app",
            "kanshichan-ai", 
            "nginx",
            "redis",
            "postgresql"
        ]
        
        for service in services:
            subprocess.run(["systemctl", "stop", service], check=True)
            
        return f"Stopped {len(services)} services"
        
    def restore_database(self, backup_date: str):
        """データベース復旧"""
        # バックアップファイル確認
        db_backup = f"{self.backup_dir}/db_{backup_date}.sql.gz"
        if not os.path.exists(db_backup):
            raise FileNotFoundError(f"Database backup not found: {db_backup}")
            
        # データベース復旧
        subprocess.run([
            "bash", "-c", 
            f"zcat {db_backup} | psql kanshichan"
        ], check=True)
        
        return f"Database restored from {db_backup}"
        
    def restore_application(self, backup_date: str):
        """アプリケーション復旧"""
        app_backup = f"{self.backup_dir}/app_{backup_date}.tar.gz"
        if not os.path.exists(app_backup):
            raise FileNotFoundError(f"Application backup not found: {app_backup}")
            
        # アプリケーションファイル復旧
        subprocess.run([
            "tar", "-xzf", app_backup, "-C", "/"
        ], check=True)
        
        return f"Application files restored from {app_backup}"
        
    def verify_restoration(self, backup_date: str):
        """復旧検証"""
        verification_results = []
        
        # データベース接続確認
        try:
            subprocess.run([
                "psql", "-d", "kanshichan", "-c", "SELECT COUNT(*) FROM behavior_logs;"
            ], check=True, capture_output=True)
            verification_results.append("Database: OK")
        except:
            verification_results.append("Database: FAILED")
            
        # ファイル存在確認
        critical_files = [
            "/opt/kanshichan/config/config.yaml",
            "/opt/kanshichan/models/yolo.pt",
            "/etc/kanshichan/app.conf"
        ]
        
        for file_path in critical_files:
            if os.path.exists(file_path):
                verification_results.append(f"File {file_path}: OK")
            else:
                verification_results.append(f"File {file_path}: MISSING")
                
        return verification_results
        
    def start_services(self, backup_date: str):
        """サービス開始"""
        services = [
            "postgresql",
            "redis", 
            "kanshichan-app",
            "kanshichan-ai",
            "nginx"
        ]
        
        for service in services:
            subprocess.run(["systemctl", "start", service], check=True)
            subprocess.run(["systemctl", "enable", service], check=True)
            
        return f"Started {len(services)} services"

def main():
    parser = argparse.ArgumentParser(description='KanshiChan Disaster Recovery')
    parser.add_argument('--backup-date', required=True, 
                       help='Backup date in YYYYMMDD_HHMMSS format')
    parser.add_argument('--backup-dir', default='/backup/kanshichan',
                       help='Backup directory path')
    
    args = parser.parse_args()
    
    recovery = DisasterRecovery(args.backup_dir)
    
    try:
        recovery.full_system_recovery(args.backup_date)
        print("Disaster recovery completed successfully")
        sys.exit(0)
    except Exception as e:
        print(f"Disaster recovery failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

== 📊 データ整合性チェック

=== データ整合性監視

**整合性チェックスクリプト**
```python
# scripts/data_integrity_check.py
import psycopg2
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any

class DataIntegrityChecker:
    """データ整合性チェッカー"""
    
    def __init__(self, db_config: Dict[str, str]):
        self.db_config = db_config
        self.integrity_checks = [
            self.check_referential_integrity,
            self.check_data_consistency,
            self.check_orphaned_records,
            self.check_duplicate_data,
            self.check_data_corruption
        ]
        
    def run_full_integrity_check(self) -> Dict[str, Any]:
        """完全整合性チェック実行"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'checks': [],
            'summary': {
                'total_checks': len(self.integrity_checks),
                'passed': 0,
                'failed': 0,
                'warnings': 0
            }
        }
        
        with psycopg2.connect(**self.db_config) as conn:
            for check_func in self.integrity_checks:
                try:
                    check_result = check_func(conn)
                    check_result['status'] = 'passed' if not check_result.get('issues') else 'failed'
                    results['checks'].append(check_result)
                    
                    if check_result['status'] == 'passed':
                        results['summary']['passed'] += 1
                    else:
                        results['summary']['failed'] += 1
                        
                except Exception as e:
                    results['checks'].append({
                        'name': check_func.__name__,
                        'status': 'error',
                        'error': str(e)
                    })
                    results['summary']['failed'] += 1
                    
        return results
        
    def check_referential_integrity(self, conn) -> Dict[str, Any]:
        """参照整合性チェック"""
        cursor = conn.cursor()
        issues = []
        
        # 外部キー制約違反チェック
        integrity_queries = [
            ("behavior_logs_user_fk", 
             "SELECT COUNT(*) FROM behavior_logs bl WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = bl.user_id)"),
            ("analysis_results_user_fk",
             "SELECT COUNT(*) FROM analysis_results ar WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = ar.user_id)"),
            ("user_profiles_user_fk", 
             "SELECT COUNT(*) FROM user_profiles up WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = up.user_id)")
        ]
        
        for check_name, query in integrity_queries:
            cursor.execute(query)
            violation_count = cursor.fetchone()[0]
            
            if violation_count > 0:
                issues.append({
                    'type': 'referential_integrity',
                    'check': check_name,
                    'violation_count': violation_count
                })
                
        return {
            'name': 'referential_integrity',
            'issues': issues,
            'description': 'Foreign key constraint violations'
        }
        
    def check_data_consistency(self, conn) -> Dict[str, Any]:
        """データ一貫性チェック"""
        cursor = conn.cursor()
        issues = []
        
        # データ一貫性チェック
        consistency_checks = [
            ("negative_timestamps", 
             "SELECT COUNT(*) FROM behavior_logs WHERE created_at > NOW()"),
            ("invalid_detection_confidence",
             "SELECT COUNT(*) FROM behavior_logs WHERE confidence < 0 OR confidence > 1"),
            ("null_required_fields",
             "SELECT COUNT(*) FROM users WHERE username IS NULL OR email IS NULL"),
            ("inconsistent_session_times",
             "SELECT COUNT(*) FROM behavior_logs WHERE session_end_time < session_start_time")
        ]
        
        for check_name, query in consistency_checks:
            cursor.execute(query)
            inconsistent_count = cursor.fetchone()[0]
            
            if inconsistent_count > 0:
                issues.append({
                    'type': 'data_consistency',
                    'check': check_name,
                    'inconsistent_count': inconsistent_count
                })
                
        return {
            'name': 'data_consistency',
            'issues': issues,
            'description': 'Data consistency violations'
        }
        
    def check_data_corruption(self, conn) -> Dict[str, Any]:
        """データ破損チェック"""
        cursor = conn.cursor()
        issues = []
        
        # チェックサム検証
        cursor.execute("""
            SELECT table_name, 
                   COUNT(*) as record_count,
                   MD5(string_agg(CAST(ctid AS text), '' ORDER BY ctid)) as checksum
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            GROUP BY table_name
        """)
        
        table_checksums = cursor.fetchall()
        
        # 前回のチェックサムと比較（実装要）
        # 簡略化のため、レコード数の急激な変化をチェック
        for table_name, record_count, checksum in table_checksums:
            if record_count == 0 and table_name in ['users', 'behavior_logs']:
                issues.append({
                    'type': 'data_corruption',
                    'table': table_name,
                    'issue': 'empty_critical_table'
                })
                
        return {
            'name': 'data_corruption',
            'issues': issues,
            'description': 'Data corruption detection'
        }

# 定期実行用スクリプト
def run_daily_integrity_check():
    """日次整合性チェック実行"""
    db_config = {
        'host': 'localhost',
        'database': 'kanshichan',
        'user': 'kanshichan_user',
        'password': os.environ.get('DB_PASSWORD')
    }
    
    checker = DataIntegrityChecker(db_config)
    results = checker.run_full_integrity_check()
    
    # 結果をファイルに保存
    report_path = f"/var/log/kanshichan/integrity_checks/{datetime.now().strftime('%Y%m%d')}.json"
    with open(report_path, 'w') as f:
        json.dump(results, f, indent=2)
        
    # 重要な問題がある場合はアラート送信
    if results['summary']['failed'] > 0:
        send_integrity_alert(results)
        
    return results

if __name__ == "__main__":
    run_daily_integrity_check()
```

== 🔧 システムクリーンアップ

=== 自動クリーンアップタスク

**クリーンアップスケジューラー**
```python
# scripts/system_cleanup.py
import os
import shutil
import subprocess
from datetime import datetime, timedelta
import logging

class SystemCleanup:
    """システムクリーンアップ"""
    
    def __init__(self):
        self.cleanup_tasks = {
            'daily': [
                self.cleanup_temp_files,
                self.cleanup_log_files,
                self.cleanup_cache_files
            ],
            'weekly': [
                self.cleanup_old_backups,
                self.cleanup_archived_data,
                self.optimize_storage
            ],
            'monthly': [
                self.deep_cleanup,
                self.cleanup_unused_ai_models,
                self.compress_old_logs
            ]
        }
        
    def run_cleanup(self, frequency: str = 'daily'):
        """クリーンアップ実行"""
        tasks = self.cleanup_tasks.get(frequency, [])
        results = []
        
        for task in tasks:
            try:
                result = task()
                results.append({
                    'task': task.__name__,
                    'status': 'success',
                    'result': result
                })
                logging.info(f"Cleanup task completed: {task.__name__}")
            except Exception as e:
                results.append({
                    'task': task.__name__,
                    'status': 'failed',
                    'error': str(e)
                })
                logging.error(f"Cleanup task failed: {task.__name__}: {e}")
                
        return results
        
    def cleanup_temp_files(self):
        """一時ファイルクリーンアップ"""
        temp_dirs = [
            "/tmp/kanshichan/",
            "/var/tmp/kanshichan/",
            "/opt/kanshichan/temp/"
        ]
        
        cleaned_size = 0
        cleaned_files = 0
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                for root, dirs, files in os.walk(temp_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        if file_age > timedelta(hours=24):  # 24時間以上経過
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            cleaned_size += file_size
                            cleaned_files += 1
                            
        return {
            'cleaned_files': cleaned_files,
            'cleaned_size_mb': cleaned_size / (1024 * 1024)
        }
        
    def cleanup_log_files(self):
        """ログファイルクリーンアップ"""
        log_dir = "/var/log/kanshichan/"
        retention_days = 30
        
        cleaned_files = []
        
        if os.path.exists(log_dir):
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            for file in os.listdir(log_dir):
                file_path = os.path.join(log_dir, file)
                
                if os.path.isfile(file_path):
                    file_date = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_date < cutoff_date and not file.endswith('.log'):  # アクティブログは保持
                        os.remove(file_path)
                        cleaned_files.append(file)
                        
        return {
            'cleaned_log_files': len(cleaned_files),
            'retention_days': retention_days
        }
        
    def cleanup_old_backups(self):
        """古いバックアップクリーンアップ"""
        backup_dir = "/backup/kanshichan/"
        retention_days = 90  # 90日保持
        
        if not os.path.exists(backup_dir):
            return {'status': 'backup_dir_not_found'}
            
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        cleaned_backups = []
        
        for backup_file in os.listdir(backup_dir):
            backup_path = os.path.join(backup_dir, backup_file)
            
            if os.path.isfile(backup_path):
                backup_date = datetime.fromtimestamp(os.path.getmtime(backup_path))
                
                if backup_date < cutoff_date:
                    os.remove(backup_path)
                    cleaned_backups.append(backup_file)
                    
        return {
            'cleaned_backups': len(cleaned_backups),
            'retention_days': retention_days
        }

# 定期実行用設定
def schedule_cleanup_tasks():
    """クリーンアップタスクスケジュール設定"""
    import schedule
    
    cleanup = SystemCleanup()
    
    # 日次クリーンアップ（午前2時）
    schedule.every().day.at("02:00").do(
        lambda: cleanup.run_cleanup('daily')
    )
    
    # 週次クリーンアップ（日曜午前3時）
    schedule.every().sunday.at("03:00").do(
        lambda: cleanup.run_cleanup('weekly')
    )
    
    # 月次クリーンアップ（月初午前1時）
    schedule.every().month.do(
        lambda: cleanup.run_cleanup('monthly')
    )
    
    return schedule

if __name__ == "__main__":
    cleanup = SystemCleanup()
    results = cleanup.run_cleanup('daily')
    print(f"Cleanup completed: {results}")
```

== 🎯 まとめ

KanshiChanの保守手順は以下の体系的プロセスで構成されています：

=== 主要保守プロセス

* ✅ **定期保守**: 日次/週次/月次の自動化された保守タスク
* ✅ **バックアップ・復旧**: 多層バックアップ戦略と災害復旧手順
* ✅ **データ整合性**: 包括的な整合性チェックと修復手順
* ✅ **システムクリーンアップ**: 自動化されたクリーンアップタスク
* ✅ **監査・レポート**: 定期的な保守状況レポート生成

=== 保守効率化

[cols="2,2,2", options="header"]
|===
|保守項目 |自動化レベル |効果
|日次チェック |100% 自動 |✅ 人的エラー削減
|バックアップ |100% 自動 |✅ 確実なデータ保護
|整合性チェック |90% 自動 |✅ 早期問題発見
|クリーンアップ |100% 自動 |✅ ストレージ最適化
|===

=== 保守品質管理

1. **標準化**: 全保守作業の手順標準化
2. **自動化**: 可能な限りの作業自動化
3. **監査**: 保守作業の完全なログ記録
4. **継続改善**: 定期的な手順見直しと最適化

---

**📞 Contact**: team@kanshichan.dev +
**🔗 Repository**: https://github.com/kanshichan/backend +
**📅 Last Updated**: {docdate} +
**📝 Document Version**: {revnumber}