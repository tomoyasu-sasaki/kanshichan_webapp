= KanshiChanãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
:toc: left
:toclevels: 3
:source-highlighter: highlight.js
:icons: font
:doctype: book

[cols="1,3"]
|===
|é …ç›® |è©³ç´°
|**ä½œæˆè€…** |KanshiChané–‹ç™ºãƒãƒ¼ãƒ   
|**æœ€çµ‚æ›´æ–°æ—¥** |{docdate}
|**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³** |v1.0.0
|**å¯¾è±¡èª­è€…** |ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
|**å‰æçŸ¥è­˜** |Redisã€ãƒ¡ãƒ¢ãƒªç®¡ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
|**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ** |<<performance-optimization.adoc#,ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–>>, <<data-management.adoc#,ãƒ‡ãƒ¼ã‚¿ç®¡ç†>>
|===

[abstract]
== æ¦‚è¦

KanshiChanã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥è¨­è¨ˆæ›¸ã§ã™ã€‚å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€LRUã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…ã€Redisæ´»ç”¨æˆ¦ç•¥ã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ‰‹æ³•ã«ã¤ã„ã¦è©³èª¬ã—ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ã¨åŠ¹ç‡çš„ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨ã‚’ä¸¡ç«‹ã™ã‚‹è¨­è¨ˆã‚’æä¾›ã—ã¾ã™ã€‚

== ğŸ—ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

=== ğŸ“Š å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­è¨ˆ

[mermaid]
....
graph TB
    subgraph "ğŸ¯ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤"
        APP[Application Process]
        REQ[Request Handler]
        PROC[Processing Engine]
    end
    
    subgraph "ğŸ’¾ L1: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥"
        FRAME_CACHE[Frame Cache<br/>LRUå®Ÿè£…]
        RESULT_CACHE[Result Cache<br/>æ¤œå‡ºçµæœ]
        AUDIO_CACHE[Audio Cache<br/>TTSéŸ³å£°]
        LLM_CACHE[LLM Cache<br/>åˆ†æçµæœ]
    end
    
    subgraph "ğŸ”„ L2: Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥"
        REDIS_HOT[Hot Data<br/>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ]
        REDIS_SESSION[Session Data<br/>ãƒ¦ãƒ¼ã‚¶ãƒ¼çŠ¶æ…‹]
        REDIS_METRICS[Metrics Cache<br/>çµ±è¨ˆæƒ…å ±]
        REDIS_CONFIG[Config Cache<br/>è¨­å®šãƒ‡ãƒ¼ã‚¿]
    end
    
    subgraph "ğŸ“ L3: æ°¸ç¶šã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸"
        DATABASE[Primary Database]
        FILE_STORAGE[File Storage]
        BACKUP[Backup Storage]
    end
    
    %% ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    APP --> REQ
    REQ --> PROC
    
    PROC --> FRAME_CACHE
    PROC --> RESULT_CACHE
    PROC --> AUDIO_CACHE
    PROC --> LLM_CACHE
    
    FRAME_CACHE --> REDIS_HOT
    RESULT_CACHE --> REDIS_SESSION
    AUDIO_CACHE --> REDIS_METRICS
    LLM_CACHE --> REDIS_CONFIG
    
    REDIS_HOT --> DATABASE
    REDIS_SESSION --> FILE_STORAGE
    REDIS_METRICS --> BACKUP
    
    classDef app fill:#e3f2fd
    classDef l1 fill:#e8f5e8
    classDef l2 fill:#fff3e0
    classDef l3 fill:#f3e5f5
    
    class APP,REQ,PROC app
    class FRAME_CACHE,RESULT_CACHE,AUDIO_CACHE,LLM_CACHE l1
    class REDIS_HOT,REDIS_SESSION,REDIS_METRICS,REDIS_CONFIG l2
    class DATABASE,FILE_STORAGE,BACKUP l3
....

=== ğŸ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ãƒãƒˆãƒªã‚¯ã‚¹

[cols="2,1,1,1,2,2", options="header"]
|===
|ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— |ãƒ’ãƒƒãƒˆç‡ç›®æ¨™ |TTL |ã‚µã‚¤ã‚ºåˆ¶é™ |ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤ |ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥
|**ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿** |85% |5ç§’ |50MB |L1 |LRU + ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹
|**æ¤œå‡ºçµæœ** |90% |30ç§’ |25MB |L1 + L2 |LRU + TTL
|**éŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥** |95% |24æ™‚é–“ |200MB |L1 |ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ãƒ™ãƒ¼ã‚¹
|**LLMåˆ†æçµæœ** |80% |24æ™‚é–“ |100MB |L1 + L2 |TTL + LRU
|**ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿** |99% |1æ™‚é–“ |ç„¡åˆ¶é™ |L2 |TTL
|**è¨­å®šãƒ‡ãƒ¼ã‚¿** |99% |12æ™‚é–“ |10MB |L2 |æ‰‹å‹•æ›´æ–°
|**çµ±è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹** |70% |5åˆ† |50MB |L2 |æ™‚é–“ãƒ™ãƒ¼ã‚¹
|===

== âš™ï¸ L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

=== ğŸ§  ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆLRUï¼‰

==== ã‚³ã‚¢å®Ÿè£…

```python
# backend/src/core/memory_manager.py
class MemoryCache:
    """é«˜æ€§èƒ½LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…"""
    
    def __init__(self, max_size: int = 100, max_memory_mb: float = 50.0):
        """LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        
        Args:
            max_size: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°
            max_memory_mb: æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
        """
        self.max_size = max_size
        self.max_memory_mb = max_memory_mb
        self.cache = OrderedDict()  # LRUå®Ÿè£…ã®åŸºç›¤
        self.memory_usage = 0.0
        self._lock = threading.Lock()  # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'memory_cleanups': 0
        }
        
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆLRUæ›´æ–°ï¼‰"""
        with self._lock:
            if key in self.cache:
                # LRU: æœ€è¿‘ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’æœ«å°¾ã«ç§»å‹•
                value = self.cache.pop(key)
                self.cache[key] = value
                self._stats['hits'] += 1
                return value
            
            self._stats['misses'] += 1
            return None
            
    def put(self, key: str, value: Any) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’æ ¼ç´ï¼ˆè‡ªå‹•ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ï¼‰"""
        with self._lock:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š
            value_size = self._estimate_size(value)
            
            # æ—¢å­˜ã‚­ãƒ¼æ›´æ–°ã®å ´åˆ
            if key in self.cache:
                old_size = self._estimate_size(self.cache[key])
                self.memory_usage -= old_size
                del self.cache[key]
            
            # ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            while (len(self.cache) >= self.max_size or 
                   self.memory_usage + value_size > self.max_memory_mb):
                if not self.cache:
                    break
                    
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆFIFO in OrderedDictï¼‰
                oldest_key, oldest_value = self.cache.popitem(last=False)
                self.memory_usage -= self._estimate_size(oldest_value)
                self._stats['evictions'] += 1
                
            # æ–°ã—ã„å€¤ã‚’è¿½åŠ 
            self.cache[key] = value
            self.memory_usage += value_size
```

==== ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºæ¨å®š

[mermaid]
....
graph TB
    subgraph "ğŸ” ã‚µã‚¤ã‚ºæ¨å®šã‚¨ãƒ³ã‚¸ãƒ³"
        SIZE_EST[Size Estimator]
        NUMPY_EST[NumPy Array Handler]
        STR_EST[String Handler]
        DICT_EST[Dictionary Handler]
        LIST_EST[List Handler]
    end
    
    subgraph "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥å‡¦ç†"
        NUMPY[NumPy Array<br/>nbytesç›´æ¥å–å¾—]
        STRING[String/Bytes<br/>len()ãƒ™ãƒ¼ã‚¹]
        COMPLEX[è¤‡åˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ<br/>å†å¸°å‡¦ç†]
        DEFAULT[ãã®ä»–<br/>å›ºå®šã‚µã‚¤ã‚ºæ¨å®š]
    end
    
    subgraph "âš¡ æœ€é©åŒ–æˆ¦ç•¥"
        CACHE_SIZE[ã‚µã‚¤ã‚ºã‚­ãƒ£ãƒƒã‚·ãƒ¥]
        THRESHOLD[æ¨å®šé–¾å€¤]
        FALLBACK[ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤]
    end
    
    SIZE_EST --> NUMPY_EST
    SIZE_EST --> STR_EST
    SIZE_EST --> DICT_EST
    SIZE_EST --> LIST_EST
    
    NUMPY_EST --> NUMPY
    STR_EST --> STRING
    DICT_EST --> COMPLEX
    LIST_EST --> COMPLEX
    
    NUMPY --> CACHE_SIZE
    STRING --> THRESHOLD
    COMPLEX --> FALLBACK
    
    classDef estimator fill:#e3f2fd
    classDef datatype fill:#e8f5e8
    classDef optimization fill:#fff3e0
    
    class SIZE_EST,NUMPY_EST,STR_EST,DICT_EST,LIST_EST estimator
    class NUMPY,STRING,COMPLEX,DEFAULT datatype
    class CACHE_SIZE,THRESHOLD,FALLBACK optimization
....

==== å®Ÿè£…ä¾‹ï¼ˆã‚µã‚¤ã‚ºæ¨å®šï¼‰

```python
def _estimate_size(self, obj: Any) -> float:
    """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆMBå˜ä½ï¼‰
    
    Args:
        obj: ã‚µã‚¤ã‚ºã‚’æ¨å®šã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        
    Returns:
        float: æ¨å®šã‚µã‚¤ã‚ºï¼ˆMBï¼‰
    """
    try:
        if isinstance(obj, np.ndarray):
            # NumPyé…åˆ—: æ­£ç¢ºãªãƒã‚¤ãƒˆæ•°
            return obj.nbytes / (1024 * 1024)
            
        elif isinstance(obj, (str, bytes)):
            # æ–‡å­—åˆ—ãƒ»ãƒã‚¤ãƒˆåˆ—: é•·ã•ãƒ™ãƒ¼ã‚¹
            return len(obj) / (1024 * 1024)
            
        elif isinstance(obj, (list, tuple)):
            # ãƒªã‚¹ãƒˆãƒ»ã‚¿ãƒ—ãƒ«: å†å¸°çš„ã‚µã‚¤ã‚ºè¨ˆç®—
            return sum(self._estimate_size(item) for item in obj)
            
        elif isinstance(obj, dict):
            # è¾æ›¸: ã‚­ãƒ¼ãƒ»å€¤ä¸¡æ–¹ã®ã‚µã‚¤ã‚º
            return sum(
                self._estimate_size(k) + self._estimate_size(v) 
                for k, v in obj.items()
            )
            
        else:
            # ãã®ä»–: å›ºå®šæ¨å®šå€¤ï¼ˆ1KBï¼‰
            return 0.001
            
    except Exception:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ãªæ¨å®šå€¤
        return 0.001
```

=== ğŸ­ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

==== AIæœ€é©åŒ–çµ±åˆ

```python
# backend/src/core/ai_optimizer.py
class AIOptimizer:
    """AIå‡¦ç†æœ€é©åŒ–ï¼ˆçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        # æ¤œå‡ºçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæç”»ç¶™ç¶šæ€§ç¢ºä¿ï¼‰
        self.last_yolo_results = None
        self.last_yolo_results_age = 0  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµŒéãƒ•ãƒ¬ãƒ¼ãƒ æ•°
        self.max_cache_age = 10  # æœ€å¤§ä¿æŒãƒ•ãƒ¬ãƒ¼ãƒ æ•°
        
    def optimize_yolo_inference(self, model, frame: np.ndarray) -> Optional[Any]:
        """YOLOæ¨è«–æœ€é©åŒ–ï¼ˆçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¹´é½¢æ›´æ–°
        self.last_yolo_results_age += 1
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
        current_fps = self.performance_monitor.get_current_fps()
        should_skip = not self.frame_skipper.should_process_frame(current_fps)
        
        if should_skip:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµæœæ´»ç”¨ã«ã‚ˆã‚‹æç”»ç¶™ç¶šæ€§ç¶­æŒ
            if (self.last_yolo_results is not None and 
                self.last_yolo_results_age <= self.max_cache_age):
                logger.debug(f"Using cached YOLO results (age: {self.last_yolo_results_age})")
                return self.last_yolo_results
            else:
                logger.debug(f"Cache expired (age: {self.last_yolo_results_age})")
                return None
                
        # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†å®Ÿè¡Œ
        with torch.no_grad():
            results = model(frame, verbose=False)
            
        # æˆåŠŸçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.last_yolo_results = results
        self.last_yolo_results_age = 0  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
        
        return results
```

==== ãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼

[mermaid]
....
sequenceDiagram
    participant APP as Application
    participant OPTIMIZER as AI Optimizer
    participant CACHE as Frame Cache
    participant MODEL as AI Model
    
    APP->>OPTIMIZER: ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†è¦æ±‚
    OPTIMIZER->>OPTIMIZER: FPSç›£è¦–ãƒ»ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
    
    alt ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—
        OPTIMIZER->>CACHE: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµæœç¢ºèª
        alt ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹
            CACHE-->>OPTIMIZER: å‰å›çµæœè¿”å´
            OPTIMIZER-->>APP: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµæœ
        else ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹
            OPTIMIZER-->>APP: Noneï¼ˆå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        end
    else ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
        OPTIMIZER->>MODEL: AIæ¨è«–å®Ÿè¡Œ
        MODEL-->>OPTIMIZER: æ¨è«–çµæœ
        OPTIMIZER->>CACHE: çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        OPTIMIZER-->>APP: æ¨è«–çµæœ
    end
    
    Note over OPTIMIZER,CACHE: æç”»ç¶™ç¶šæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸¡ç«‹
....

=== ğŸ”Š éŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 

==== TTSéŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥

```python
# backend/src/services/tts/audio_processor.py
class AudioProcessor:
    """éŸ³å£°å‡¦ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self, tts_config: 'TTSConfig'):
        # éŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self._audio_cache = {}
        self._cache_lock = threading.Lock()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        self._metrics = {
            'total_generations': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_generation_time': 0
        }
        
    def _get_cache_key(self, text: str, voice_settings: Dict[str, Any]) -> str:
        """éŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        cache_data = {
            'text': text,
            'voice': voice_settings.get('voice', 'default'),
            'speed': voice_settings.get('speed', 1.0),
            'emotion': voice_settings.get('emotion', 'neutral')
        }
        
        # SHA256ãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ä¸€æ„ã‚­ãƒ¼ç”Ÿæˆ
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.sha256(cache_string.encode()).hexdigest()
    
    def get_cached_audio(self, text: str, voice_settings: Dict[str, Any]) -> Optional[str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        if not self.tts_config.enable_audio_cache:
            return None
            
        cache_key = self._get_cache_key(text, voice_settings)
        
        with self._cache_lock:
            if cache_key in self._audio_cache:
                entry = self._audio_cache[cache_key]
                
                # TTLç¢ºèª
                if self._is_cache_valid(entry):
                    self._metrics['cache_hits'] += 1
                    return entry['file_path']
                else:
                    # æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
                    self._remove_cache_entry(cache_key)
            
            self._metrics['cache_misses'] += 1
            return None
```

==== ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æˆ¦ç•¥

[mermaid]
....
graph TB
    subgraph "ğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒˆãƒªã‚¬ãƒ¼"
        SIZE_LIMIT[ã‚µã‚¤ã‚ºåˆ¶é™è¶…é]
        TTL_EXPIRE[TTLæœŸé™åˆ‡ã‚Œ]
        MEMORY_PRESSURE[ãƒ¡ãƒ¢ãƒªåœ§è¿«]
        MANUAL_CLEAN[æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—]
    end
    
    subgraph "ğŸ“Š ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥"
        LRU_EVICT[LRU ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³]
        SIZE_EVICT[ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹å‰Šé™¤]
        TTL_EVICT[æœŸé™ãƒ™ãƒ¼ã‚¹å‰Šé™¤]
        FREQ_EVICT[ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ãƒ™ãƒ¼ã‚¹]
    end
    
    subgraph "ğŸ”„ å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹"
        SCAN[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ã‚­ãƒ£ãƒ³]
        SELECT[å‰Šé™¤å¯¾è±¡é¸æŠ]
        DELETE[ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤]
        UPDATE[çµ±è¨ˆæ›´æ–°]
    end
    
    SIZE_LIMIT --> LRU_EVICT
    TTL_EXPIRE --> TTL_EVICT
    MEMORY_PRESSURE --> SIZE_EVICT
    MANUAL_CLEAN --> FREQ_EVICT
    
    LRU_EVICT --> SCAN
    SIZE_EVICT --> SCAN
    TTL_EVICT --> SCAN
    FREQ_EVICT --> SCAN
    
    SCAN --> SELECT
    SELECT --> DELETE
    DELETE --> UPDATE
    
    classDef trigger fill:#ffebee
    classDef strategy fill:#e8f5e8
    classDef process fill:#e3f2fd
    
    class SIZE_LIMIT,TTL_EXPIRE,MEMORY_PRESSURE,MANUAL_CLEAN trigger
    class LRU_EVICT,SIZE_EVICT,TTL_EVICT,FREQ_EVICT strategy
    class SCAN,SELECT,DELETE,UPDATE process
....

== ğŸ”„ L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰

=== ğŸ“Š Redisãƒ‡ãƒ¼ã‚¿æ§‹é€ æœ€é©åŒ–

==== éšå±¤ãƒ‡ãƒ¼ã‚¿ç®¡ç†

```python
# Redis Data Structure Optimization
REDIS_SCHEMA = {
    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆSorted Setæ´»ç”¨ï¼‰
    "detection_timeline": {
        "key_pattern": "detection:timeline:{session_id}",
        "type": "sorted_set",
        "score": "timestamp",
        "member": "detection_data_json",
        "ttl": 86400  # 24æ™‚é–“
    },
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ï¼ˆHashæ´»ç”¨ï¼‰
    "session_state": {
        "key_pattern": "session:{session_id}",
        "type": "hash",
        "fields": ["user_id", "start_time", "last_activity", "status"],
        "ttl": 3600  # 1æ™‚é–“
    },
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆTime Seriesï¼‰
    "performance_metrics": {
        "key_pattern": "metrics:{instance_id}:{YYYYMMDD_HH}",
        "type": "hash",
        "fields": ["cpu_usage", "memory_usage", "fps", "latency"],
        "ttl": 3600  # 1æ™‚é–“
    },
    
    # è¨­å®šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆString + JSONï¼‰
    "config_cache": {
        "key_pattern": "config:{config_type}:{version}",
        "type": "string",
        "value": "json_encoded_config",
        "ttl": 43200  # 12æ™‚é–“
    }
}
```

==== ã‚¯ã‚¨ãƒªæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

[mermaid]
....
graph TB
    subgraph "ğŸ” ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³"
        RANGE_Q[ç¯„å›²ã‚¯ã‚¨ãƒª<br/>ZRANGEBYSCORE]
        SESSION_Q[ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ã‚¨ãƒª<br/>HGETALL]
        TIMELINE_Q[ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¯ã‚¨ãƒª<br/>ZREVRANGE]
        METRICS_Q[ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¯ã‚¨ãƒª<br/>MGET]
    end
    
    subgraph "âš¡ æœ€é©åŒ–æŠ€è¡“"
        PIPELINE[Pipeline Batching]
        LUA_SCRIPT[Lua Scripts]
        INDEX[Secondary Index]
        COMPRESSION[Data Compression]
    end
    
    subgraph "ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"
        BATCH_OPS[ãƒãƒƒãƒæ“ä½œ<br/>10-100xé«˜é€ŸåŒ–]
        SERVER_SIDE[ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰å‡¦ç†<br/>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‰Šæ¸›]
        SMART_INDEX[ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨<br/>O(log N)æ¤œç´¢]
        MEMORY_OPT[ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–<br/>50%å‰Šæ¸›å¯èƒ½]
    end
    
    RANGE_Q --> PIPELINE
    SESSION_Q --> LUA_SCRIPT
    TIMELINE_Q --> INDEX
    METRICS_Q --> COMPRESSION
    
    PIPELINE --> BATCH_OPS
    LUA_SCRIPT --> SERVER_SIDE
    INDEX --> SMART_INDEX
    COMPRESSION --> MEMORY_OPT
    
    classDef query fill:#e3f2fd
    classDef optimization fill:#e8f5e8
    classDef performance fill:#fff3e0
    
    class RANGE_Q,SESSION_Q,TIMELINE_Q,METRICS_Q query
    class PIPELINE,LUA_SCRIPT,INDEX,COMPRESSION optimization
    class BATCH_OPS,SERVER_SIDE,SMART_INDEX,MEMORY_OPT performance
....

==== å®Ÿè£…ä¾‹ï¼ˆåŠ¹ç‡çš„ã‚¯ã‚¨ãƒªï¼‰

```python
# backend/src/services/data/redis_optimizer.py
class RedisOptimizer:
    """Redisæœ€é©åŒ–ã‚¯ã‚¨ãƒªå®Ÿè£…"""
    
    def get_recent_detections_optimized(self, session_id: str, limit: int = 100):
        """æœ€æ–°æ¤œå‡ºçµæœã®åŠ¹ç‡çš„å–å¾—"""
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½¿ç”¨ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯RTTå‰Šæ¸›
        pipe = self.redis.pipeline()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æ¤œå‡ºIDã‚’å–å¾—
        timeline_key = f"detection:timeline:{session_id}"
        pipe.zrevrange(timeline_key, 0, limit-1)
        
        # ãƒãƒƒãƒã§è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—
        detection_ids = pipe.execute()[0]
        
        pipe = self.redis.pipeline()
        for detection_id in detection_ids:
            pipe.hgetall(f"detection:{detection_id}")
        
        return pipe.execute()
    
    def aggregate_performance_metrics_lua(self, instance_id: str, hours: int = 24):
        """Luaã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚‹ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰é›†è¨ˆ"""
        lua_script = """
        local instance_id = ARGV[1]
        local hours = tonumber(ARGV[2])
        local current_hour = math.floor(os.time() / 3600)
        
        local metrics = {}
        local total_cpu = 0
        local total_memory = 0
        local count = 0
        
        for i = 0, hours-1 do
            local hour_key = 'metrics:' .. instance_id .. ':' .. (current_hour - i)
            local data = redis.call('HGETALL', hour_key)
            
            if #data > 0 then
                local hour_metrics = {}
                for j = 1, #data, 2 do
                    hour_metrics[data[j]] = tonumber(data[j+1]) or 0
                end
                
                total_cpu = total_cpu + (hour_metrics['cpu_usage'] or 0)
                total_memory = total_memory + (hour_metrics['memory_usage'] or 0)
                count = count + 1
            end
        end
        
        return {
            avg_cpu = count > 0 and (total_cpu / count) or 0,
            avg_memory = count > 0 and (total_memory / count) or 0,
            data_points = count
        }
        """
        
        return self.redis.eval(lua_script, 0, instance_id, hours)
```

=== ğŸ’¾ Redisè¨­å®šæœ€é©åŒ–

==== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š

```yaml
# Redis Configuration for KanshiChan
redis_optimization:
  # ãƒ¡ãƒ¢ãƒªç®¡ç†
  memory:
    max_memory: "4gb"
    eviction_policy: "allkeys-lru"  # LRU ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³
    lazy_freeing: true  # éåŒæœŸå‰Šé™¤
    
  # æ°¸ç¶šåŒ–æœ€é©åŒ–
  persistence:
    save_points:
      - "900 1"     # 900ç§’é–“ã«1å›ã®å¤‰æ›´ã§ä¿å­˜
      - "300 10"    # 300ç§’é–“ã«10å›ã®å¤‰æ›´ã§ä¿å­˜
      - "60 10000"  # 60ç§’é–“ã«10000å›ã®å¤‰æ›´ã§ä¿å­˜
    rdb_compression: true
    rdb_checksum: true
    
  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
  network:
    tcp_keepalive: 300
    timeout: 0  # æ°¸ç¶šæ¥ç¶š
    tcp_backlog: 511
    maxclients: 10000
    
  # ãƒ‡ãƒ¼ã‚¿åœ§ç¸®
  compression:
    enable: true
    algorithm: "lz4"  # é«˜é€Ÿåœ§ç¸®
    threshold_bytes: 1024
    
  # TTLç®¡ç†
  ttl_policy:
    detection_data: 604800    # 7æ—¥
    session_data: 86400       # 24æ™‚é–“
    metrics_data: 3600        # 1æ™‚é–“
    config_data: 43200        # 12æ™‚é–“
```

== ğŸ”§ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–æˆ¦ç•¥

=== âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

==== å‹•çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºèª¿æ•´

[mermaid]
....
graph TB
    subgraph "ğŸ“Š ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹"
        HIT_RATE[ãƒ’ãƒƒãƒˆç‡ç›£è¦–]
        MEMORY_USAGE[ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡]
        LATENCY[ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“]
        EVICTION_RATE[ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ç‡]
    end
    
    subgraph "ğŸ›ï¸ èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
        CACHE_SIZE[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º]
        TTL_VALUES[TTLå€¤]
        EVICTION_POLICY[ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒªã‚·ãƒ¼]
        COMPRESSION[åœ§ç¸®è¨­å®š]
    end
    
    subgraph "ğŸ¤– è‡ªå‹•èª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯"
        THRESHOLD_CHECK[é–¾å€¤ãƒã‚§ãƒƒã‚¯]
        DECISION_ENGINE[æ±ºå®šã‚¨ãƒ³ã‚¸ãƒ³]
        ADJUSTMENT[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´]
        VALIDATION[åŠ¹æœæ¤œè¨¼]
    end
    
    HIT_RATE --> THRESHOLD_CHECK
    MEMORY_USAGE --> THRESHOLD_CHECK
    LATENCY --> THRESHOLD_CHECK
    EVICTION_RATE --> THRESHOLD_CHECK
    
    THRESHOLD_CHECK --> DECISION_ENGINE
    DECISION_ENGINE --> ADJUSTMENT
    
    ADJUSTMENT --> CACHE_SIZE
    ADJUSTMENT --> TTL_VALUES
    ADJUSTMENT --> EVICTION_POLICY
    ADJUSTMENT --> COMPRESSION
    
    CACHE_SIZE --> VALIDATION
    TTL_VALUES --> VALIDATION
    EVICTION_POLICY --> VALIDATION
    COMPRESSION --> VALIDATION
    
    classDef monitoring fill:#e3f2fd
    classDef parameters fill:#e8f5e8
    classDef logic fill:#fff3e0
    
    class HIT_RATE,MEMORY_USAGE,LATENCY,EVICTION_RATE monitoring
    class CACHE_SIZE,TTL_VALUES,EVICTION_POLICY,COMPRESSION parameters
    class THRESHOLD_CHECK,DECISION_ENGINE,ADJUSTMENT,VALIDATION logic
....

==== å®Ÿè£…ä¾‹ï¼ˆå‹•çš„æœ€é©åŒ–ï¼‰

```python
# Dynamic Cache Optimization
class DynamicCacheOptimizer:
    def __init__(self, memory_manager: MemoryManager):
        self.memory_manager = memory_manager
        self.optimization_interval = 60  # 60ç§’é–“éš”
        self.target_hit_rate = 0.85
        self.target_memory_usage = 0.75
        
    def optimize_cache_parameters(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹•çš„æœ€é©åŒ–"""
        current_stats = self.memory_manager.get_memory_stats()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        hit_rate = current_stats.get('hit_rate', 0.0)
        memory_usage_ratio = current_stats.get('memory_usage_ratio', 0.0)
        eviction_rate = current_stats.get('eviction_rate', 0.0)
        
        adjustments = {}
        
        # ãƒ’ãƒƒãƒˆç‡ãŒä½ã„å ´åˆ
        if hit_rate < self.target_hit_rate:
            if memory_usage_ratio < self.target_memory_usage:
                # ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºæ‹¡å¤§
                new_size = int(current_stats['cache_size'] * 1.2)
                adjustments['cache_size'] = new_size
                logger.info(f"Increasing cache size to {new_size} for better hit rate")
            else:
                # TTLå»¶é•·ã§ã‚ˆã‚Šé•·æœŸé–“ä¿æŒ
                adjustments['ttl_multiplier'] = 1.5
                logger.info("Extending TTL for better hit rate")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„å ´åˆ
        if memory_usage_ratio > self.target_memory_usage:
            if eviction_rate > 0.1:  # 10%ä»¥ä¸Šã®ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ç‡
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºç¸®å°
                new_size = int(current_stats['cache_size'] * 0.8)
                adjustments['cache_size'] = new_size
                logger.info(f"Reducing cache size to {new_size} for memory optimization")
        
        # èª¿æ•´é©ç”¨
        if adjustments:
            self._apply_adjustments(adjustments)
        
        return adjustments
```

=== ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç›£è¦–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

==== åŒ…æ‹¬çš„ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

```python
# Cache Monitoring System
class CacheMonitoringSystem:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_thresholds = {
            'hit_rate_low': 0.7,
            'memory_usage_high': 0.9,
            'eviction_rate_high': 0.2,
            'latency_high_ms': 10.0
        }
        
    def collect_cache_metrics(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        l1_stats = self._collect_l1_metrics()
        
        # Redisãƒ¡ãƒˆãƒªã‚¯ã‚¹
        redis_stats = self._collect_redis_metrics()
        
        # çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
        combined_metrics = {
            'timestamp': datetime.utcnow().isoformat(),
            'l1_cache': l1_stats,
            'redis_cache': redis_stats,
            'overall': self._calculate_overall_metrics(l1_stats, redis_stats)
        }
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        self._check_alerts(combined_metrics)
        
        return combined_metrics
    
    def _collect_l1_metrics(self) -> Dict[str, Any]:
        """L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
        frame_cache_stats = memory_manager.frame_cache.get_stats()
        result_cache_stats = memory_manager.result_cache.get_stats()
        
        return {
            'frame_cache': {
                'size': len(frame_cache_stats),
                'memory_mb': frame_cache_stats.get('memory_usage', 0),
                'hit_rate': frame_cache_stats.get('hit_rate', 0),
                'evictions': frame_cache_stats.get('evictions', 0)
            },
            'result_cache': {
                'size': len(result_cache_stats),
                'memory_mb': result_cache_stats.get('memory_usage', 0),
                'hit_rate': result_cache_stats.get('hit_rate', 0),
                'evictions': result_cache_stats.get('evictions', 0)
            }
        }
```

==== ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

[mermaid]
....
graph TB
    subgraph "ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"
        L1_METRICS[L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹]
        REDIS_METRICS[Redisãƒ¡ãƒˆãƒªã‚¯ã‚¹]
        APP_METRICS[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹]
        SYS_METRICS[ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹]
    end
    
    subgraph "ğŸ“ˆ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"
        HIT_RATE_CHART[ãƒ’ãƒƒãƒˆç‡ãƒãƒ£ãƒ¼ãƒˆ]
        MEMORY_CHART[ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡]
        LATENCY_CHART[ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†æ]
        EVICTION_CHART[ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ]
    end
    
    subgraph "ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ "
        THRESHOLD_MONITOR[é–¾å€¤ç›£è¦–]
        ALERT_GENERATOR[ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ]
        NOTIFICATION[é€šçŸ¥é…ä¿¡]
        ESCALATION[ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    end
    
    L1_METRICS --> HIT_RATE_CHART
    REDIS_METRICS --> MEMORY_CHART
    APP_METRICS --> LATENCY_CHART
    SYS_METRICS --> EVICTION_CHART
    
    HIT_RATE_CHART --> THRESHOLD_MONITOR
    MEMORY_CHART --> THRESHOLD_MONITOR
    LATENCY_CHART --> THRESHOLD_MONITOR
    EVICTION_CHART --> THRESHOLD_MONITOR
    
    THRESHOLD_MONITOR --> ALERT_GENERATOR
    ALERT_GENERATOR --> NOTIFICATION
    NOTIFICATION --> ESCALATION
    
    classDef metrics fill:#e3f2fd
    classDef dashboard fill:#e8f5e8
    classDef alerts fill:#ffebee
    
    class L1_METRICS,REDIS_METRICS,APP_METRICS,SYS_METRICS metrics
    class HIT_RATE_CHART,MEMORY_CHART,LATENCY_CHART,EVICTION_CHART dashboard
    class THRESHOLD_MONITOR,ALERT_GENERATOR,NOTIFICATION,ESCALATION alerts
....

== ğŸ› ï¸ é‹ç”¨ãƒ»ä¿å®ˆ

=== ğŸ”§ ã‚­ãƒ£ãƒƒã‚·ãƒ¥é‹ç”¨æ‰‹é †

==== æ—¥å¸¸ç›£è¦–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

[cols="3,1,4", options="header"]
|===
|ç›£è¦–é …ç›® |é »åº¦ |å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³
|**ãƒ’ãƒƒãƒˆç‡ç›£è¦–** |æ¯æ™‚ |85%æœªæº€ã®å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºèª¿æ•´æ¤œè¨
|**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** |æ¯æ™‚ |90%è¶…éã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
|**ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³ç‡** |æ¯æ™‚ |20%è¶…éã§TTLãƒ»ã‚µã‚¤ã‚ºè¨­å®šè¦‹ç›´ã—
|**ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“** |æ¯æ™‚ |10msè¶…éã§æœ€é©åŒ–å®Ÿè¡Œ
|**Redisæ¥ç¶šæ•°** |æ¯æ™‚ |ä¸Šé™è¿‘æ¥æ™‚ã«ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«èª¿æ•´
|**ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡** |æ—¥æ¬¡ |RDBãƒ»AOFãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
|**ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ¤œçŸ¥** |å¸¸æ™‚ |è‡ªå‹•å¾©æ—§ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆé…ä¿¡
|===

==== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
# Performance Tuning Guide
CACHE_TUNING_PATTERNS = {
    # é«˜ãƒ’ãƒƒãƒˆç‡ãƒ‘ã‚¿ãƒ¼ãƒ³
    "high_hit_rate": {
        "cache_size": "large",
        "ttl": "extended", 
        "eviction": "lru",
        "use_case": "é »ç¹ã‚¢ã‚¯ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿"
    },
    
    # ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ‘ã‚¿ãƒ¼ãƒ³
    "low_latency": {
        "cache_size": "medium",
        "ttl": "short",
        "eviction": "fifo",
        "compression": "disabled",
        "use_case": "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†"
    },
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³
    "memory_efficient": {
        "cache_size": "small",
        "ttl": "adaptive",
        "eviction": "lru_with_ttl",
        "compression": "enabled",
        "use_case": "ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ç’°å¢ƒ"
    },
    
    # ãƒãƒ©ãƒ³ã‚¹å‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    "balanced": {
        "cache_size": "medium",
        "ttl": "standard",
        "eviction": "lru",
        "compression": "selective",
        "use_case": "æ±ç”¨ç”¨é€”"
    }
}
```

=== ğŸ”„ éšœå®³å¯¾å¿œ

==== éšœå®³å¯¾å¿œãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

[mermaid]
....
flowchart TD
    ALERT[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ©ãƒ¼ãƒˆ] --> ASSESS[å½±éŸ¿åº¦è©•ä¾¡]
    
    ASSESS --> CRITICAL{ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«?}
    CRITICAL -->|Yes| EMERGENCY[ç·Šæ€¥å¯¾å¿œãƒ¢ãƒ¼ãƒ‰]
    CRITICAL -->|No| STANDARD[æ¨™æº–å¯¾å¿œ]
    
    EMERGENCY --> CACHE_DISABLE[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–]
    EMERGENCY --> DIRECT_DB[DBç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹]
    
    STANDARD --> IDENTIFY{å•é¡Œç‰¹å®š}
    IDENTIFY -->|ãƒ¡ãƒ¢ãƒªä¸è¶³| MEMORY_CLEAN[ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—]
    IDENTIFY -->|ä½ãƒ’ãƒƒãƒˆç‡| OPTIMIZE[æœ€é©åŒ–å®Ÿè¡Œ]
    IDENTIFY -->|Rediséšœå®³| REDIS_RECOVER[Rediså¾©æ—§]
    
    MEMORY_CLEAN --> EVICT[å¼·åˆ¶ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³]
    OPTIMIZE --> RETUNE[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†èª¿æ•´]
    REDIS_RECOVER --> RESTART[ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•]
    
    EVICT --> MONITOR[ç›£è¦–å¼·åŒ–]
    RETUNE --> MONITOR
    RESTART --> MONITOR
    DIRECT_DB --> MONITOR
    
    MONITOR --> VERIFY[å¾©æ—§ç¢ºèª]
    VERIFY --> DOCUMENT[å¯¾å¿œè¨˜éŒ²]
    
    style EMERGENCY fill:#ffebee
    style CACHE_DISABLE fill:#ffebee
    style DIRECT_DB fill:#ffebee
....

=== ğŸ“ˆ ç¶™ç¶šçš„æœ€é©åŒ–

==== æœ€é©åŒ–ã‚µã‚¤ã‚¯ãƒ«

```python
# Continuous Optimization Cycle
class CacheOptimizationCycle:
    def __init__(self):
        self.cycle_interval = timedelta(hours=6)  # 6æ™‚é–“ã‚µã‚¤ã‚¯ãƒ«
        self.optimization_history = []
        
    def run_optimization_cycle(self):
        """ç¶™ç¶šçš„æœ€é©åŒ–ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ"""
        cycle_start = datetime.utcnow()
        
        # 1. ãƒ‡ãƒ¼ã‚¿åé›†
        current_metrics = self.collect_performance_metrics()
        
        # 2. åˆ†æ
        analysis = self.analyze_performance_trends(current_metrics)
        
        # 3. æœ€é©åŒ–æˆ¦ç•¥æ±ºå®š
        optimization_plan = self.create_optimization_plan(analysis)
        
        # 4. æœ€é©åŒ–å®Ÿè¡Œ
        results = self.execute_optimization(optimization_plan)
        
        # 5. åŠ¹æœæ¸¬å®š
        effectiveness = self.measure_optimization_effectiveness(results)
        
        # 6. å±¥æ­´è¨˜éŒ²
        self.optimization_history.append({
            'timestamp': cycle_start,
            'metrics': current_metrics,
            'plan': optimization_plan,
            'results': results,
            'effectiveness': effectiveness
        })
        
        logger.info(f"Optimization cycle completed: {effectiveness}")
        return effectiveness
```

== ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

=== ğŸ“‹ æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

[cols="3,1,2,2", options="header"]
|===
|æ”¹å–„é …ç›® |å„ªå…ˆåº¦ |å®Ÿè£…æ™‚æœŸ |æœŸå¾…åŠ¹æœ
|**åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹ã‚¿** |é«˜ |Q2 2025 |å¯ç”¨æ€§ãƒ»æ‹¡å¼µæ€§å‘ä¸Š
|**æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹äºˆæ¸¬** |ä¸­ |Q3 2025 |ãƒ’ãƒƒãƒˆç‡15%å‘ä¸Š
|**ã‚¨ãƒƒã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±•é–‹** |ä¸­ |Q3 2025 |ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·50%å‰Šæ¸›
|**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–** |é«˜ |Q2 2025 |è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
|**ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œ** |ä½ |Q4 2025 |ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š
|===

=== ğŸ¯ é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

* <<performance-optimization.adoc#,ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–>>
* <<data-management.adoc#,ãƒ‡ãƒ¼ã‚¿ç®¡ç†è¨­è¨ˆ>>
* <<monitoring-core.adoc#,ç›£è¦–ã‚³ã‚¢ã‚·ã‚¹ãƒ†ãƒ >>
* <<scalability-design.adoc#,ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¨­è¨ˆ>>

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: v1.0.0 