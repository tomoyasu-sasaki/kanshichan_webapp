= KanshiChanバックエンド: キャッシュ戦略
:toc: left
:toclevels: 3
:source-highlighter: highlight.js
:icons: font
:doctype: book

[cols="1,3"]
|===
|項目 |詳細
|**作成者** |KanshiChan開発チーム  
|**最終更新日** |{docdate}
|**ドキュメントバージョン** |v1.0.0
|**対象読者** |システムエンジニア、バックエンド開発者、パフォーマンスエンジニア
|**前提知識** |Redis、メモリ管理、キャッシュアルゴリズム
|**関連ドキュメント** |<<performance-optimization.adoc#,パフォーマンス最適化>>, <<data-management.adoc#,データ管理>>
|===

[abstract]
== 概要

KanshiChanシステムの包括的キャッシュ戦略設計書です。多層キャッシュアーキテクチャ、LRUアルゴリズム実装、Redis活用戦略、メモリ最適化手法について詳説します。リアルタイム性能と効率的リソース利用を両立する設計を提供します。

== 🏗️ キャッシュアーキテクチャ

=== 📊 多層キャッシュ設計

[mermaid]
....
graph TB
    subgraph "🎯 アプリケーション層"
        APP[Application Process]
        REQ[Request Handler]
        PROC[Processing Engine]
    end
    
    subgraph "💾 L1: アプリケーションキャッシュ"
        FRAME_CACHE[Frame Cache<br/>LRU実装]
        RESULT_CACHE[Result Cache<br/>検出結果]
        AUDIO_CACHE[Audio Cache<br/>TTS音声]
        LLM_CACHE[LLM Cache<br/>分析結果]
    end
    
    subgraph "🔄 L2: Redisキャッシュ"
        REDIS_HOT[Hot Data<br/>リアルタイム]
        REDIS_SESSION[Session Data<br/>ユーザー状態]
        REDIS_METRICS[Metrics Cache<br/>統計情報]
        REDIS_CONFIG[Config Cache<br/>設定データ]
    end
    
    subgraph "📁 L3: 永続ストレージ"
        DATABASE[Primary Database]
        FILE_STORAGE[File Storage]
        BACKUP[Backup Storage]
    end
    
    %% データフロー
    APP --> REQ
    REQ --> PROC
    
    PROC --> FRAME_CACHE
    PROC --> RESULT_CACHE
    PROC --> AUDIO_CACHE
    PROC --> LLM_CACHE
    
    FRAME_CACHE --> REDIS_HOT
    RESULT_CACHE --> REDIS_SESSION
    AUDIO_CACHE --> REDIS_METRICS
    LLM_CACHE --> REDIS_CONFIG
    
    REDIS_HOT --> DATABASE
    REDIS_SESSION --> FILE_STORAGE
    REDIS_METRICS --> BACKUP
    
    classDef app fill:#e3f2fd
    classDef l1 fill:#e8f5e8
    classDef l2 fill:#fff3e0
    classDef l3 fill:#f3e5f5
    
    class APP,REQ,PROC app
    class FRAME_CACHE,RESULT_CACHE,AUDIO_CACHE,LLM_CACHE l1
    class REDIS_HOT,REDIS_SESSION,REDIS_METRICS,REDIS_CONFIG l2
    class DATABASE,FILE_STORAGE,BACKUP l3
....

=== 🎯 キャッシュ戦略マトリクス

[cols="2,1,1,1,2,2", options="header"]
|===
|データタイプ |ヒット率目標 |TTL |サイズ制限 |キャッシュ層 |エビクション戦略
|**フレームデータ** |85% |5秒 |50MB |L1 |LRU + サイズベース
|**検出結果** |90% |30秒 |25MB |L1 + L2 |LRU + TTL
|**音声キャッシュ** |95% |24時間 |200MB |L1 |アクセス頻度ベース
|**LLM分析結果** |80% |24時間 |100MB |L1 + L2 |TTL + LRU
|**セッションデータ** |99% |1時間 |無制限 |L2 |TTL
|**設定データ** |99% |12時間 |10MB |L2 |手動更新
|**統計メトリクス** |70% |5分 |50MB |L2 |時間ベース
|===

== ⚙️ L1キャッシュ実装

=== 🧠 メモリキャッシュ（LRU）

==== コア実装

```python
# backend/src/core/memory_manager.py
class MemoryCache:
    """高性能LRUキャッシュ実装"""
    
    def __init__(self, max_size: int = 100, max_memory_mb: float = 50.0):
        """LRUキャッシュ初期化
        
        Args:
            max_size: 最大エントリ数
            max_memory_mb: 最大メモリ使用量（MB）
        """
        self.max_size = max_size
        self.max_memory_mb = max_memory_mb
        self.cache = OrderedDict()  # LRU実装の基盤
        self.memory_usage = 0.0
        self._lock = threading.Lock()  # スレッドセーフ
        
        # パフォーマンス統計
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'memory_cleanups': 0
        }
        
    def get(self, key: str) -> Optional[Any]:
        """キャッシュから値を取得（LRU更新）"""
        with self._lock:
            if key in self.cache:
                # LRU: 最近アクセスしたアイテムを末尾に移動
                value = self.cache.pop(key)
                self.cache[key] = value
                self._stats['hits'] += 1
                return value
            
            self._stats['misses'] += 1
            return None
            
    def put(self, key: str, value: Any) -> None:
        """キャッシュに値を格納（自動エビクション）"""
        with self._lock:
            # メモリ使用量推定
            value_size = self._estimate_size(value)
            
            # 既存キー更新の場合
            if key in self.cache:
                old_size = self._estimate_size(self.cache[key])
                self.memory_usage -= old_size
                del self.cache[key]
            
            # エビクション実行
            while (len(self.cache) >= self.max_size or 
                   self.memory_usage + value_size > self.max_memory_mb):
                if not self.cache:
                    break
                    
                # 最も古いエントリを削除（FIFO in OrderedDict）
                oldest_key, oldest_value = self.cache.popitem(last=False)
                self.memory_usage -= self._estimate_size(oldest_value)
                self._stats['evictions'] += 1
                
            # 新しい値を追加
            self.cache[key] = value
            self.memory_usage += value_size
```

==== メモリサイズ推定

[mermaid]
....
graph TB
    subgraph "🔍 サイズ推定エンジン"
        SIZE_EST[Size Estimator]
        NUMPY_EST[NumPy Array Handler]
        STR_EST[String Handler]
        DICT_EST[Dictionary Handler]
        LIST_EST[List Handler]
    end
    
    subgraph "📊 データタイプ別処理"
        NUMPY[NumPy Array<br/>nbytes直接取得]
        STRING[String/Bytes<br/>len()ベース]
        COMPLEX[複合オブジェクト<br/>再帰処理]
        DEFAULT[その他<br/>固定サイズ推定]
    end
    
    subgraph "⚡ 最適化戦略"
        CACHE_SIZE[サイズキャッシュ]
        THRESHOLD[推定閾値]
        FALLBACK[フォールバック値]
    end
    
    SIZE_EST --> NUMPY_EST
    SIZE_EST --> STR_EST
    SIZE_EST --> DICT_EST
    SIZE_EST --> LIST_EST
    
    NUMPY_EST --> NUMPY
    STR_EST --> STRING
    DICT_EST --> COMPLEX
    LIST_EST --> COMPLEX
    
    NUMPY --> CACHE_SIZE
    STRING --> THRESHOLD
    COMPLEX --> FALLBACK
    
    classDef estimator fill:#e3f2fd
    classDef datatype fill:#e8f5e8
    classDef optimization fill:#fff3e0
    
    class SIZE_EST,NUMPY_EST,STR_EST,DICT_EST,LIST_EST estimator
    class NUMPY,STRING,COMPLEX,DEFAULT datatype
    class CACHE_SIZE,THRESHOLD,FALLBACK optimization
....

==== 実装例（サイズ推定）

```python
def _estimate_size(self, obj: Any) -> float:
    """オブジェクトのメモリサイズを推定（MB単位）
    
    Args:
        obj: サイズを推定するオブジェクト
        
    Returns:
        float: 推定サイズ（MB）
    """
    try:
        if isinstance(obj, np.ndarray):
            # NumPy配列: 正確なバイト数
            return obj.nbytes / (1024 * 1024)
            
        elif isinstance(obj, (str, bytes)):
            # 文字列・バイト列: 長さベース
            return len(obj) / (1024 * 1024)
            
        elif isinstance(obj, (list, tuple)):
            # リスト・タプル: 再帰的サイズ計算
            return sum(self._estimate_size(item) for item in obj)
            
        elif isinstance(obj, dict):
            # 辞書: キー・値両方のサイズ
            return sum(
                self._estimate_size(k) + self._estimate_size(v) 
                for k, v in obj.items()
            )
            
        else:
            # その他: 固定推定値（1KB）
            return 0.001
            
    except Exception:
        # エラー時は安全な推定値
        return 0.001
```

=== 🎭 フレームキャッシュ実装

==== AI最適化統合

```python
# backend/src/core/ai_optimizer.py
class AIOptimizer:
    """AI処理最適化（結果キャッシュ付き）"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        # 検出結果キャッシュ（描画継続性確保）
        self.last_yolo_results = None
        self.last_yolo_results_age = 0  # キャッシュ経過フレーム数
        self.max_cache_age = 10  # 最大保持フレーム数
        
    def optimize_yolo_inference(self, model, frame: np.ndarray) -> Optional[Any]:
        """YOLO推論最適化（結果キャッシュ活用）"""
        # キャッシュ年齢更新
        self.last_yolo_results_age += 1
        
        # フレームスキップ判定
        current_fps = self.performance_monitor.get_current_fps()
        should_skip = not self.frame_skipper.should_process_frame(current_fps)
        
        if should_skip:
            # キャッシュ結果活用による描画継続性維持
            if (self.last_yolo_results is not None and 
                self.last_yolo_results_age <= self.max_cache_age):
                logger.debug(f"Using cached YOLO results (age: {self.last_yolo_results_age})")
                return self.last_yolo_results
            else:
                logger.debug(f"Cache expired (age: {self.last_yolo_results_age})")
                return None
                
        # フレーム処理実行
        with torch.no_grad():
            results = model(frame, verbose=False)
            
        # 成功結果をキャッシュ
        self.last_yolo_results = results
        self.last_yolo_results_age = 0  # キャッシュリフレッシュ
        
        return results
```

==== フレームキャッシュフロー

[mermaid]
....
sequenceDiagram
    participant APP as Application
    participant OPTIMIZER as AI Optimizer
    participant CACHE as Frame Cache
    participant MODEL as AI Model
    
    APP->>OPTIMIZER: フレーム処理要求
    OPTIMIZER->>OPTIMIZER: FPS監視・スキップ判定
    
    alt フレームスキップ
        OPTIMIZER->>CACHE: キャッシュ結果確認
        alt キャッシュ有効
            CACHE-->>OPTIMIZER: 前回結果返却
            OPTIMIZER-->>APP: キャッシュ結果
        else キャッシュ無効
            OPTIMIZER-->>APP: None（処理スキップ）
        end
    else フレーム処理
        OPTIMIZER->>MODEL: AI推論実行
        MODEL-->>OPTIMIZER: 推論結果
        OPTIMIZER->>CACHE: 結果キャッシュ
        OPTIMIZER-->>APP: 推論結果
    end
    
    Note over OPTIMIZER,CACHE: 描画継続性とパフォーマンスを両立
....

=== 🔊 音声キャッシュシステム

==== TTS音声キャッシュ

```python
# backend/src/services/tts/audio_processor.py
class AudioProcessor:
    """音声処理・キャッシュ管理"""
    
    def __init__(self, tts_config: 'TTSConfig'):
        # 音声キャッシュシステム
        self._audio_cache = {}
        self._cache_lock = threading.Lock()
        
        # パフォーマンス指標
        self._metrics = {
            'total_generations': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_generation_time': 0
        }
        
    def _get_cache_key(self, text: str, voice_settings: Dict[str, Any]) -> str:
        """音声キャッシュキー生成"""
        cache_data = {
            'text': text,
            'voice': voice_settings.get('voice', 'default'),
            'speed': voice_settings.get('speed', 1.0),
            'emotion': voice_settings.get('emotion', 'neutral')
        }
        
        # SHA256ハッシュによる一意キー生成
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.sha256(cache_string.encode()).hexdigest()
    
    def get_cached_audio(self, text: str, voice_settings: Dict[str, Any]) -> Optional[str]:
        """キャッシュから音声ファイルを取得"""
        if not self.tts_config.enable_audio_cache:
            return None
            
        cache_key = self._get_cache_key(text, voice_settings)
        
        with self._cache_lock:
            if cache_key in self._audio_cache:
                entry = self._audio_cache[cache_key]
                
                # TTL確認
                if self._is_cache_valid(entry):
                    self._metrics['cache_hits'] += 1
                    return entry['file_path']
                else:
                    # 期限切れエントリ削除
                    self._remove_cache_entry(cache_key)
            
            self._metrics['cache_misses'] += 1
            return None
```

==== キャッシュクリーンアップ戦略

[mermaid]
....
graph TB
    subgraph "🧹 クリーンアップトリガー"
        SIZE_LIMIT[サイズ制限超過]
        TTL_EXPIRE[TTL期限切れ]
        MEMORY_PRESSURE[メモリ圧迫]
        MANUAL_CLEAN[手動クリーンアップ]
    end
    
    subgraph "📊 エビクション戦略"
        LRU_EVICT[LRU エビクション]
        SIZE_EVICT[サイズベース削除]
        TTL_EVICT[期限ベース削除]
        FREQ_EVICT[アクセス頻度ベース]
    end
    
    subgraph "🔄 実行プロセス"
        SCAN[キャッシュスキャン]
        SELECT[削除対象選択]
        DELETE[ファイル削除]
        UPDATE[統計更新]
    end
    
    SIZE_LIMIT --> LRU_EVICT
    TTL_EXPIRE --> TTL_EVICT
    MEMORY_PRESSURE --> SIZE_EVICT
    MANUAL_CLEAN --> FREQ_EVICT
    
    LRU_EVICT --> SCAN
    SIZE_EVICT --> SCAN
    TTL_EVICT --> SCAN
    FREQ_EVICT --> SCAN
    
    SCAN --> SELECT
    SELECT --> DELETE
    DELETE --> UPDATE
    
    classDef trigger fill:#ffebee
    classDef strategy fill:#e8f5e8
    classDef process fill:#e3f2fd
    
    class SIZE_LIMIT,TTL_EXPIRE,MEMORY_PRESSURE,MANUAL_CLEAN trigger
    class LRU_EVICT,SIZE_EVICT,TTL_EVICT,FREQ_EVICT strategy
    class SCAN,SELECT,DELETE,UPDATE process
....

== 🔄 L2キャッシュ（Redis）

=== 📊 Redisデータ構造最適化

==== 階層データ管理

```python
# Redis Data Structure Optimization
REDIS_SCHEMA = {
    # 時系列データ（Sorted Set活用）
    "detection_timeline": {
        "key_pattern": "detection:timeline:{session_id}",
        "type": "sorted_set",
        "score": "timestamp",
        "member": "detection_data_json",
        "ttl": 86400  # 24時間
    },
    
    # セッション状態（Hash活用）
    "session_state": {
        "key_pattern": "session:{session_id}",
        "type": "hash",
        "fields": ["user_id", "start_time", "last_activity", "status"],
        "ttl": 3600  # 1時間
    },
    
    # パフォーマンスメトリクス（Time Series）
    "performance_metrics": {
        "key_pattern": "metrics:{instance_id}:{YYYYMMDD_HH}",
        "type": "hash",
        "fields": ["cpu_usage", "memory_usage", "fps", "latency"],
        "ttl": 3600  # 1時間
    },
    
    # 設定キャッシュ（String + JSON）
    "config_cache": {
        "key_pattern": "config:{config_type}:{version}",
        "type": "string",
        "value": "json_encoded_config",
        "ttl": 43200  # 12時間
    }
}
```

==== クエリ最適化パターン

[mermaid]
....
graph TB
    subgraph "🔍 クエリパターン"
        RANGE_Q[範囲クエリ<br/>ZRANGEBYSCORE]
        SESSION_Q[セッションクエリ<br/>HGETALL]
        TIMELINE_Q[タイムラインクエリ<br/>ZREVRANGE]
        METRICS_Q[メトリクスクエリ<br/>MGET]
    end
    
    subgraph "⚡ 最適化技術"
        PIPELINE[Pipeline Batching]
        LUA_SCRIPT[Lua Scripts]
        INDEX[Secondary Index]
        COMPRESSION[Data Compression]
    end
    
    subgraph "📊 パフォーマンス"
        BATCH_OPS[バッチ操作<br/>10-100x高速化]
        SERVER_SIDE[サーバーサイド処理<br/>ネットワーク削減]
        SMART_INDEX[インデックス活用<br/>O(log N)検索]
        MEMORY_OPT[メモリ最適化<br/>50%削減可能]
    end
    
    RANGE_Q --> PIPELINE
    SESSION_Q --> LUA_SCRIPT
    TIMELINE_Q --> INDEX
    METRICS_Q --> COMPRESSION
    
    PIPELINE --> BATCH_OPS
    LUA_SCRIPT --> SERVER_SIDE
    INDEX --> SMART_INDEX
    COMPRESSION --> MEMORY_OPT
    
    classDef query fill:#e3f2fd
    classDef optimization fill:#e8f5e8
    classDef performance fill:#fff3e0
    
    class RANGE_Q,SESSION_Q,TIMELINE_Q,METRICS_Q query
    class PIPELINE,LUA_SCRIPT,INDEX,COMPRESSION optimization
    class BATCH_OPS,SERVER_SIDE,SMART_INDEX,MEMORY_OPT performance
....

==== 実装例（効率的クエリ）

```python
# backend/src/services/data/redis_optimizer.py
class RedisOptimizer:
    """Redis最適化クエリ実装"""
    
    def get_recent_detections_optimized(self, session_id: str, limit: int = 100):
        """最新検出結果の効率的取得"""
        # パイプライン使用でネットワークRTT削減
        pipe = self.redis.pipeline()
        
        # インデックスから検出IDを取得
        timeline_key = f"detection:timeline:{session_id}"
        pipe.zrevrange(timeline_key, 0, limit-1)
        
        # バッチで詳細データ取得
        detection_ids = pipe.execute()[0]
        
        pipe = self.redis.pipeline()
        for detection_id in detection_ids:
            pipe.hgetall(f"detection:{detection_id}")
        
        return pipe.execute()
    
    def aggregate_performance_metrics_lua(self, instance_id: str, hours: int = 24):
        """Luaスクリプトによるサーバーサイド集計"""
        lua_script = """
        local instance_id = ARGV[1]
        local hours = tonumber(ARGV[2])
        local current_hour = math.floor(os.time() / 3600)
        
        local metrics = {}
        local total_cpu = 0
        local total_memory = 0
        local count = 0
        
        for i = 0, hours-1 do
            local hour_key = 'metrics:' .. instance_id .. ':' .. (current_hour - i)
            local data = redis.call('HGETALL', hour_key)
            
            if #data > 0 then
                local hour_metrics = {}
                for j = 1, #data, 2 do
                    hour_metrics[data[j]] = tonumber(data[j+1]) or 0
                end
                
                total_cpu = total_cpu + (hour_metrics['cpu_usage'] or 0)
                total_memory = total_memory + (hour_metrics['memory_usage'] or 0)
                count = count + 1
            end
        end
        
        return {
            avg_cpu = count > 0 and (total_cpu / count) or 0,
            avg_memory = count > 0 and (total_memory / count) or 0,
            data_points = count
        }
        """
        
        return self.redis.eval(lua_script, 0, instance_id, hours)
```

=== 💾 Redis設定最適化

==== メモリ最適化設定

```yaml
# Redis Configuration for KanshiChan
redis_optimization:
  # メモリ管理
  memory:
    max_memory: "4gb"
    eviction_policy: "allkeys-lru"  # LRU エビクション
    lazy_freeing: true  # 非同期削除
    
  # 永続化最適化
  persistence:
    save_points:
      - "900 1"     # 900秒間に1回の変更で保存
      - "300 10"    # 300秒間に10回の変更で保存
      - "60 10000"  # 60秒間に10000回の変更で保存
    rdb_compression: true
    rdb_checksum: true
    
  # ネットワーク最適化
  network:
    tcp_keepalive: 300
    timeout: 0  # 永続接続
    tcp_backlog: 511
    maxclients: 10000
    
  # データ圧縮
  compression:
    enable: true
    algorithm: "lz4"  # 高速圧縮
    threshold_bytes: 1024
    
  # TTL管理
  ttl_policy:
    detection_data: 604800    # 7日
    session_data: 86400       # 24時間
    metrics_data: 3600        # 1時間
    config_data: 43200        # 12時間
```

== 🔧 キャッシュ最適化戦略

=== ⚡ パフォーマンス最適化

==== 動的キャッシュサイズ調整

[mermaid]
....
graph TB
    subgraph "📊 監視メトリクス"
        HIT_RATE[ヒット率監視]
        MEMORY_USAGE[メモリ使用量]
        LATENCY[レスポンス時間]
        EVICTION_RATE[エビクション率]
    end
    
    subgraph "🎛️ 調整パラメータ"
        CACHE_SIZE[キャッシュサイズ]
        TTL_VALUES[TTL値]
        EVICTION_POLICY[エビクションポリシー]
        COMPRESSION[圧縮設定]
    end
    
    subgraph "🤖 自動調整ロジック"
        THRESHOLD_CHECK[閾値チェック]
        DECISION_ENGINE[決定エンジン]
        ADJUSTMENT[パラメータ調整]
        VALIDATION[効果検証]
    end
    
    HIT_RATE --> THRESHOLD_CHECK
    MEMORY_USAGE --> THRESHOLD_CHECK
    LATENCY --> THRESHOLD_CHECK
    EVICTION_RATE --> THRESHOLD_CHECK
    
    THRESHOLD_CHECK --> DECISION_ENGINE
    DECISION_ENGINE --> ADJUSTMENT
    
    ADJUSTMENT --> CACHE_SIZE
    ADJUSTMENT --> TTL_VALUES
    ADJUSTMENT --> EVICTION_POLICY
    ADJUSTMENT --> COMPRESSION
    
    CACHE_SIZE --> VALIDATION
    TTL_VALUES --> VALIDATION
    EVICTION_POLICY --> VALIDATION
    COMPRESSION --> VALIDATION
    
    classDef monitoring fill:#e3f2fd
    classDef parameters fill:#e8f5e8
    classDef logic fill:#fff3e0
    
    class HIT_RATE,MEMORY_USAGE,LATENCY,EVICTION_RATE monitoring
    class CACHE_SIZE,TTL_VALUES,EVICTION_POLICY,COMPRESSION parameters
    class THRESHOLD_CHECK,DECISION_ENGINE,ADJUSTMENT,VALIDATION logic
....

==== 実装例（動的最適化）

```python
# Dynamic Cache Optimization
class DynamicCacheOptimizer:
    def __init__(self, memory_manager: MemoryManager):
        self.memory_manager = memory_manager
        self.optimization_interval = 60  # 60秒間隔
        self.target_hit_rate = 0.85
        self.target_memory_usage = 0.75
        
    def optimize_cache_parameters(self) -> Dict[str, Any]:
        """キャッシュパラメータの動的最適化"""
        current_stats = self.memory_manager.get_memory_stats()
        
        # パフォーマンス分析
        hit_rate = current_stats.get('hit_rate', 0.0)
        memory_usage_ratio = current_stats.get('memory_usage_ratio', 0.0)
        eviction_rate = current_stats.get('eviction_rate', 0.0)
        
        adjustments = {}
        
        # ヒット率が低い場合
        if hit_rate < self.target_hit_rate:
            if memory_usage_ratio < self.target_memory_usage:
                # メモリに余裕があればキャッシュサイズ拡大
                new_size = int(current_stats['cache_size'] * 1.2)
                adjustments['cache_size'] = new_size
                logger.info(f"Increasing cache size to {new_size} for better hit rate")
            else:
                # TTL延長でより長期間保持
                adjustments['ttl_multiplier'] = 1.5
                logger.info("Extending TTL for better hit rate")
        
        # メモリ使用量が高い場合
        if memory_usage_ratio > self.target_memory_usage:
            if eviction_rate > 0.1:  # 10%以上のエビクション率
                # キャッシュサイズ縮小
                new_size = int(current_stats['cache_size'] * 0.8)
                adjustments['cache_size'] = new_size
                logger.info(f"Reducing cache size to {new_size} for memory optimization")
        
        # 調整適用
        if adjustments:
            self._apply_adjustments(adjustments)
        
        return adjustments
```

=== 📊 キャッシュ監視とメトリクス

==== 包括的監視システム

```python
# Cache Monitoring System
class CacheMonitoringSystem:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_thresholds = {
            'hit_rate_low': 0.7,
            'memory_usage_high': 0.9,
            'eviction_rate_high': 0.2,
            'latency_high_ms': 10.0
        }
        
    def collect_cache_metrics(self) -> Dict[str, Any]:
        """包括的キャッシュメトリクス収集"""
        # L1キャッシュメトリクス
        l1_stats = self._collect_l1_metrics()
        
        # Redisメトリクス
        redis_stats = self._collect_redis_metrics()
        
        # 統合メトリクス
        combined_metrics = {
            'timestamp': datetime.utcnow().isoformat(),
            'l1_cache': l1_stats,
            'redis_cache': redis_stats,
            'overall': self._calculate_overall_metrics(l1_stats, redis_stats)
        }
        
        # アラートチェック
        self._check_alerts(combined_metrics)
        
        return combined_metrics
    
    def _collect_l1_metrics(self) -> Dict[str, Any]:
        """L1キャッシュメトリクス"""
        frame_cache_stats = memory_manager.frame_cache.get_stats()
        result_cache_stats = memory_manager.result_cache.get_stats()
        
        return {
            'frame_cache': {
                'size': len(frame_cache_stats),
                'memory_mb': frame_cache_stats.get('memory_usage', 0),
                'hit_rate': frame_cache_stats.get('hit_rate', 0),
                'evictions': frame_cache_stats.get('evictions', 0)
            },
            'result_cache': {
                'size': len(result_cache_stats),
                'memory_mb': result_cache_stats.get('memory_usage', 0),
                'hit_rate': result_cache_stats.get('hit_rate', 0),
                'evictions': result_cache_stats.get('evictions', 0)
            }
        }
```

==== 監視ダッシュボード

[mermaid]
....
graph TB
    subgraph "📊 メトリクス収集"
        L1_METRICS[L1キャッシュメトリクス]
        REDIS_METRICS[Redisメトリクス]
        APP_METRICS[アプリケーションメトリクス]
        SYS_METRICS[システムメトリクス]
    end
    
    subgraph "📈 ダッシュボード"
        HIT_RATE_CHART[ヒット率チャート]
        MEMORY_CHART[メモリ使用量]
        LATENCY_CHART[レイテンシ分析]
        EVICTION_CHART[エビクション統計]
    end
    
    subgraph "🚨 アラートシステム"
        THRESHOLD_MONITOR[閾値監視]
        ALERT_GENERATOR[アラート生成]
        NOTIFICATION[通知配信]
        ESCALATION[エスカレーション]
    end
    
    L1_METRICS --> HIT_RATE_CHART
    REDIS_METRICS --> MEMORY_CHART
    APP_METRICS --> LATENCY_CHART
    SYS_METRICS --> EVICTION_CHART
    
    HIT_RATE_CHART --> THRESHOLD_MONITOR
    MEMORY_CHART --> THRESHOLD_MONITOR
    LATENCY_CHART --> THRESHOLD_MONITOR
    EVICTION_CHART --> THRESHOLD_MONITOR
    
    THRESHOLD_MONITOR --> ALERT_GENERATOR
    ALERT_GENERATOR --> NOTIFICATION
    NOTIFICATION --> ESCALATION
    
    classDef metrics fill:#e3f2fd
    classDef dashboard fill:#e8f5e8
    classDef alerts fill:#ffebee
    
    class L1_METRICS,REDIS_METRICS,APP_METRICS,SYS_METRICS metrics
    class HIT_RATE_CHART,MEMORY_CHART,LATENCY_CHART,EVICTION_CHART dashboard
    class THRESHOLD_MONITOR,ALERT_GENERATOR,NOTIFICATION,ESCALATION alerts
....

== 🛠️ 運用・保守

=== 🔧 キャッシュ運用手順

==== 日常監視チェックリスト

[cols="3,1,4", options="header"]
|===
|監視項目 |頻度 |対応アクション
|**ヒット率監視** |毎時 |85%未満の場合はキャッシュサイズ調整検討
|**メモリ使用量** |毎時 |90%超過でクリーンアップ実行
|**エビクション率** |毎時 |20%超過でTTL・サイズ設定見直し
|**レスポンス時間** |毎時 |10ms超過で最適化実行
|**Redis接続数** |毎時 |上限近接時にコネクションプール調整
|**ディスク使用量** |日次 |RDB・AOFファイルサイズ確認
|**クラッシュ検知** |常時 |自動復旧・アラート配信
|===

==== パフォーマンスチューニング

```python
# Performance Tuning Guide
CACHE_TUNING_PATTERNS = {
    # 高ヒット率パターン
    "high_hit_rate": {
        "cache_size": "large",
        "ttl": "extended", 
        "eviction": "lru",
        "use_case": "頻繁アクセスデータ"
    },
    
    # 低レイテンシパターン
    "low_latency": {
        "cache_size": "medium",
        "ttl": "short",
        "eviction": "fifo",
        "compression": "disabled",
        "use_case": "リアルタイム処理"
    },
    
    # メモリ効率パターン
    "memory_efficient": {
        "cache_size": "small",
        "ttl": "adaptive",
        "eviction": "lru_with_ttl",
        "compression": "enabled",
        "use_case": "メモリ制約環境"
    },
    
    # バランス型パターン
    "balanced": {
        "cache_size": "medium",
        "ttl": "standard",
        "eviction": "lru",
        "compression": "selective",
        "use_case": "汎用用途"
    }
}
```

=== 🔄 障害対応

==== 障害対応フローチャート

[mermaid]
....
flowchart TD
    ALERT[キャッシュアラート] --> ASSESS[影響度評価]
    
    ASSESS --> CRITICAL{クリティカル?}
    CRITICAL -->|Yes| EMERGENCY[緊急対応モード]
    CRITICAL -->|No| STANDARD[標準対応]
    
    EMERGENCY --> CACHE_DISABLE[キャッシュ無効化]
    EMERGENCY --> DIRECT_DB[DB直接アクセス]
    
    STANDARD --> IDENTIFY{問題特定}
    IDENTIFY -->|メモリ不足| MEMORY_CLEAN[メモリクリーンアップ]
    IDENTIFY -->|低ヒット率| OPTIMIZE[最適化実行]
    IDENTIFY -->|Redis障害| REDIS_RECOVER[Redis復旧]
    
    MEMORY_CLEAN --> EVICT[強制エビクション]
    OPTIMIZE --> RETUNE[パラメータ再調整]
    REDIS_RECOVER --> RESTART[サービス再起動]
    
    EVICT --> MONITOR[監視強化]
    RETUNE --> MONITOR
    RESTART --> MONITOR
    DIRECT_DB --> MONITOR
    
    MONITOR --> VERIFY[復旧確認]
    VERIFY --> DOCUMENT[対応記録]
    
    style EMERGENCY fill:#ffebee
    style CACHE_DISABLE fill:#ffebee
    style DIRECT_DB fill:#ffebee
....

=== 📈 継続的最適化

==== 最適化サイクル

```python
# Continuous Optimization Cycle
class CacheOptimizationCycle:
    def __init__(self):
        self.cycle_interval = timedelta(hours=6)  # 6時間サイクル
        self.optimization_history = []
        
    def run_optimization_cycle(self):
        """継続的最適化サイクル実行"""
        cycle_start = datetime.utcnow()
        
        # 1. データ収集
        current_metrics = self.collect_performance_metrics()
        
        # 2. 分析
        analysis = self.analyze_performance_trends(current_metrics)
        
        # 3. 最適化戦略決定
        optimization_plan = self.create_optimization_plan(analysis)
        
        # 4. 最適化実行
        results = self.execute_optimization(optimization_plan)
        
        # 5. 効果測定
        effectiveness = self.measure_optimization_effectiveness(results)
        
        # 6. 履歴記録
        self.optimization_history.append({
            'timestamp': cycle_start,
            'metrics': current_metrics,
            'plan': optimization_plan,
            'results': results,
            'effectiveness': effectiveness
        })
        
        logger.info(f"Optimization cycle completed: {effectiveness}")
        return effectiveness
```

== 🚀 次のステップ

=== 📋 改善ロードマップ

[cols="3,1,2,2", options="header"]
|===
|改善項目 |優先度 |実装時期 |期待効果
|**分散キャッシュクラスタ** |高 |Q2 2025 |可用性・拡張性向上
|**機械学習ベース予測** |中 |Q3 2025 |ヒット率15%向上
|**エッジキャッシュ展開** |中 |Q3 2025 |レイテンシ50%削減
|**リアルタイム最適化** |高 |Q2 2025 |自動チューニング
|**マルチテナント対応** |低 |Q4 2025 |スケーラビリティ向上
|===

=== 🎯 関連ドキュメント

* <<performance-optimization.adoc#,パフォーマンス最適化>>
* <<data-management.adoc#,データ管理設計>>
* <<monitoring-core.adoc#,監視コアシステム>>
* <<scalability-design.adoc#,スケーラビリティ設計>>

---

**📞 Contact**: team@kanshichan.dev +
**🔗 Repository**: https://github.com/kanshichan/backend +
**📅 Last Updated**: {docdate} +
**📝 Document Version**: v1.0.0 