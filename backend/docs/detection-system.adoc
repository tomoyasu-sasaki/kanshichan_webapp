= ğŸ¤– ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) ç‰©ä½“ãƒ»å§¿å‹¢æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 4
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:version: 2.0.0
:author: KanshiChan Development Team
:email: team@kanshichan.dev
:revnumber: 2.0
:revdate: {docdate}
:experimental:

== ğŸ“– æ¦‚è¦

ç›£è¦–ã¡ã‚ƒã‚“ã®ä¸­æ ¸ã‚’æ‹…ã†AIæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã¯ã€YOLO v8ã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡ºã¨MediaPipeã«ã‚ˆã‚‹å§¿å‹¢æ¤œå‡ºã‚’çµ±åˆã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®äººç‰©ãƒ»ç‰©ä½“ç›£è¦–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€å®Ÿè£…ä»•æ§˜ã€æœ€é©åŒ–æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

[NOTE]
====
ğŸ“‹ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±**

* **å¯¾è±¡èª­è€…**: AI/MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…ã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆè€…
* **æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**: YOLO v8 / MediaPipe / PyTorch / OpenCV
* **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v2.0.0 (AIæœ€é©åŒ–ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯¾å¿œç‰ˆ)
* **æœ€çµ‚æ›´æ–°**: {docdate}
====

== ğŸ¯ æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

=== ğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ ç›®çš„

==== ä¸»è¦æ©Ÿèƒ½
* **äººç‰©æ¤œå‡º**: ä½œæ¥­è€…ã®åœ¨å¸­ãƒ»é›¢å¸­çŠ¶æ³ã®ç›£è¦–
* **å§¿å‹¢åˆ†æ**: ä½œæ¥­å§¿å‹¢ã®å“è³ªè©•ä¾¡ã¨æ”¹å–„ææ¡ˆ
* **ç‰©ä½“æ¤œå‡º**: ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãªã©æ³¨æ„æ•£æ¼«è¦å› ã®æ¤œå‡º
* **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†**: 15 FPSä»¥ä¸Šã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ

==== æ¤œå‡ºå¯¾è±¡
[cols="2,3,3", options="header"]
|===
|æ¤œå‡ºã‚«ãƒ†ã‚´ãƒª |å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ |ä½¿ç”¨æŠ€è¡“
|**ğŸ‘¤ äººç‰©æ¤œå‡º** |äººã®å­˜åœ¨ãƒ»ä¸åœ¨ |YOLO v8 + MediaPipe
|**ğŸƒ å§¿å‹¢æ¤œå‡º** |é ­ãƒ»è‚©ãƒ»è…•ã®ä½ç½® |MediaPipe Pose
|**ğŸ“± ç‰©ä½“æ¤œå‡º** |ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ |YOLO v8
|**ğŸ–ï¸ æ‰‹æŒ‡æ¤œå‡º** |æ‰‹ã®ä½ç½®ãƒ»ã‚¸ã‚§ã‚¹ãƒãƒ£ |MediaPipe Hands
|**ğŸ˜Š è¡¨æƒ…æ¤œçŸ¥** |é¡”ã®å‘ããƒ»è¡¨æƒ… |MediaPipe Face Mesh
|===

=== ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åŸå‰‡

==== è¨­è¨ˆåŸå‰‡
* **ğŸ”„ çµ±åˆå‡¦ç†**: è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’çµ±åˆã—ãŸç·åˆåˆ¤å®š
* **âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **: ä½é…å»¶ã§ã®æ¤œå‡ºã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
* **ğŸ§  é©å¿œå­¦ç¿’**: å€‹äººå·®ãƒ»ç’°å¢ƒå¤‰åŒ–ã¸ã®è‡ªå‹•é©å¿œ
* **ğŸ”§ ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ**: ç‹¬ç«‹ã—ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›
* **ğŸ“Š å“è³ªä¿è¨¼**: æ¤œå‡ºç²¾åº¦ã®ç¶™ç¶šçš„ç›£è¦–ã¨æ”¹å–„

== ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

=== ğŸ“ æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ å…¨ä½“å›³

[mermaid]
....
graph TB
    subgraph "ğŸ“¹ Input Layer"
        CAM[Web Camera]
        FRAME[Frame Stream<br/>15+ FPS]
    end
    
    subgraph "ğŸ”§ Preprocessing Layer"
        PREP[Frame Preprocessor]
        OPT[AI Optimizer]
        CACHE[Frame Cache]
    end
    
    subgraph "ğŸ¤– AI Detection Layer"
        direction LR
        subgraph "YOLO v8 Pipeline"
            YOLO[YOLO v8 Model]
            NMS[NMS Processing]
            OBJ[Object Detection]
        end
        
        subgraph "MediaPipe Pipeline"
            MP_POSE[MediaPipe Pose]
            MP_HANDS[MediaPipe Hands]
            MP_FACE[MediaPipe Face]
        end
    end
    
    subgraph "ğŸ§  Intelligence Layer"
        SMOOTH[Detection Smoother<br/>ç‚¹æ»…æŠ‘åˆ¶]
        THRESH[Threshold Manager<br/>é©å¿œçš„é–¾å€¤]
        FUSION[Result Fusion<br/>çµæœçµ±åˆ]
    end
    
    subgraph "ğŸ“Š Processing Layer"
        RENDER[Detection Renderer<br/>å¯è¦–åŒ–]
        STATE[State Manager<br/>çŠ¶æ…‹ç®¡ç†]
        BROAD[Status Broadcaster<br/>é…ä¿¡]
    end
    
    subgraph "ğŸ’¾ Output Layer"
        WS[WebSocket<br/>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡]
        DB[Database<br/>ãƒ­ã‚°ä¿å­˜]
        API[REST API<br/>çŠ¶æ…‹å–å¾—]
    end
    
    %% ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    CAM --> FRAME
    FRAME --> PREP
    PREP --> OPT
    OPT --> CACHE
    
    CACHE --> YOLO
    CACHE --> MP_POSE
    CACHE --> MP_HANDS
    CACHE --> MP_FACE
    
    YOLO --> NMS
    NMS --> OBJ
    
    OBJ --> FUSION
    MP_POSE --> FUSION
    MP_HANDS --> FUSION
    MP_FACE --> FUSION
    
    FUSION --> SMOOTH
    SMOOTH --> THRESH
    THRESH --> RENDER
    THRESH --> STATE
    
    RENDER --> WS
    STATE --> BROAD
    STATE --> DB
    BROAD --> API
    
    %% ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    classDef input fill:#e6f3ff,stroke:#4488ff
    classDef preprocessing fill:#fff3e6,stroke:#ff8844
    classDef ai fill:#ffe6f3,stroke:#ff44aa
    classDef intelligence fill:#f3ffe6,stroke:#44aa44
    classDef processing fill:#f3e6ff,stroke:#8844ff
    classDef output fill:#fffce6,stroke:#ccaa44
    
    class CAM,FRAME input
    class PREP,OPT,CACHE preprocessing
    class YOLO,NMS,OBJ,MP_POSE,MP_HANDS,MP_FACE ai
    class SMOOTH,THRESH,FUSION intelligence
    class RENDER,STATE,BROAD processing
    class WS,DB,API output
....

=== ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°

[mermaid]
....
sequenceDiagram
    participant C as Camera
    participant P as Preprocessor
    participant Y as YOLO v8
    participant M as MediaPipe
    participant F as Fusion Engine
    participant S as Smoother
    participant R as Renderer
    participant O as Output
    
    Note over C,O: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºãƒ•ãƒ­ãƒ¼ (15 FPS)
    
    loop æ¯ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
        C->>P: ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
        P->>P: å‰å‡¦ç†ãƒ»æœ€é©åŒ–
        
        par ä¸¦åˆ—AIæ¨è«–
            P->>Y: YOLOæ¨è«–
            Y->>Y: ç‰©ä½“æ¤œå‡º
            Y->>F: æ¤œå‡ºçµæœ
        and
            P->>M: MediaPipeæ¨è«–
            M->>M: å§¿å‹¢ãƒ»æ‰‹ãƒ»é¡”æ¤œå‡º
            M->>F: ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯çµæœ
        end
        
        F->>F: çµæœçµ±åˆãƒ»ä¿¡é ¼åº¦è¨ˆç®—
        F->>S: çµ±åˆçµæœ
        S->>S: å¹³æ»‘åŒ–ãƒ»ç‚¹æ»…æŠ‘åˆ¶
        S->>R: å®‰å®šåŒ–çµæœ
        
        par å‡ºåŠ›å‡¦ç†
            R->>O: WebSocketé…ä¿¡
        and
            R->>O: çŠ¶æ…‹æ›´æ–°
        and
            R->>O: ãƒ­ã‚°ä¿å­˜
        end
    end
....

== ğŸ¤– YOLO v8 ç‰©ä½“æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 

=== ğŸ“Š YOLO v8 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph LR
    subgraph "ğŸ“¥ Input Processing"
        INPUT[Input Frame<br/>640x640]
        NORM[Normalization<br/>0-1 Scale]
        BATCH[Batch Processing]
    end
    
    subgraph "ğŸ§  YOLO v8 Network"
        BACKBONE[CSPDarknet Backbone<br/>ç‰¹å¾´æŠ½å‡º]
        NECK[FPN + PAN Neck<br/>ç‰¹å¾´èåˆ]
        HEAD[Detection Head<br/>äºˆæ¸¬å±¤]
    end
    
    subgraph "ğŸ“Š Post Processing"
        DECODE[Box Decoding<br/>åº§æ¨™å¤‰æ›]
        NMS[Non-Max Suppression<br/>é‡è¤‡é™¤å»]
        FILTER[Confidence Filtering<br/>ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿]
    end
    
    subgraph "ğŸ“‹ Output"
        PERSON[Person Detection<br/>äººç‰©æ¤œå‡º]
        PHONE[Phone Detection<br/>ã‚¹ãƒãƒ›æ¤œå‡º]
        OTHER[Other Objects<br/>ãã®ä»–ç‰©ä½“]
    end
    
    %% ãƒ•ãƒ­ãƒ¼
    INPUT --> NORM
    NORM --> BATCH
    BATCH --> BACKBONE
    BACKBONE --> NECK
    NECK --> HEAD
    HEAD --> DECODE
    DECODE --> NMS
    NMS --> FILTER
    FILTER --> PERSON
    FILTER --> PHONE
    FILTER --> OTHER
    
    %% ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    classDef input fill:#e6f3ff
    classDef network fill:#ffe6f3
    classDef postprocess fill:#f3ffe6
    classDef output fill:#fff3e6
    
    class INPUT,NORM,BATCH input
    class BACKBONE,NECK,HEAD network
    class DECODE,NMS,FILTER postprocess
    class PERSON,PHONE,OTHER output
....

=== âš™ï¸ YOLO è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

==== ãƒ¢ãƒ‡ãƒ«è¨­å®š
```python
# YOLO v8 è¨­å®š
model_config = {
    'model_type': 'yolov8n.pt',      # Nanoç‰ˆï¼ˆè»½é‡ãƒ»é«˜é€Ÿï¼‰
    'confidence': 0.5,               # ä¿¡é ¼åº¦é–¾å€¤
    'iou_threshold': 0.7,            # IoUé–¾å€¤ï¼ˆNMSï¼‰
    'max_detections': 10,            # æœ€å¤§æ¤œå‡ºæ•°
    'agnostic_nms': False,           # ã‚¯ãƒ©ã‚¹åˆ¥NMS
    'device': 'auto',                # GPU/CPUè‡ªå‹•é¸æŠ
    'half_precision': False,         # åŠç²¾åº¦è¨ˆç®—ï¼ˆCPUç„¡åŠ¹ï¼‰
    'verbose': False,                # ãƒ­ã‚°æŠ‘åˆ¶
}
```

==== æ¤œå‡ºå¯¾è±¡è¨­å®š
[cols="2,2,2,2", options="header"]
|===
|ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ |ã‚¯ãƒ©ã‚¹å |ä¿¡é ¼åº¦é–¾å€¤ |ç”¨é€”
|**äººç‰©** |`person` |0.5 |åœ¨å¸­ç›£è¦–
|**ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³** |`cell phone` |0.6 |æ³¨æ„æ•£æ¼«æ¤œå‡º
|**ãƒãƒ¼ãƒˆPC** |`laptop` |0.5 |ä½œæ¥­çŠ¶æ³åˆ†æ
|**ãƒã‚¦ã‚¹** |`mouse` |0.4 |ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨çŠ¶æ³
|===

=== ğŸ”§ YOLO æœ€é©åŒ–æ‰‹æ³•

==== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
[mermaid]
....
graph TD
    subgraph "ğŸ¯ æœ€é©åŒ–æˆ¦ç•¥"
        RESIZE[Dynamic Resizing<br/>å‹•çš„ãƒªã‚µã‚¤ã‚º]
        SKIP[Frame Skipping<br/>ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ã]
        BATCH[Batch Inference<br/>ãƒãƒƒãƒæ¨è«–]
        CACHE[Result Caching<br/>çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥]
    end
    
    subgraph "ğŸ“Š å“è³ªåˆ¶å¾¡"
        CONF[Confidence Tuning<br/>ä¿¡é ¼åº¦èª¿æ•´]
        NMS_OPT[NMS Optimization<br/>NMSæœ€é©åŒ–]
        FILTER[Post-Filter<br/>å¾Œå‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿]
    end
    
    subgraph "ğŸ”„ é©å¿œåˆ¶å¾¡"
        MONITOR[Performance Monitor<br/>æ€§èƒ½ç›£è¦–]
        AUTO[Auto Scaling<br/>è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«]
        DEGRADE[Graceful Degradation<br/>å“è³ªèª¿æ•´]
    end
    
    %% é–¢ä¿‚
    RESIZE --> MONITOR
    SKIP --> MONITOR
    BATCH --> MONITOR
    CACHE --> MONITOR
    
    MONITOR --> AUTO
    AUTO --> DEGRADE
    DEGRADE --> CONF
    DEGRADE --> NMS_OPT
    DEGRADE --> FILTER
    
    %% ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    classDef optimization fill:#e6f3ff
    classDef quality fill:#f3ffe6
    classDef adaptive fill:#ffe6f3
    
    class RESIZE,SKIP,BATCH,CACHE optimization
    class CONF,NMS_OPT,FILTER quality
    class MONITOR,AUTO,DEGRADE adaptive
....

==== å‹•çš„æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def optimize_yolo_inference(self, model, frame):
    """YOLOæ¨è«–ã®å‹•çš„æœ€é©åŒ–"""
    
    # 1. ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†æœ€é©åŒ–
    optimized_frame = self._optimize_frame_preprocessing(frame)
    
    # 2. ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
    if self.frame_skipper.should_skip():
        return self.last_yolo_results  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµæœä½¿ç”¨
    
    # 3. ãƒãƒƒãƒå‡¦ç†ï¼ˆè¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ åŒæ™‚å‡¦ç†ï¼‰
    if self.batch_processor.enabled:
        results = self._batch_inference(model, optimized_frame)
    else:
        results = model(optimized_frame, **self.yolo_predict_args)
    
    # 4. çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    self.last_yolo_results = results
    self.last_yolo_results_age = 0
    
    return results
```

== ğŸ­ MediaPipe å§¿å‹¢æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 

=== ğŸ—ï¸ MediaPipe ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph TB
    subgraph "ğŸ“¥ Input Layer"
        RGB[RGB Frame<br/>è‰²ç©ºé–“å¤‰æ›]
        PREPROCESS[Preprocessing<br/>æ­£è¦åŒ–ãƒ»ãƒªã‚µã‚¤ã‚º]
    end
    
    subgraph "ğŸ¤– MediaPipe Models"
        direction LR
        subgraph "Pose Detection"
            POSE_DET[Pose Detector<br/>BlazePose]
            POSE_LAND[Pose Landmarks<br/>33ãƒã‚¤ãƒ³ãƒˆ]
        end
        
        subgraph "Hand Detection"
            HAND_DET[Hand Detector<br/>Palm Detection]
            HAND_LAND[Hand Landmarks<br/>21ãƒã‚¤ãƒ³ãƒˆÃ—2]
        end
        
        subgraph "Face Detection"
            FACE_DET[Face Detector<br/>BlazeFace]
            FACE_MESH[Face Mesh<br/>468ãƒã‚¤ãƒ³ãƒˆ]
        end
    end
    
    subgraph "ğŸ“Š Analysis Layer"
        POSE_ANAL[Posture Analysis<br/>å§¿å‹¢è©•ä¾¡]
        HAND_ANAL[Hand Gesture<br/>ã‚¸ã‚§ã‚¹ãƒãƒ£èªè­˜]
        FACE_ANAL[Face Direction<br/>è¦–ç·šåˆ†æ]
    end
    
    subgraph "ğŸ¯ Output Layer"
        POSTURE[Posture Score<br/>å§¿å‹¢ã‚¹ã‚³ã‚¢]
        ATTENTION[Attention Level<br/>é›†ä¸­åº¦]
        PRESENCE[Presence Status<br/>åœ¨å¸­çŠ¶æ³]
    end
    
    %% ãƒ•ãƒ­ãƒ¼
    RGB --> PREPROCESS
    PREPROCESS --> POSE_DET
    PREPROCESS --> HAND_DET
    PREPROCESS --> FACE_DET
    
    POSE_DET --> POSE_LAND
    HAND_DET --> HAND_LAND
    FACE_DET --> FACE_MESH
    
    POSE_LAND --> POSE_ANAL
    HAND_LAND --> HAND_ANAL
    FACE_MESH --> FACE_ANAL
    
    POSE_ANAL --> POSTURE
    HAND_ANAL --> ATTENTION
    FACE_ANAL --> PRESENCE
    
    %% ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    classDef input fill:#e6f3ff
    classDef models fill:#ffe6f3
    classDef analysis fill:#f3ffe6
    classDef output fill:#fff3e6
    
    class RGB,PREPROCESS input
    class POSE_DET,POSE_LAND,HAND_DET,HAND_LAND,FACE_DET,FACE_MESH models
    class POSE_ANAL,HAND_ANAL,FACE_ANAL analysis
    class POSTURE,ATTENTION,PRESENCE output
....

=== ğŸ“ ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºè©³ç´°

==== Pose Landmarks (33ç‚¹)
[mermaid]
....
graph LR
    subgraph "ğŸ‘¤ äººä½“ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯"
        HEAD[Head Region<br/>é¼»ãƒ»ç›®ãƒ»è€³]
        SHOULDER[Shoulder Region<br/>å·¦å³è‚©]
        ARM[Arm Region<br/>è‚˜ãƒ»æ‰‹é¦–]
        TORSO[Torso Region<br/>èƒ¸ãƒ»è…°]
        LEG[Leg Region<br/>è†ãƒ»è¶³é¦–]
    end
    
    subgraph "ğŸ“Š å§¿å‹¢åˆ†æ"
        ANGLE[Joint Angles<br/>é–¢ç¯€è§’åº¦]
        ALIGN[Body Alignment<br/>ä½“è»¸æ•´åˆ—]
        LEAN[Forward Lean<br/>å‰å‚¾åº¦]
        SYMM[Symmetry<br/>å·¦å³å¯¾ç§°æ€§]
    end
    
    subgraph "ğŸ¯ è©•ä¾¡æŒ‡æ¨™"
        SCORE[Posture Score<br/>0-100ç‚¹]
        WARN[Warning Level<br/>è­¦å‘Šãƒ¬ãƒ™ãƒ«]
        ADVICE[Advice<br/>æ”¹å–„ææ¡ˆ]
    end
    
    %% é–¢ä¿‚
    HEAD --> ANGLE
    SHOULDER --> ALIGN
    ARM --> LEAN
    TORSO --> SYMM
    
    ANGLE --> SCORE
    ALIGN --> SCORE
    LEAN --> WARN
    SYMM --> ADVICE
....

==== å§¿å‹¢è©•ä¾¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def analyze_posture(self, pose_landmarks):
    """å§¿å‹¢åˆ†æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
    
    # 1. è‚©ã®æ°´å¹³åº¦ãƒã‚§ãƒƒã‚¯
    shoulder_alignment = self._calculate_shoulder_alignment(pose_landmarks)
    
    # 2. å‰å‚¾è§’åº¦è¨ˆç®—
    forward_lean = self._calculate_forward_lean(pose_landmarks)
    
    # 3. é ­éƒ¨ä½ç½®è©•ä¾¡
    head_position = self._evaluate_head_position(pose_landmarks)
    
    # 4. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
    posture_score = (
        shoulder_alignment * 0.3 +
        (100 - forward_lean * 2) * 0.4 +  # å‰å‚¾ãƒšãƒŠãƒ«ãƒ†ã‚£
        head_position * 0.3
    )
    
    return {
        'score': max(0, min(100, posture_score)),
        'shoulder_alignment': shoulder_alignment,
        'forward_lean': forward_lean,
        'head_position': head_position,
        'warnings': self._generate_posture_warnings(
            shoulder_alignment, forward_lean, head_position
        )
    }
```

=== ğŸ”§ MediaPipe æœ€é©åŒ–

==== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
[cols="2,2,2,3", options="header"]
|===
|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |è¨­å®šå€¤ |ç”¨é€” |æœ€é©åŒ–åŠ¹æœ
|**model_complexity** |0 |è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ |æ¨è«–é€Ÿåº¦å‘ä¸Š
|**min_detection_confidence** |0.7 |æ¤œå‡ºä¿¡é ¼åº¦ |èª¤æ¤œå‡ºå‰Šæ¸›
|**min_tracking_confidence** |0.7 |è¿½è·¡ä¿¡é ¼åº¦ |å®‰å®šæ€§å‘ä¸Š
|**smooth_landmarks** |True |ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å¹³æ»‘åŒ– |ã‚¸ãƒƒã‚¿ãƒ¼è»½æ¸›
|**enable_segmentation** |False |ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç„¡åŠ¹ |å‡¦ç†é‡å‰Šæ¸›
|===

== ğŸ§  æ¤œå‡ºçµæœçµ±åˆãƒ»æœ€é©åŒ–

=== ğŸ”— çµæœçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph TD
    subgraph "ğŸ“¥ Detection Inputs"
        YOLO_OUT[YOLO Results<br/>ç‰©ä½“æ¤œå‡ºçµæœ]
        MP_POSE[MediaPipe Pose<br/>å§¿å‹¢ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯]
        MP_HAND[MediaPipe Hands<br/>æ‰‹ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯]
        MP_FACE[MediaPipe Face<br/>é¡”ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯]
    end
    
    subgraph "ğŸ”„ Fusion Engine"
        VALIDATE[Data Validation<br/>ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼]
        CORRELATE[Cross-Correlation<br/>ç›¸é–¢åˆ†æ]
        WEIGHT[Confidence Weighting<br/>ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘]
        RESOLVE[Conflict Resolution<br/>çŸ›ç›¾è§£æ±º]
    end
    
    subgraph "ğŸ¯ Unified Output"
        PERSON[Person Status<br/>äººç‰©çŠ¶æ…‹]
        POSTURE[Posture Quality<br/>å§¿å‹¢å“è³ª]
        ATTENTION[Attention State<br/>æ³¨æ„çŠ¶æ…‹]
        OBJECTS[Object Presence<br/>ç‰©ä½“å­˜åœ¨]
    end
    
    subgraph "âš¡ Optimization"
        SMOOTH[Temporal Smoothing<br/>æ™‚ç³»åˆ—å¹³æ»‘åŒ–]
        THRESH[Adaptive Threshold<br/>é©å¿œçš„é–¾å€¤]
        CACHE[Result Caching<br/>çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥]
    end
    
    %% ãƒ•ãƒ­ãƒ¼
    YOLO_OUT --> VALIDATE
    MP_POSE --> VALIDATE
    MP_HAND --> VALIDATE
    MP_FACE --> VALIDATE
    
    VALIDATE --> CORRELATE
    CORRELATE --> WEIGHT
    WEIGHT --> RESOLVE
    
    RESOLVE --> PERSON
    RESOLVE --> POSTURE
    RESOLVE --> ATTENTION
    RESOLVE --> OBJECTS
    
    PERSON --> SMOOTH
    POSTURE --> THRESH
    ATTENTION --> CACHE
    
    %% ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    classDef input fill:#e6f3ff
    classDef fusion fill:#ffe6f3
    classDef output fill:#f3ffe6
    classDef optimization fill:#fff3e6
    
    class YOLO_OUT,MP_POSE,MP_HAND,MP_FACE input
    class VALIDATE,CORRELATE,WEIGHT,RESOLVE fusion
    class PERSON,POSTURE,ATTENTION,OBJECTS output
    class SMOOTH,THRESH,CACHE optimization
....

=== ğŸ”§ Detection Smootherï¼ˆæ¤œå‡ºå¹³æ»‘åŒ–ï¼‰

==== å¹³æ»‘åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
[mermaid]
....
graph LR
    subgraph "ğŸ“Š Input Smoothing"
        RAW[Raw Detection<br/>ç”Ÿæ¤œå‡ºçµæœ]
        BUFFER[Temporal Buffer<br/>æ™‚ç³»åˆ—ãƒãƒƒãƒ•ã‚¡]
        WEIGHT[Weighted Average<br/>é‡ã¿ä»˜ãå¹³å‡]
    end
    
    subgraph "ğŸ¯ Threshold Management"
        HIST[History Analysis<br/>å±¥æ­´åˆ†æ]
        ADAPT[Adaptive Threshold<br/>é©å¿œçš„é–¾å€¤]
        HYSTER[Hysteresis Filter<br/>ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹]
    end
    
    subgraph "âœ¨ Output Enhancement"
        STABLE[Stable Output<br/>å®‰å®šåŒ–å‡ºåŠ›]
        SUPPRESS[Flicker Suppression<br/>ç‚¹æ»…æŠ‘åˆ¶]
        CONF[Confidence Boost<br/>ä¿¡é ¼åº¦å‘ä¸Š]
    end
    
    %% ãƒ•ãƒ­ãƒ¼
    RAW --> BUFFER
    BUFFER --> WEIGHT
    WEIGHT --> HIST
    HIST --> ADAPT
    ADAPT --> HYSTER
    HYSTER --> STABLE
    STABLE --> SUPPRESS
    SUPPRESS --> CONF
....

==== å¹³æ»‘åŒ–è¨­å®š
```python
smoothing_config = {
    'temporal_window': 10,           # æ™‚ç³»åˆ—çª“ã‚µã‚¤ã‚ºï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰
    'confidence_threshold': 0.6,     # åŸºæœ¬ä¿¡é ¼åº¦é–¾å€¤
    'hysteresis_margin': 0.1,        # ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ãƒãƒ¼ã‚¸ãƒ³
    'max_change_rate': 0.3,          # æœ€å¤§å¤‰åŒ–ç‡
    'stability_factor': 0.8,         # å®‰å®šæ€§é‡ã¿
    'flicker_suppression': True,     # ç‚¹æ»…æŠ‘åˆ¶æœ‰åŠ¹
    'adaptive_threshold': True,      # é©å¿œçš„é–¾å€¤æœ‰åŠ¹
}
```

=== âš¡ AI Optimizerï¼ˆAIæœ€é©åŒ–ï¼‰

==== æœ€é©åŒ–æˆ¦ç•¥ãƒãƒƒãƒ—
[mermaid]
....
graph TD
    subgraph "ğŸ“Š Performance Monitoring"
        FPS[FPS Monitor<br/>ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆç›£è¦–]
        CPU[CPU Usage<br/>CPUä½¿ç”¨ç‡]
        MEM[Memory Usage<br/>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡]
        GPU[GPU Utilization<br/>GPUåˆ©ç”¨ç‡]
    end
    
    subgraph "ğŸ›ï¸ Dynamic Optimization"
        RESIZE[Dynamic Resize<br/>å‹•çš„ãƒªã‚µã‚¤ã‚º]
        SKIP[Frame Skip<br/>ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ã]
        BATCH[Batch Process<br/>ãƒãƒƒãƒå‡¦ç†]
        CACHE[Smart Cache<br/>ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥]
    end
    
    subgraph "ğŸ”„ Feedback Control"
        MEASURE[Performance Measure<br/>æ€§èƒ½æ¸¬å®š]
        ADJUST[Parameter Adjust<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´]
        VALIDATE[Quality Validate<br/>å“è³ªæ¤œè¨¼]
        OPTIMIZE[Re-optimize<br/>å†æœ€é©åŒ–]
    end
    
    %% åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼
    FPS --> MEASURE
    CPU --> MEASURE
    MEM --> MEASURE
    GPU --> MEASURE
    
    MEASURE --> ADJUST
    ADJUST --> RESIZE
    ADJUST --> SKIP
    ADJUST --> BATCH
    ADJUST --> CACHE
    
    RESIZE --> VALIDATE
    SKIP --> VALIDATE
    BATCH --> VALIDATE
    CACHE --> VALIDATE
    
    VALIDATE --> OPTIMIZE
    OPTIMIZE --> MEASURE
....

==== æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
class AIOptimizer:
    def __init__(self, config_manager):
        self.target_fps = 15.0
        self.performance_monitor = PerformanceMonitor()
        self.frame_skipper = FrameSkipper()
        self.batch_processor = BatchProcessor()
        
    def optimize_processing(self, frame):
        """å‹•çš„å‡¦ç†æœ€é©åŒ–"""
        
        # 1. ç¾åœ¨ã®æ€§èƒ½æ¸¬å®š
        current_fps = self.performance_monitor.get_current_fps()
        cpu_usage = self.performance_monitor.get_cpu_usage()
        
        # 2. æœ€é©åŒ–æˆ¦ç•¥æ±ºå®š
        if current_fps < self.target_fps * 0.8:
            # FPSä½ä¸‹æ™‚ã®æœ€é©åŒ–
            if cpu_usage > 80:
                self._enable_frame_skipping()
                self._reduce_frame_quality()
            else:
                self._enable_batch_processing()
        
        # 3. ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†æœ€é©åŒ–
        optimized_frame = self._optimize_frame_preprocessing(frame)
        
        return optimized_frame
```

== ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»å“è³ªç®¡ç†

=== âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

==== ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
[cols="2,2,2,3", options="header"]
|===
|ãƒ¡ãƒˆãƒªã‚¯ã‚¹ |ç›®æ¨™å€¤ |æ¸¬å®šé–“éš” |æœ€é©åŒ–æ‰‹æ³•
|**FPS** |15+ |ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  |ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ããƒ»ãƒªã‚µã‚¤ã‚º
|**æ¨è«–æ™‚é–“** |<50ms |æ¨è«–æ¯ |ãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ãƒ»ä¸¦åˆ—å‡¦ç†
|**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** |<2GB |1åˆ† |ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãƒ»GCæœ€é©åŒ–
|**CPUä½¿ç”¨ç‡** |<80% |30ç§’ |ãƒãƒƒãƒå‡¦ç†ãƒ»éåŒæœŸåŒ–
|**GPUä½¿ç”¨ç‡** |<90% |30ç§’ |ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ãƒ»æœ€é©åŒ–
|===

==== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
[mermaid]
....
graph TB
    subgraph "ğŸ“Š Metrics Collection"
        FRAME[Frame Metrics<br/>FPSãƒ»é…å»¶ãƒ»å“è³ª]
        SYSTEM[System Metrics<br/>CPUãƒ»ãƒ¡ãƒ¢ãƒªãƒ»GPU]
        MODEL[Model Metrics<br/>æ¨è«–æ™‚é–“ãƒ»ç²¾åº¦]
        QUALITY[Quality Metrics<br/>æ¤œå‡ºç‡ãƒ»ä¿¡é ¼åº¦]
    end
    
    subgraph "ğŸ” Analysis Engine"
        TREND[Trend Analysis<br/>å‚¾å‘åˆ†æ]
        ANOMALY[Anomaly Detection<br/>ç•°å¸¸æ¤œçŸ¥]
        PREDICT[Performance Prediction<br/>æ€§èƒ½äºˆæ¸¬]
    end
    
    subgraph "âš¡ Auto Optimization"
        SCALE[Auto Scaling<br/>è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°]
        TUNE[Parameter Tuning<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´]
        ALERT[Performance Alert<br/>æ€§èƒ½ã‚¢ãƒ©ãƒ¼ãƒˆ]
    end
    
    %% ãƒ•ãƒ­ãƒ¼
    FRAME --> TREND
    SYSTEM --> TREND
    MODEL --> ANOMALY
    QUALITY --> PREDICT
    
    TREND --> SCALE
    ANOMALY --> TUNE
    PREDICT --> ALERT
....

=== ğŸ¯ å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 

==== æ¤œå‡ºç²¾åº¦ç®¡ç†
[mermaid]
....
graph LR
    subgraph "ğŸ“ˆ Accuracy Monitoring"
        TRUE_POS[True Positive<br/>æ­£æ¤œå‡º]
        FALSE_POS[False Positive<br/>èª¤æ¤œå‡º]
        FALSE_NEG[False Negative<br/>æ¤œå‡ºæ¼ã‚Œ]
        CONF[Confidence Score<br/>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢]
    end
    
    subgraph "ğŸ“Š Quality Metrics"
        PRECISION[Precision<br/>é©åˆç‡]
        RECALL[Recall<br/>å†ç¾ç‡]
        F1[F1 Score<br/>ç·åˆæŒ‡æ¨™]
        mAP[mAP<br/>å¹³å‡ç²¾åº¦]
    end
    
    subgraph "ğŸ”§ Quality Control"
        THRESHOLD[Threshold Tuning<br/>é–¾å€¤èª¿æ•´]
        MODEL[Model Selection<br/>ãƒ¢ãƒ‡ãƒ«é¸æŠ]
        ENSEMBLE[Ensemble Method<br/>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«]
        FEEDBACK[Feedback Loop<br/>ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯]
    end
    
    %% é–¢ä¿‚
    TRUE_POS --> PRECISION
    FALSE_POS --> PRECISION
    FALSE_NEG --> RECALL
    CONF --> F1
    
    PRECISION --> THRESHOLD
    RECALL --> MODEL
    F1 --> ENSEMBLE
    mAP --> FEEDBACK
....

==== å“è³ªç®¡ç†è¨­å®š
```python
quality_config = {
    'target_precision': 0.85,       # ç›®æ¨™é©åˆç‡
    'target_recall': 0.80,          # ç›®æ¨™å†ç¾ç‡
    'min_confidence': 0.5,          # æœ€å°ä¿¡é ¼åº¦
    'quality_check_interval': 300,  # å“è³ªãƒã‚§ãƒƒã‚¯é–“éš”ï¼ˆç§’ï¼‰
    'auto_threshold_adjustment': True,  # è‡ªå‹•é–¾å€¤èª¿æ•´
    'performance_degradation_threshold': 0.1,  # æ€§èƒ½åŠ£åŒ–é–¾å€¤
}
```

== ğŸ”§ è¨­å®šãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

=== âš™ï¸ æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ è¨­å®š

==== åŸºæœ¬è¨­å®š
```yaml
# æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ è¨­å®š
detection:
  # YOLOè¨­å®š
  yolo:
    enabled: true
    model_path: "yolov8n.pt"
    confidence_threshold: 0.5
    iou_threshold: 0.7
    max_detections: 10
    device: "auto"  # auto/cpu/cuda/mps
    
  # MediaPipeè¨­å®š
  mediapipe:
    enabled: true
    pose:
      enabled: true
      model_complexity: 0
      min_detection_confidence: 0.7
      min_tracking_confidence: 0.7
      smooth_landmarks: true
    hands:
      enabled: false
      max_num_hands: 2
      min_detection_confidence: 0.5
    face:
      enabled: false
      max_num_faces: 1
      refine_landmarks: true
```

==== æœ€é©åŒ–è¨­å®š
```yaml
# AIæœ€é©åŒ–è¨­å®š
ai_optimization:
  enabled: true
  target_fps: 15.0
  min_fps: 10.0
  max_frame_skip: 3
  
  # ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–
  frame_optimization:
    auto_resize: true
    max_width: 640
    quality_vs_speed: 0.7  # 0:é€Ÿåº¦é‡è¦– 1:å“è³ªé‡è¦–
    
  # GPUè¨­å®š
  gpu:
    memory_limit: 0.8
    allow_growth: true
    mixed_precision: false
```

=== ğŸ›ï¸ ç‰©ä½“æ¤œå‡ºè¨­å®š

==== æ¤œå‡ºå¯¾è±¡è¨­å®š
```yaml
detection_objects:
  smartphone:
    enabled: true
    class_name: "cell phone"
    confidence_threshold: 0.6
    alert_threshold: 3.0  # ç§’
    
  laptop:
    enabled: false
    class_name: "laptop"
    confidence_threshold: 0.5
    
  mouse:
    enabled: false
    class_name: "mouse"
    confidence_threshold: 0.4
```

==== ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯è¨­å®š
```yaml
landmark_settings:
  pose:
    enabled: true
    draw_landmarks: true
    draw_connections: true
    landmark_color: [0, 255, 0]
    connection_color: [255, 0, 0]
    
  hands:
    enabled: false
    draw_landmarks: true
    landmark_color: [0, 0, 255]
    
  face:
    enabled: false
    draw_mesh: false
    mesh_color: [255, 255, 0]
```

== ğŸš€ é‹ç”¨ãƒ»ä¿å®ˆ

=== ğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚°

==== ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é …ç›®
[cols="2,3,2,2", options="header"]
|===
|ç›£è¦–é …ç›® |è©³ç´° |æ­£å¸¸ç¯„å›² |ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶
|**æ¤œå‡ºFPS** |å®Ÿéš›ã®å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ |15+ FPS |<10 FPS
|**æ¨è«–é…å»¶** |AIæ¨è«–ã®å‡¦ç†æ™‚é–“ |<50ms |>100ms
|**æ¤œå‡ºç²¾åº¦** |äººç‰©æ¤œå‡ºã®æˆåŠŸç‡ |>95% |<85%
|**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** |ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæ¶ˆè²» |<2GB |>3GB
|**GPUåˆ©ç”¨ç‡** |GPUä½¿ç”¨ç‡ |60-90% |>95%
|===

==== ãƒ­ã‚°å‡ºåŠ›ä¾‹
```python
# æ­£å¸¸æ™‚ã®ãƒ­ã‚°
logger.info("Detection performance - FPS: 15.2, Inference: 42ms, Accuracy: 96.3%")

# è­¦å‘Šãƒ­ã‚°
logger.warning("Performance degradation detected - FPS dropped to 8.5")

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
logger.error("YOLO inference failed - GPU memory insufficient, switching to CPU")
```

=== ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

==== ã‚ˆãã‚ã‚‹å•é¡Œã¨å¯¾å‡¦æ³•
[cols="2,3,3", options="header"]
|===
|å•é¡Œ |åŸå›  |å¯¾å‡¦æ³•
|**FPSä½ä¸‹** |CPU/GPUè² è·éå¤š |ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ããƒ»è§£åƒåº¦ä½ä¸‹
|**èª¤æ¤œå‡ºå¢—åŠ ** |ç…§æ˜ãƒ»èƒŒæ™¯å¤‰åŒ– |é–¾å€¤èª¿æ•´ãƒ»å†ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
|**ãƒ¡ãƒ¢ãƒªä¸è¶³** |é•·æ™‚é–“å®Ÿè¡Œãƒ»ãƒªãƒ¼ã‚¯ |å®šæœŸå†èµ·å‹•ãƒ»GCæœ€é©åŒ–
|**GPU ã‚¨ãƒ©ãƒ¼** |VRAMä¸è¶³ |CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
|**æ¤œå‡ºé…å»¶** |å‡¦ç†ãƒãƒƒã‚¯ãƒ­ã‚° |ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
|===

==== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´ã‚¬ã‚¤ãƒ‰
1. **ä½æ€§èƒ½ç’°å¢ƒ**: ãƒ•ãƒ¬ãƒ¼ãƒ é–“å¼•ããƒ»è§£åƒåº¦ä½ä¸‹ãƒ»MediaPipeç„¡åŠ¹
2. **é«˜ç²¾åº¦è¦æ±‚**: ä¿¡é ¼åº¦é–¾å€¤ä¸Šæ˜‡ãƒ»è¤‡æ•°ãƒ¢ãƒ‡ãƒ«çµ±åˆ
3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é‡è¦–**: ãƒãƒƒãƒå‡¦ç†ç„¡åŠ¹ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤§åŒ–
4. **çœé›»åŠ›ãƒ¢ãƒ¼ãƒ‰**: CPU ã®ã¿ä½¿ç”¨ãƒ»å‡¦ç†é–“éš”å»¶é•·

== ğŸ”® å°†æ¥æ‹¡å¼µè¨ˆç”»

=== ğŸš€ æŠ€è¡“ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

==== Phase 1: ç²¾åº¦å‘ä¸Š (çŸ­æœŸ)
* **ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«**: ä½œæ¥­ç’°å¢ƒç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
* **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: å¤šæ§˜ãªç’°å¢ƒãƒ»å§¿å‹¢ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
* **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š

==== Phase 2: æ©Ÿèƒ½æ‹¡å¼µ (ä¸­æœŸ)  
* **è¡Œå‹•äºˆæ¸¬**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬
* **æ„Ÿæƒ…èªè­˜**: è¡¨æƒ…ãƒ»å§¿å‹¢ã‹ã‚‰ã®æ„Ÿæƒ…çŠ¶æ…‹æ¨å®š
* **3Då§¿å‹¢**: ã‚¹ãƒ†ãƒ¬ã‚ªã‚«ãƒ¡ãƒ©ã«ã‚ˆã‚‹3Då§¿å‹¢è§£æ

==== Phase 3: é«˜åº¦åŒ– (é•·æœŸ)
* **ã‚¨ãƒƒã‚¸AI**: å°‚ç”¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã®æ¨è«–æœ€é©åŒ–
* **é€£åˆå­¦ç¿’**: ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·å­¦ç¿’ã®å®Ÿè£…
* **è‡ªå·±é©å¿œ**: å€‹äººç‰¹æ€§ã¸ã®è‡ªå‹•é©å¿œå­¦ç¿’

=== ğŸ”Œ æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ

[mermaid]
....
graph TB
    subgraph "ğŸ¤– Model Extensions"
        CUSTOM[Custom Models<br/>ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«]
        ENSEMBLE[Model Ensemble<br/>ãƒ¢ãƒ‡ãƒ«çµ±åˆ]
        QUANTIZED[Quantized Models<br/>é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«]
    end
    
    subgraph "ğŸ“Š Algorithm Extensions"
        TRACKING[Multi-Object Tracking<br/>å¤šç‰©ä½“è¿½è·¡]
        PREDICTION[Behavior Prediction<br/>è¡Œå‹•äºˆæ¸¬]
        ADAPTATION[Domain Adaptation<br/>ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ]
    end
    
    subgraph "ğŸ”§ Platform Extensions"
        EDGE[Edge Computing<br/>ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°]
        CLOUD[Cloud Integration<br/>ã‚¯ãƒ©ã‚¦ãƒ‰çµ±åˆ]
        MOBILE[Mobile Deployment<br/>ãƒ¢ãƒã‚¤ãƒ«å±•é–‹]
    end
    
    %% é–¢ä¿‚
    CUSTOM --> TRACKING
    ENSEMBLE --> PREDICTION
    QUANTIZED --> EDGE
    
    TRACKING --> CLOUD
    PREDICTION --> MOBILE
    ADAPTATION --> EDGE
....

== ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

=== ğŸ“– å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
* **<<backend-architecture>>**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
* **<<ai-ml-specifications>>**: AI/MLæŠ€è¡“ä»•æ§˜è©³ç´°
* **<<performance-optimization>>**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
* **<<configuration-guide>>**: è¨­å®šãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚¬ã‚¤ãƒ‰

=== ğŸ”— å¤–éƒ¨æŠ€è¡“è³‡æ–™
* [YOLO v8 Documentation](https://docs.ultralytics.com/)
* [MediaPipe Documentation](https://mediapipe.dev/)
* [PyTorch Documentation](https://pytorch.org/docs/)
* [OpenCV Documentation](https://docs.opencv.org/)

=== ğŸ“Š ç ”ç©¶ãƒ»è«–æ–‡
* YOLOv8 è«–æ–‡: "Real-Time Object Detection with YOLO"
* MediaPipe è«–æ–‡: "MediaPipe: A Framework for Building Perception Pipelines"
* å§¿å‹¢æ¨å®š: "BlazePose: On-device Real-time Body Pose tracking"

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber} 