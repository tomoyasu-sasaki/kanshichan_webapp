= ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - Data Management System
:author: KanshiChan Development Team
:version: 1.0
:date: 2024-12-27
:target_audience: ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…
:document_type: ã‚·ã‚¹ãƒ†ãƒ è©³ç´°ä»•æ§˜æ›¸
:toc: left
:toclevels: 4
:sectnums:
:source-highlighter: highlight.js

== ğŸ“‹ æ¦‚è¦

=== ğŸ“– ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€KanshiChanã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿ç®¡ç†æ©Ÿèƒ½ã«ãŠã‘ã‚‹è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®šç¾©ã—ã¾ã™ã€‚

**å¯¾è±¡èª­è€…**: ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€… +
**å‰æçŸ¥è­˜**: Python/Flaskã€Redisã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ +
**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<database-schema>>, <<behavior-analysis>>, <<performance-optimization>>

=== ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ ç›®çš„

* **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿åé›†**: é«˜æ€§èƒ½ãªç›£è¦–ãƒ‡ãƒ¼ã‚¿åé›†
* **åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆ¦ç•¥
* **ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†**: è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ»å‰Šé™¤
* **ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼**: æ¤œè¨¼ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

=== ğŸ—ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph TB
    subgraph "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"
        CAMERA[Camera Feed]
        DETECTOR[Object Detector]
        STATE[State Manager]
        MONITOR[Monitor Engine]
    end
    
    subgraph "ğŸ”„ ãƒ‡ãƒ¼ã‚¿åé›†å±¤"
        COLLECTOR[Data Collector]
        VALIDATOR[Data Validator]
        TRANSFORMER[Data Transformer]
        QUEUE[Processing Queue]
    end
    
    subgraph "ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å±¤"
        REDIS[Redis Cache]
        DATABASE[Primary Database]
        FILE_STORAGE[File Storage]
        ARCHIVE[Archive Storage]
    end
    
    subgraph "ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†"
        STORAGE_SVC[Storage Service]
        BACKUP_MGR[Backup Manager]
        ARCHIVE_MGR[Archive Manager]
        CLEANUP[Data Cleanup]
    end
    
    subgraph "ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹"
        API[Data Access API]
        QUERY[Query Engine]
        EXPORT[Export Service]
        ANALYTICS[Analytics Interface]
    end
    
    %% ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    CAMERA --> COLLECTOR
    DETECTOR --> COLLECTOR
    STATE --> COLLECTOR
    MONITOR --> COLLECTOR
    
    COLLECTOR --> VALIDATOR
    VALIDATOR --> TRANSFORMER
    TRANSFORMER --> QUEUE
    
    QUEUE --> REDIS
    QUEUE --> DATABASE
    QUEUE --> FILE_STORAGE
    
    DATABASE --> STORAGE_SVC
    STORAGE_SVC --> BACKUP_MGR
    STORAGE_SVC --> ARCHIVE_MGR
    STORAGE_SVC --> CLEANUP
    
    REDIS --> API
    DATABASE --> API
    FILE_STORAGE --> API
    
    API --> QUERY
    API --> EXPORT
    API --> ANALYTICS
    
    BACKUP_MGR --> ARCHIVE
    ARCHIVE_MGR --> ARCHIVE
    
    classDef source fill:#e3f2fd
    classDef collection fill:#e8f5e8
    classDef storage fill:#fff3e0
    classDef management fill:#f3e5f5
    classDef access fill:#fce4ec
    
    class CAMERA,DETECTOR,STATE,MONITOR source
    class COLLECTOR,VALIDATOR,TRANSFORMER,QUEUE collection
    class REDIS,DATABASE,FILE_STORAGE,ARCHIVE storage
    class STORAGE_SVC,BACKUP_MGR,ARCHIVE_MGR,CLEANUP management
    class API,QUERY,EXPORT,ANALYTICS access
....

== ğŸ“¥ ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 

=== ğŸ” ãƒ‡ãƒ¼ã‚¿åé›†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
classDiagram
    class DataCollector {
        -camera: Camera
        -detector: Detector
        -state_manager: StateManager
        -collection_interval: float
        -pending_data: List[Dict]
        -current_session_id: str
        
        +start_collection(session_id?: str): bool
        +stop_collection(): bool
        +collect_frame_data(): Dict[str, Any]
        +_flush_pending_data(): void
        +_save_batch_to_database(batch: List): void
        +get_session_summary(): Dict[str, Any]
    }
    
    class DataValidator {
        -validation_rules: Dict
        -schema_definitions: Dict
        
        +validate_detection_result(data: Dict): bool
        +validate_behavior_log(log: Dict): bool
        +sanitize_data(data: Dict): Dict
        +check_data_integrity(data: Dict): ValidationResult
    }
    
    class DataTransformer {
        -transformation_rules: Dict
        
        +transform_detection_data(data: Dict): Dict
        +normalize_timestamps(data: Dict): Dict
        +enrich_context_data(data: Dict): Dict
        +standardize_format(data: Dict): Dict
    }
    
    class ProcessingQueue {
        -queue: Queue
        -batch_size: int
        -flush_interval: int
        
        +add_data(data: Dict): void
        +get_batch(): List[Dict]
        +flush_queue(): void
        +get_queue_stats(): Dict
    }
    
    DataCollector --> DataValidator
    DataCollector --> DataTransformer
    DataCollector --> ProcessingQueue
....

=== ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿åé›†

**ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè£…**

```python
# backend/src/services/data/data_collector.py
class DataCollector:
    def __init__(self, 
                 camera: Camera,
                 detector: Detector,
                 state_manager: StateManager,
                 collection_interval: float = 2.0,
                 flask_app=None):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–"""
        self.camera = camera
        self.detector = detector
        self.state_manager = state_manager
        self.collection_interval = collection_interval
        self.flask_app = flask_app
        
        # ãƒ‡ãƒ¼ã‚¿åé›†åˆ¶å¾¡
        self._collecting = False
        self._collection_thread = None
        self._stop_event = threading.Event()
        self._data_lock = threading.Lock()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.current_session_id = None
        self.session_start_time = None
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
        self._pending_data = []
        self._batch_size = 5
        self._data_callbacks = []

    def start_collection(self, session_id: Optional[str] = None) -> bool:
        """ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹"""
        if self._collecting:
            logger.warning("Data collection already running")
            return False
        
        self.current_session_id = session_id or self._generate_session_id()
        self.session_start_time = datetime.utcnow()
        
        self._collecting = True
        self._stop_event.clear()
        
        # åé›†ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self._collection_thread = threading.Thread(
            target=self._collection_loop,
            name=f"DataCollector-{self.current_session_id}"
        )
        self._collection_thread.start()
        
        logger.info(f"Data collection started - Session: {self.current_session_id}")
        return True
```

**ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾©**

```python
# åé›†ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–æ§‹é€ 
BEHAVIOR_LOG_SCHEMA = {
    "behavior_id": str,  # UUID
    "timestamp": datetime,
    "session_id": str,
    "behavior_type": str,  # "presence", "absence", "smartphone_usage"
    "current_state": str,  # "PRESENT", "ABSENT", "SMARTPHONE_DETECTED" 
    "previous_state": str,
    "duration_seconds": int,
    "confidence_score": float,  # 0.0 - 1.0
    "trigger_event": str,  # "detection_result", "timer_expiry", "manual"
    "context_data": {
        "recent_detections": list,
        "environmental_factors": dict,
        "system_state": dict
    },
    "analysis_result": {
        "pattern_match": bool,
        "anomaly_score": float,
        "prediction_confidence": float,
        "recommendations": list
    }
}
```

=== ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»æ­£è¦åŒ–

```python
# backend/src/services/data/data_transformer.py
class DataTransformer:
    def transform_detection_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ãƒ»æ¨™æº–åŒ–"""
        try:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ­£è¦åŒ–
            normalized_timestamp = self._normalize_timestamp(
                raw_data.get('timestamp')
            )
            
            # æ¤œå‡ºçµæœã®æ§‹é€ åŒ–
            structured_detections = self._structure_detections(
                raw_data.get('detections', [])
            )
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
            normalized_confidence = self._normalize_confidence_scores(
                raw_data.get('confidence_scores', {})
            )
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µ
            enriched_context = self._enrich_context_data(
                raw_data.get('context', {}),
                raw_data.get('environment', {})
            )
            
            transformed_data = {
                'timestamp': normalized_timestamp,
                'detections': structured_detections,
                'confidence_scores': normalized_confidence,
                'context_data': enriched_context,
                'processing_metadata': {
                    'transformation_version': '1.0',
                    'processing_time': datetime.utcnow(),
                    'data_quality_score': self._calculate_quality_score(raw_data)
                }
            }
            
            return transformed_data
            
        except Exception as e:
            logger.error(f"Data transformation error: {e}")
            return raw_data  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```

== ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†

=== ğŸ—ƒï¸ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
classDiagram
    class StorageService {
        -backup_dir: Path
        -archive_dir: Path
        -retention_days: int
        
        +save_behavior_logs_batch(logs: List): bool
        +save_analysis_result(data: Dict): bool
        +compress_old_data(days_threshold: int): bool
        +delete_old_data(force: bool): Dict[str, int]
        +backup_database(): bool
        +restore_from_backup(file_path: str): bool
        +get_storage_stats(): Dict[str, Any]
    }
    
    class BackupManager {
        -backup_schedule: Dict
        -retention_policy: Dict
        -compression_settings: Dict
        
        +create_backup(backup_type: str): bool
        +restore_backup(backup_id: str): bool
        +list_backups(): List[BackupInfo]
        +cleanup_old_backups(): int
        +verify_backup_integrity(backup_id: str): bool
    }
    
    class ArchiveManager {
        -archive_strategy: ArchiveStrategy
        -compression_ratio: float
        
        +archive_data(date_range: DateRange): bool
        +retrieve_archived_data(query: ArchiveQuery): List
        +compress_archives(): bool
        +manage_archive_lifecycle(): Dict
    }
    
    class DataRetentionPolicy {
        -hot_data_retention: int
        -warm_data_retention: int
        -cold_data_retention: int
        
        +apply_retention_policy(): Dict
        +calculate_data_age(data: DataObject): int
        +determine_storage_tier(age: int): StorageTier
        +schedule_data_migration(): List[MigrationTask]
    }
    
    StorageService --> BackupManager
    StorageService --> ArchiveManager
    StorageService --> DataRetentionPolicy
....

=== ğŸ“š éšå±¤åŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æˆ¦ç•¥

[mermaid]
....
graph TB
    subgraph "ğŸ”¥ ãƒ›ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ (< 1æ—¥)"
        REDIS_HOT[Redis Cache<br/>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¯ã‚»ã‚¹]
        DB_HOT[Primary DB<br/>ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿]
    end
    
    subgraph "ğŸŒ¡ï¸ ã‚¦ã‚©ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ (1-30æ—¥)"
        DB_WARM[Primary DB<br/>ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ã]
        CACHE_WARM[Read-Only Cache<br/>ã‚¯ã‚¨ãƒªæœ€é©åŒ–]
    end
    
    subgraph "â„ï¸ ã‚³ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ (30-90æ—¥)"
        DB_COLD[Archive DB<br/>åœ§ç¸®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸]
        FILE_COLD[Compressed Files<br/>JSON.gzå½¢å¼]
    end
    
    subgraph "ğŸ§Š ãƒ•ãƒ­ãƒ¼ã‚ºãƒ³ãƒ‡ãƒ¼ã‚¿ (> 90æ—¥)"
        ARCHIVE[Deep Archive<br/>é•·æœŸä¿å­˜]
        BACKUP[Backup Storage<br/>ç½å®³å¾©æ—§ç”¨]
    end
    
    %% ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    REDIS_HOT --> DB_HOT
    DB_HOT --> DB_WARM
    DB_WARM --> CACHE_WARM
    
    DB_WARM --> DB_COLD
    CACHE_WARM --> FILE_COLD
    
    DB_COLD --> ARCHIVE
    FILE_COLD --> BACKUP
    
    %% ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
    REDIS_HOT -.->|é«˜é »åº¦èª­ã¿å–ã‚Š| CLIENT[Client Apps]
    DB_HOT -.->|ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°| CLIENT
    CACHE_WARM -.->|åˆ†æã‚¯ã‚¨ãƒª| ANALYTICS[Analytics Engine]
    FILE_COLD -.->|ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ| REPORTS[Report Generator]
    
    classDef hot fill:#ffebee
    classDef warm fill:#fff3e0
    classDef cold fill:#e8f5e8
    classDef frozen fill:#f3e5f5
    
    class REDIS_HOT,DB_HOT hot
    class DB_WARM,CACHE_WARM warm
    class DB_COLD,FILE_COLD cold
    class ARCHIVE,BACKUP frozen
....

=== ğŸ’½ åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ä¿å­˜

**ãƒãƒƒãƒä¿å­˜å®Ÿè£…**

```python
# backend/src/services/data/storage_service.py
class StorageService:
    def save_behavior_logs_batch(self, logs_data: List[Dict[str, Any]]) -> bool:
        """è¡Œå‹•ãƒ­ã‚°ã®ãƒãƒƒãƒä¿å­˜"""
        if not logs_data:
            return True
        
        try:
            db.session.begin()
            
            logs_to_save = []
            for log_data in logs_data:
                behavior_log = BehaviorLog.create_log(**log_data)
                logs_to_save.append(behavior_log)
            
            # ãƒãƒƒãƒä¿å­˜ã§æ€§èƒ½å‘ä¸Š
            db.session.add_all(logs_to_save)
            db.session.commit()
            
            logger.debug(f"Batch saved {len(logs_to_save)} behavior logs")
            return True
            
        except Exception as e:
            logger.error(f"Error in batch save behavior logs: {e}")
            db.session.rollback()
            return False
    
    def compress_old_data(self, days_threshold: int = 30) -> bool:
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã®åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days_threshold)
            
            # å¤ã„è¡Œå‹•ãƒ­ã‚°ã‚’å–å¾—
            old_logs = BehaviorLog.query.filter(
                BehaviorLog.created_at < cutoff_date
            ).all()
            
            if not old_logs:
                logger.info("No old logs to compress")
                return True
            
            # æ—¥ä»˜åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            logs_by_date = {}
            for log in old_logs:
                date_key = log.created_at.date().isoformat()
                if date_key not in logs_by_date:
                    logs_by_date[date_key] = []
                logs_by_date[date_key].append(log.to_dict())
            
            # æ—¥ä»˜åˆ¥ã«åœ§ç¸®ä¿å­˜
            compressed_count = 0
            for date_key, logs in logs_by_date.items():
                if self._compress_logs_for_date(date_key, logs):
                    compressed_count += len(logs)
            
            logger.info(f"Compressed {compressed_count} old logs")
            return True
            
        except Exception as e:
            logger.error(f"Error compressing old data: {e}")
            return False
```

=== ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 

```python
# backend/src/services/data/backup_manager.py
class BackupManager:
    def create_incremental_backup(self) -> bool:
        """å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ"""
        try:
            # æœ€å¾Œã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»¥é™ã®å¤‰æ›´ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®š
            last_backup_time = self._get_last_backup_timestamp()
            
            # å¤‰æ›´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            changed_logs = BehaviorLog.query.filter(
                BehaviorLog.updated_at > last_backup_time
            ).all()
            
            changed_analyses = AnalysisResult.query.filter(
                AnalysisResult.updated_at > last_backup_time
            ).all()
            
            # å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            backup_data = {
                'backup_type': 'incremental',
                'base_backup_id': self._get_latest_full_backup_id(),
                'timestamp': datetime.utcnow().isoformat(),
                'changed_logs': [log.to_dict() for log in changed_logs],
                'changed_analyses': [analysis.to_dict() for analysis in changed_analyses],
                'checksum': self._calculate_checksum(changed_logs + changed_analyses)
            }
            
            # åœ§ç¸®ä¿å­˜
            backup_file = self._save_compressed_backup(backup_data, 'incremental')
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
            self._update_backup_metadata(backup_file, backup_data)
            
            logger.info(f"Incremental backup created: {backup_file}")
            return True
            
        except Exception as e:
            logger.error(f"Error creating incremental backup: {e}")
            return False
```

== ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†

=== ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

[mermaid]
....
flowchart TD
    INPUT[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿] --> SCHEMA[ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼]
    
    SCHEMA --> VALID{æ¤œè¨¼çµæœ}
    VALID -->|Pass| INTEGRITY[æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯]
    VALID -->|Fail| REJECT[ãƒ‡ãƒ¼ã‚¿æ‹’å¦]
    
    INTEGRITY --> CONSISTENT{æ•´åˆæ€§OK?}
    CONSISTENT -->|Yes| QUALITY[å“è³ªè©•ä¾¡]
    CONSISTENT -->|No| REPAIR[ãƒ‡ãƒ¼ã‚¿ä¿®å¾©]
    
    REPAIR --> REPAIRABLE{ä¿®å¾©å¯èƒ½?}
    REPAIRABLE -->|Yes| QUALITY
    REPAIRABLE -->|No| FLAG[å“è³ªãƒ•ãƒ©ã‚°ä»˜ä¸]
    
    QUALITY --> SCORE[å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º]
    SCORE --> THRESHOLD{é–¾å€¤ãƒã‚§ãƒƒã‚¯}
    
    THRESHOLD -->|Pass| ACCEPT[ãƒ‡ãƒ¼ã‚¿å—å®¹]
    THRESHOLD -->|Fail| FLAG
    
    REJECT --> LOG[ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°]
    FLAG --> ACCEPT
    ACCEPT --> STORE[ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä¿å­˜]
    
    LOG --> MONITOR[ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆ]
    
    classDef process fill:#e3f2fd
    classDef decision fill:#fff3e0
    classDef terminal fill:#e8f5e8
    classDef error fill:#ffebee
    
    class SCHEMA,INTEGRITY,REPAIR,QUALITY,SCORE process
    class VALID,CONSISTENT,REPAIRABLE,THRESHOLD decision
    class ACCEPT,STORE terminal
    class REJECT,LOG,MONITOR error
....

**ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡å®Ÿè£…**

```python
# backend/src/services/data/data_quality.py
class DataQualityManager:
    def __init__(self):
        self.quality_rules = self._load_quality_rules()
        self.quality_metrics = QualityMetrics()
        
    def evaluate_data_quality(self, data: Dict[str, Any]) -> QualityReport:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã®åŒ…æ‹¬çš„è©•ä¾¡"""
        try:
            report = QualityReport()
            
            # 1. å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
            completeness_score = self._check_completeness(data)
            report.add_metric('completeness', completeness_score)
            
            # 2. æ­£ç¢ºæ€§ãƒã‚§ãƒƒã‚¯
            accuracy_score = self._check_accuracy(data)
            report.add_metric('accuracy', accuracy_score)
            
            # 3. ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            consistency_score = self._check_consistency(data)
            report.add_metric('consistency', consistency_score)
            
            # 4. é©æ™‚æ€§ãƒã‚§ãƒƒã‚¯
            timeliness_score = self._check_timeliness(data)
            report.add_metric('timeliness', timeliness_score)
            
            # 5. å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            validity_score = self._check_validity(data)
            report.add_metric('validity', validity_score)
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
            overall_score = self._calculate_overall_score(report.metrics)
            report.set_overall_score(overall_score)
            
            # å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š
            quality_level = self._determine_quality_level(overall_score)
            report.set_quality_level(quality_level)
            
            return report
            
        except Exception as e:
            logger.error(f"Error evaluating data quality: {e}")
            return QualityReport(error=str(e))
    
    def _check_completeness(self, data: Dict[str, Any]) -> float:
        """ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        required_fields = self.quality_rules.get('required_fields', [])
        present_fields = [field for field in required_fields if field in data and data[field] is not None]
        
        if not required_fields:
            return 1.0
        
        completeness = len(present_fields) / len(required_fields)
        
        # é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®é‡ã¿ä»˜ã‘
        weighted_score = 0.0
        total_weight = 0.0
        
        for field in required_fields:
            weight = self.quality_rules.get('field_weights', {}).get(field, 1.0)
            total_weight += weight
            
            if field in data and data[field] is not None:
                weighted_score += weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
```

=== ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–

```python
# backend/src/services/data/quality_monitor.py
class QualityMonitor:
    def __init__(self):
        self.quality_thresholds = {
            'critical': 0.95,  # 95%ä»¥ä¸Š
            'warning': 0.85,   # 85%ä»¥ä¸Š
            'acceptable': 0.70  # 70%ä»¥ä¸Š
        }
        self.alert_manager = AlertManager()
        
    def monitor_continuous_quality(self, data_stream: Iterator[Dict]) -> None:
        """ç¶™ç¶šçš„ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–"""
        quality_window = deque(maxlen=100)  # ç›´è¿‘100ä»¶ã®å“è³ªã‚¹ã‚³ã‚¢
        
        for data in data_stream:
            try:
                # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
                quality_report = self.quality_manager.evaluate_data_quality(data)
                quality_score = quality_report.overall_score
                
                quality_window.append(quality_score)
                
                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                self._check_quality_thresholds(quality_score, data)
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
                if len(quality_window) >= 10:
                    trend = self._analyze_quality_trend(quality_window)
                    self._handle_quality_trend(trend)
                
                # ç•°å¸¸æ¤œçŸ¥
                if len(quality_window) >= 20:
                    anomaly_score = self._detect_quality_anomaly(quality_window)
                    if anomaly_score > 0.8:
                        self._trigger_quality_alert('anomaly', {
                            'anomaly_score': anomaly_score,
                            'current_quality': quality_score,
                            'data_sample': data
                        })
                
            except Exception as e:
                logger.error(f"Error in quality monitoring: {e}")
                self._trigger_quality_alert('monitoring_error', {'error': str(e)})
```

== ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

=== â° è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«

[mermaid]
....
gantt
    title ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    dateFormat X
    axisFormat %d
    
    section ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿
    ãƒ‡ãƒ¼ã‚¿åé›†     :active, collect, 0, 1
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† :active, process, 0, 1
    ãƒ›ãƒƒãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ :hot, 0, 1
    
    section ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
    ã‚¦ã‚©ãƒ¼ãƒ ç§»è¡Œ   :warm, 1, 7
    ã‚³ãƒ¼ãƒ«ãƒ‰ç§»è¡Œ   :cold, 7, 30
    ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç§»è¡Œ :archive, 30, 90
    
    section ãƒ‡ãƒ¼ã‚¿ä¿æŒ
    ãƒ›ãƒƒãƒˆä¿æŒ    :crit, hot-retain, 0, 1
    ã‚¦ã‚©ãƒ¼ãƒ ä¿æŒ  :warm-retain, 1, 30
    ã‚³ãƒ¼ãƒ«ãƒ‰ä¿æŒ  :cold-retain, 30, 90
    ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¿æŒ :archive-retain, 90, 365
    
    section ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ :cleanup-temp, 0, 1
    ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ :cleanup-logs, 1, 7
    å¤ãƒ‡ãƒ¼ã‚¿å‰Šé™¤   :crit, cleanup-old, 365, 400
....

**ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†å®Ÿè£…**

```python
# backend/src/services/data/lifecycle_manager.py
class DataLifecycleManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.retention_policy = self._load_retention_policy()
        self.migration_scheduler = MigrationScheduler()
        
    async def apply_lifecycle_policies(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒãƒªã‚·ãƒ¼ã®é©ç”¨"""
        results = {
            'migrations_completed': 0,
            'data_deleted': 0,
            'archives_created': 0,
            'errors': []
        }
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã®å®Ÿè¡Œ
            migration_results = await self._execute_data_migrations()
            results['migrations_completed'] = migration_results['completed']
            
            # 2. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
            archive_results = await self._create_scheduled_archives()
            results['archives_created'] = archive_results['created']
            
            # 3. æœŸé™åˆ‡ã‚Œãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
            deletion_results = await self._delete_expired_data()
            results['data_deleted'] = deletion_results['deleted']
            
            # 4. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
            optimization_results = await self._optimize_storage()
            results['storage_optimized'] = optimization_results['optimized_size']
            
            logger.info(f"Lifecycle policies applied: {results}")
            return results
            
        except Exception as e:
            logger.error(f"Error applying lifecycle policies: {e}")
            results['errors'].append(str(e))
            return results
    
    async def _execute_data_migrations(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿éšå±¤é–“ã®ç§»è¡Œå®Ÿè¡Œ"""
        migrations = {
            'hot_to_warm': 0,
            'warm_to_cold': 0,
            'cold_to_archive': 0,
            'completed': 0
        }
        
        try:
            # ãƒ›ãƒƒãƒˆ â†’ ã‚¦ã‚©ãƒ¼ãƒ ç§»è¡Œ
            hot_cutoff = datetime.utcnow() - timedelta(hours=24)
            hot_data = await self._get_data_by_age('hot', hot_cutoff)
            
            for data_batch in self._batch_data(hot_data, 1000):
                await self._migrate_data_batch(data_batch, 'hot', 'warm')
                migrations['hot_to_warm'] += len(data_batch)
            
            # ã‚¦ã‚©ãƒ¼ãƒ  â†’ ã‚³ãƒ¼ãƒ«ãƒ‰ç§»è¡Œ
            warm_cutoff = datetime.utcnow() - timedelta(days=30)
            warm_data = await self._get_data_by_age('warm', warm_cutoff)
            
            for data_batch in self._batch_data(warm_data, 500):
                await self._migrate_data_batch(data_batch, 'warm', 'cold')
                migrations['warm_to_cold'] += len(data_batch)
            
            # ã‚³ãƒ¼ãƒ«ãƒ‰ â†’ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç§»è¡Œ
            cold_cutoff = datetime.utcnow() - timedelta(days=90)
            cold_data = await self._get_data_by_age('cold', cold_cutoff)
            
            for data_batch in self._batch_data(cold_data, 100):
                await self._migrate_data_batch(data_batch, 'cold', 'archive')
                migrations['cold_to_archive'] += len(data_batch)
            
            migrations['completed'] = (
                migrations['hot_to_warm'] + 
                migrations['warm_to_cold'] + 
                migrations['cold_to_archive']
            )
            
            return migrations
            
        except Exception as e:
            logger.error(f"Error in data migrations: {e}")
            return migrations
```

=== ğŸ§¹ è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```python
# backend/src/services/data/cleanup_service.py
class CleanupService:
    def __init__(self):
        self.cleanup_rules = self._load_cleanup_rules()
        self.safety_checks = SafetyChecks()
        
    async def execute_cleanup_cycle(self) -> CleanupReport:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ"""
        report = CleanupReport()
        
        try:
            # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if not await self.safety_checks.verify_system_state():
                raise CleanupError("System not in safe state for cleanup")
            
            # 1. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            temp_cleanup = await self._cleanup_temporary_files()
            report.add_cleanup_result('temporary_files', temp_cleanup)
            
            # 2. æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤
            cache_cleanup = await self._cleanup_expired_cache()
            report.add_cleanup_result('expired_cache', cache_cleanup)
            
            # 3. å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            log_cleanup = await self._cleanup_old_logs()
            report.add_cleanup_result('old_logs', log_cleanup)
            
            # 4. å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
            orphan_cleanup = await self._cleanup_orphaned_data()
            report.add_cleanup_result('orphaned_data', orphan_cleanup)
            
            # 5. é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»
            duplicate_cleanup = await self._cleanup_duplicate_data()
            report.add_cleanup_result('duplicate_data', duplicate_cleanup)
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµ±è¨ˆã®æ›´æ–°
            await self._update_cleanup_statistics(report)
            
            logger.info(f"Cleanup cycle completed: {report.summary}")
            return report
            
        except Exception as e:
            logger.error(f"Error in cleanup cycle: {e}")
            report.add_error(str(e))
            return report
```

== ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

=== âš¡ ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–

[mermaid]
....
graph LR
    subgraph "ğŸ“Š ã‚¯ã‚¨ãƒªæœ€é©åŒ–"
        INDEX[ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥]
        PARTITION[ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°]
        CACHE[ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥]
    end
    
    subgraph "ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–"
        COMPRESS[ãƒ‡ãƒ¼ã‚¿åœ§ç¸®]
        BATCH[ãƒãƒƒãƒå‡¦ç†]
        ASYNC[éåŒæœŸå‡¦ç†]
    end
    
    subgraph "ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥"
        L1[L1: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³]
        L2[L2: Redis]
        L3[L3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹]
    end
    
    INDEX --> PARTITION
    PARTITION --> CACHE
    
    COMPRESS --> BATCH
    BATCH --> ASYNC
    
    L1 --> L2
    L2 --> L3
    
    CACHE -.-> L1
    ASYNC -.-> L2
    
    classDef query fill:#e3f2fd
    classDef storage fill:#e8f5e8
    classDef cache fill:#fff3e0
    
    class INDEX,PARTITION,CACHE query
    class COMPRESS,BATCH,ASYNC storage
    class L1,L2,L3 cache
....

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–å®Ÿè£…**

```python
# backend/src/services/data/performance_monitor.py
class DataPerformanceMonitor:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.performance_analyzer = PerformanceAnalyzer()
        
    async def monitor_data_operations(self) -> PerformanceReport:
        """ãƒ‡ãƒ¼ã‚¿æ“ä½œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
        report = PerformanceReport()
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            db_metrics = await self._collect_database_metrics()
            report.add_section('database', db_metrics)
            
            # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            cache_metrics = await self._collect_cache_metrics()
            report.add_section('cache', cache_metrics)
            
            # 3. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸I/O ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            io_metrics = await self._collect_io_metrics()
            report.add_section('storage_io', io_metrics)
            
            # 4. ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            query_metrics = await self._collect_query_metrics()
            report.add_section('queries', query_metrics)
            
            # 5. å…¨ä½“çš„ãªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
            throughput_metrics = await self._collect_throughput_metrics()
            report.add_section('throughput', throughput_metrics)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            analysis = await self.performance_analyzer.analyze_report(report)
            report.set_analysis(analysis)
            
            # æœ€é©åŒ–æ¨å¥¨äº‹é …
            recommendations = await self._generate_optimization_recommendations(report)
            report.set_recommendations(recommendations)
            
            return report
            
        except Exception as e:
            logger.error(f"Error monitoring data performance: {e}")
            return PerformanceReport(error=str(e))
```

== ğŸ”’ ãƒ‡ãƒ¼ã‚¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

=== ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…

```python
# backend/src/services/data/security_manager.py
class DataSecurityManager:
    def __init__(self):
        self.encryption_service = EncryptionService()
        self.access_control = AccessControlManager()
        self.audit_logger = AuditLogger()
        
    async def secure_data_operation(self, 
                                  operation: str, 
                                  data: Dict[str, Any],
                                  user_context: UserContext) -> SecureOperationResult:
        """å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿æ“ä½œã®å®Ÿè¡Œ"""
        try:
            # 1. ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®ç¢ºèª
            if not await self.access_control.check_permission(user_context, operation):
                raise UnauthorizedError(f"User not authorized for operation: {operation}")
            
            # 2. ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿå¯†æ€§ãƒã‚§ãƒƒã‚¯
            sensitivity_level = self._classify_data_sensitivity(data)
            
            # 3. æš—å·åŒ–ã®é©ç”¨
            if sensitivity_level >= SensitivityLevel.CONFIDENTIAL:
                encrypted_data = await self.encryption_service.encrypt_data(data)
            else:
                encrypted_data = data
            
            # 4. æ“ä½œã®å®Ÿè¡Œ
            result = await self._execute_secure_operation(operation, encrypted_data)
            
            # 5. ç›£æŸ»ãƒ­ã‚°ã®è¨˜éŒ²
            await self.audit_logger.log_data_operation({
                'operation': operation,
                'user': user_context.user_id,
                'timestamp': datetime.utcnow(),
                'data_size': len(str(data)),
                'sensitivity_level': sensitivity_level.value,
                'success': True
            })
            
            return SecureOperationResult(success=True, result=result)
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¨˜éŒ²
            await self.audit_logger.log_security_event({
                'event_type': 'data_operation_failed',
                'operation': operation,
                'user': user_context.user_id,
                'error': str(e),
                'timestamp': datetime.utcnow()
            })
            
            raise SecurityError(f"Secure data operation failed: {e}")
```

== ğŸ“Š ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

=== ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å¥å…¨æ€§ç›£è¦–

```python
# backend/src/services/data/health_monitor.py
class DataHealthMonitor:
    def __init__(self):
        self.health_checks = self._initialize_health_checks()
        self.alert_thresholds = self._load_alert_thresholds()
        
    async def perform_comprehensive_health_check(self) -> HealthReport:
        """åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        health_report = HealthReport()
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¥å…¨æ€§
            db_health = await self._check_database_health()
            health_report.add_check('database', db_health)
            
            # 2. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¥å…¨æ€§
            storage_health = await self._check_storage_health()
            health_report.add_check('storage', storage_health)
            
            # 3. ãƒ‡ãƒ¼ã‚¿å“è³ªå¥å…¨æ€§
            quality_health = await self._check_data_quality_health()
            health_report.add_check('data_quality', quality_health)
            
            # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¥å…¨æ€§
            backup_health = await self._check_backup_health()
            health_report.add_check('backup', backup_health)
            
            # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¥å…¨æ€§
            performance_health = await self._check_performance_health()
            health_report.add_check('performance', performance_health)
            
            # å…¨ä½“çš„ãªå¥å…¨æ€§ã‚¹ã‚³ã‚¢ã®ç®—å‡º
            overall_score = health_report.calculate_overall_score()
            health_report.set_overall_score(overall_score)
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆã®ç”Ÿæˆ
            alerts = await self._generate_health_alerts(health_report)
            health_report.set_alerts(alerts)
            
            return health_report
            
        except Exception as e:
            logger.error(f"Error in comprehensive health check: {e}")
            return HealthReport(error=str(e))
```

== ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒªã‚½ãƒ¼ã‚¹

=== ğŸ“– å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

* **<<database-schema>>**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆè©³ç´°
* **<<behavior-analysis>>**: è¡Œå‹•åˆ†æãƒ‡ãƒ¼ã‚¿ä»•æ§˜
* **<<performance-optimization>>**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
* **<<configuration-guide>>**: ãƒ‡ãƒ¼ã‚¿ç®¡ç†è¨­å®š

=== ğŸ”— å¤–éƒ¨ãƒªã‚½ãƒ¼ã‚¹

* **Redis Documentation**: https://redis.io/documentation
* **SQLAlchemy ORM**: https://docs.sqlalchemy.org/
* **Data Quality Framework**: https://github.com/great-expectations/great_expectations
* **Backup Best Practices**: https://www.postgresql.org/docs/current/backup.html

=== ğŸ› ï¸ é–‹ç™ºæ”¯æ´ãƒ„ãƒ¼ãƒ«

```bash
# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
python scripts/check_data_quality.py --days 7

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
python scripts/create_backup.py --type full

# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±è¨ˆ
python scripts/storage_stats.py --format json

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
python scripts/cleanup_data.py --dry-run
```

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: 2024-12-27