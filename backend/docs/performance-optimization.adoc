= âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - Performance Optimization System
:author: KanshiChan Development Team
:version: 1.0
:date: 2024-12-27
:target_audience: ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…
:document_type: ã‚·ã‚¹ãƒ†ãƒ è©³ç´°ä»•æ§˜æ›¸
:toc: left
:toclevels: 4
:sectnums:
:source-highlighter: highlight.js

== ğŸ“‹ æ¦‚è¦

=== ğŸ“– ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€KanshiChanã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æ©Ÿèƒ½ã«ãŠã‘ã‚‹è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®šç¾©ã—ã¾ã™ã€‚

**å¯¾è±¡èª­è€…**: ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€… +
**å‰æçŸ¥è­˜**: Pythonæœ€é©åŒ–ã€AI/MLæ¨è«–ã€ãƒ¡ãƒ¢ãƒªç®¡ç† +
**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<ai-ml-specifications>>, <<system-design>>, <<configuration-guide>>

=== ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ ç›®çš„

* **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½**: 15FPSä»¥ä¸Šã®å®‰å®šã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
* **æ™ºæ…§çš„è² è·èª¿æ•´**: å‹•çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
* **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªä½¿ç”¨ã¨ç®¡ç†
* **ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–**: CPU/GPU/MPSè‡ªå‹•é¸æŠã¨æœ€é©åŒ–

=== ğŸ—ï¸ æœ€é©åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
graph TB
    subgraph "ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"
        PERF_MON[Performance Monitor]
        FPS_TRACKER[FPS Tracker]
        MEM_MONITOR[Memory Monitor]
        CPU_MONITOR[CPU Monitor]
    end
    
    subgraph "âš¡ AIæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"
        AI_OPT[AI Optimizer]
        FRAME_SKIP[Frame Skipper]
        BATCH_PROC[Batch Processor]
        DEVICE_MGR[Device Manager]
    end
    
    subgraph "ğŸ’¾ ãƒ¡ãƒ¢ãƒªç®¡ç†"
        MEM_MGR[Memory Manager]
        CACHE_MGR[Cache Manager]
        GC_OPT[GC Optimizer]
        LRU_CACHE[LRU Cache]
    end
    
    subgraph "ğŸ”§ å‹•çš„æœ€é©åŒ–"
        DYN_ADJUSTER[Dynamic Adjuster]
        THRESHOLD_MGR[Threshold Manager]
        FEEDBACK_CTRL[Feedback Control]
        AUTO_TUNER[Auto Tuner]
    end
    
    subgraph "ğŸ›ï¸ è¨­å®šç®¡ç†"
        CONFIG_MGR[Config Manager]
        PERF_PROFILE[Performance Profile]
        ENV_DETECTOR[Environment Detector]
        PRESET_MGR[Preset Manager]
    end
    
    %% ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
    PERF_MON --> FPS_TRACKER
    PERF_MON --> MEM_MONITOR
    PERF_MON --> CPU_MONITOR
    
    FPS_TRACKER --> DYN_ADJUSTER
    MEM_MONITOR --> MEM_MGR
    CPU_MONITOR --> AI_OPT
    
    AI_OPT --> FRAME_SKIP
    AI_OPT --> BATCH_PROC
    AI_OPT --> DEVICE_MGR
    
    MEM_MGR --> CACHE_MGR
    MEM_MGR --> GC_OPT
    MEM_MGR --> LRU_CACHE
    
    DYN_ADJUSTER --> THRESHOLD_MGR
    DYN_ADJUSTER --> FEEDBACK_CTRL
    DYN_ADJUSTER --> AUTO_TUNER
    
    CONFIG_MGR --> PERF_PROFILE
    CONFIG_MGR --> ENV_DETECTOR
    CONFIG_MGR --> PRESET_MGR
    
    FEEDBACK_CTRL --> AI_OPT
    AUTO_TUNER --> MEM_MGR
    ENV_DETECTOR --> DEVICE_MGR
    
    classDef monitoring fill:#e3f2fd
    classDef optimization fill:#e8f5e8
    classDef memory fill:#fff3e0
    classDef dynamic fill:#f3e5f5
    classDef config fill:#fce4ec
    
    class PERF_MON,FPS_TRACKER,MEM_MONITOR,CPU_MONITOR monitoring
    class AI_OPT,FRAME_SKIP,BATCH_PROC,DEVICE_MGR optimization
    class MEM_MGR,CACHE_MGR,GC_OPT,LRU_CACHE memory
    class DYN_ADJUSTER,THRESHOLD_MGR,FEEDBACK_CTRL,AUTO_TUNER dynamic
    class CONFIG_MGR,PERF_PROFILE,ENV_DETECTOR,PRESET_MGR config
....

== âš¡ AIæ¨è«–æœ€é©åŒ–

=== ğŸ§  AIæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³

[mermaid]
....
classDiagram
    class AIOptimizer {
        -performance_monitor: PerformanceMonitor
        -frame_skipper: FrameSkipper
        -batch_processor: BatchProcessor
        -last_yolo_results: Any
        -last_yolo_results_age: int
        
        +optimize_yolo_inference(model, frame): Optional[Any]
        +optimize_mediapipe_pipeline(model, frame): Optional[Any]
        +update_performance_stats(): void
        +get_performance_stats(): Dict[str, Any]
        +log_performance_summary(): void
    }
    
    class PerformanceMonitor {
        -frame_times: deque
        -inference_times: deque
        -memory_usage: deque
        -last_frame_time: float
        
        +record_frame(): void
        +record_inference_time(time: float): void
        +get_current_fps(): float
        +get_avg_inference_time(): float
        +get_memory_usage(): float
        +get_stats(): Dict[str, float]
    }
    
    class FrameSkipper {
        -skip_rate: int
        -frame_counter: int
        -target_fps: float
        -min_fps: float
        -max_skip_rate: int
        
        +should_process_frame(current_fps: float): bool
        +_adjust_skip_rate(current_fps: float): void
    }
    
    class BatchProcessor {
        -batch_size: int
        -timeout_ms: int
        -frame_buffer: List
        -enabled: bool
        
        +add_frame(frame: ndarray): Optional[List[ndarray]]
    }
    
    AIOptimizer --> PerformanceMonitor
    AIOptimizer --> FrameSkipper
    AIOptimizer --> BatchProcessor
....

=== ğŸ¯ YOLOæœ€é©åŒ–æˆ¦ç•¥

**çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãYOLOæ¨è«–**

```python
# backend/src/core/ai_optimizer.py
class AIOptimizer:
    def optimize_yolo_inference(self, model, frame: np.ndarray) -> Optional[Any]:
        """YOLOæ¨è«–ã®æœ€é©åŒ–ï¼ˆæç”»ç¶™ç¶šæ€§ã‚’è€ƒæ…®ã—ãŸæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            inference_start = time.time()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å¹´é½¢ã‚’æ›´æ–°
            self.last_yolo_results_age += 1
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
            current_fps = self.performance_monitor.get_current_fps()
            should_skip = not self.frame_skipper.should_process_frame(current_fps)
            
            if should_skip:
                # ã‚¹ã‚­ãƒƒãƒ—æ™‚ã‚‚å‰å›ã®æ¤œå‡ºçµæœã‚’è¿”ã™ãƒ¢ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                if (self.last_yolo_results is not None and 
                    self.last_yolo_results_age <= self.max_cache_age):
                    # å‰å›çµæœã‚’è¿”ã—ã¦æç”»ç¶™ç¶šæ€§ã‚’ç¶­æŒ
                    logger.debug(f"Using cached YOLO results (age: {self.last_yolo_results_age} frames)")
                    return self.last_yolo_results
                else:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ã™ãã‚‹å ´åˆã¯Noneã‚’è¿”ã™
                    logger.debug(f"Cache too old or empty, returning None (age: {self.last_yolo_results_age})")
                return None
                
            # ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†ã®æœ€é©åŒ–
            optimized_frame = self._optimize_frame_preprocessing(frame)
            
            # YOLOæ¨è«–å®Ÿè¡Œ
            with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
                results = model(optimized_frame, verbose=False)
                
            # æˆåŠŸã—ãŸæ¨è«–çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.last_yolo_results = results
            self.last_yolo_results_age = 0  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
                
            inference_time = time.time() - inference_start
            self.performance_monitor.record_inference_time(inference_time)
            
            return results
            
        except Exception as e:
            model_error = wrap_exception(
                e, ModelError,
                "YOLO inference optimization failed",
                details={'fallback_to_standard': True}
            )
            logger.warning(f"YOLO optimization error: {model_error.to_dict()}")
            return None
```

=== ğŸ“Š å‹•çš„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—

[mermaid]
....
stateDiagram-v2
    [*] --> Monitoring
    Monitoring --> LowFPS : FPS < 5.0
    Monitoring --> HighFPS : FPS > 18.0
    Monitoring --> OptimalFPS : 5.0 â‰¤ FPS â‰¤ 18.0
    
    LowFPS --> IncreaseSkip : skip_rate += 1
    IncreaseSkip --> Monitoring : max 5x
    
    HighFPS --> DecreaseSkip : skip_rate -= 1
    DecreaseSkip --> Monitoring : min 1x
    
    OptimalFPS --> Monitoring : ç¶­æŒ
    
    note right of LowFPS
        ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆå¢—åŠ 
        å‡¦ç†è² è·è»½æ¸›
    end note
    
    note left of HighFPS
        ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆæ¸›å°‘
        å“è³ªå‘ä¸Š
    end note
....

**ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—å®Ÿè£…**

```python
# backend/src/core/ai_optimizer.py
class FrameSkipper:
    def should_process_frame(self, current_fps: float) -> bool:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        self.frame_counter += 1
        
        # å‹•çš„ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆèª¿æ•´
        self._adjust_skip_rate(current_fps)
        
        # ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ã„ã¦å‡¦ç†åˆ¤å®š
        return (self.frame_counter % self.skip_rate) == 0
        
    def _adjust_skip_rate(self, current_fps: float) -> None:
        """ç¾åœ¨ã®FPSã«åŸºã¥ã„ã¦ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å‹•çš„èª¿æ•´"""
        current_time = time.time()
        
        if current_time - self.last_adjustment < self.adjustment_interval:
            return
            
        self.last_adjustment = current_time
        
        if current_fps < self.min_fps:
            # FPSãŒä½ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä¸Šã’ã‚‹
            self.skip_rate = min(self.skip_rate + 1, self.max_skip_rate)
            logger.debug(f"Low FPS detected ({current_fps:.1f}), increasing skip rate to {self.skip_rate}")
        elif current_fps > self.target_fps * 1.2:
            # FPSãŒååˆ†é«˜ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä¸‹ã’ã‚‹
            self.skip_rate = max(self.skip_rate - 1, 1)
            logger.debug(f"High FPS detected ({current_fps:.1f}), decreasing skip rate to {self.skip_rate}")
```

== ğŸ’¾ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

=== ğŸ§¹ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

[mermaid]
....
classDiagram
    class MemoryManager {
        -frame_cache: Dict
        -frame_cache_stats: Dict
        -gc_threshold: float
        -max_memory_mb: float
        -monitor_interval: float
        -cache_max_size: int
        
        +cache_frame(key: str, frame: ndarray): bool
        +get_cached_frame(key: str): Optional[ndarray]
        +clear_frame_cache(): void
        +monitor_memory_usage(): float
        +cleanup_memory(): void
        +get_memory_stats(): Dict[str, Any]
    }
    
    class LRUCache {
        -max_size: int
        -cache: OrderedDict
        -access_order: deque
        
        +get(key: str): Optional[Any]
        +put(key: str, value: Any): void
        +evict_lru(): void
        +clear(): void
        +size(): int
    }
    
    class GCOptimizer {
        -gc_interval: float
        -memory_threshold: float
        -last_gc_time: float
        
        +should_trigger_gc(): bool
        +force_gc(): void
        +optimize_gc_settings(): void
        +get_gc_stats(): Dict[str, Any]
    }
    
    class CacheMonitor {
        -hit_rate: float
        -miss_rate: float
        -eviction_count: int
        
        +record_hit(): void
        +record_miss(): void
        +record_eviction(): void
        +get_cache_metrics(): Dict[str, float]
    }
    
    MemoryManager --> LRUCache
    MemoryManager --> GCOptimizer
    MemoryManager --> CacheMonitor
....

=== ğŸ”„ LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

```python
# backend/src/core/memory_manager.py
class MemoryManager:
    def cache_frame(self, key: str, frame: np.ndarray) -> bool:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            if self.is_memory_usage_high():
                self.cleanup_memory()
                
            frame_size = frame.nbytes / 1024 / 1024  # MB
            
            if frame_size > self.max_frame_size_mb:
                logger.warning(f"Frame too large for cache: {frame_size:.1f}MB")
                return False
                
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self.frame_cache) >= self.cache_max_size:
                self._evict_oldest_frame()
                
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.frame_cache[key] = frame.copy()
            self.frame_cache_stats[key] = {
                'timestamp': time.time(),
                'size_mb': frame_size,
                'access_count': 0
            }
            
            logger.debug(f"Frame cached: {key} ({frame_size:.1f}MB)")
            return True
            
        except Exception as e:
            logger.error(f"Error caching frame {key}: {e}")
            return False
    
    def cleanup_memory(self) -> None:
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ"""
        try:
            # 1. ãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            old_cache_size = len(self.frame_cache)
            self._cleanup_old_cached_frames()
            
            # 2. å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            if self.is_memory_usage_critical():
                import gc
                collected = gc.collect()
                logger.info(f"Forced GC collected {collected} objects")
            
            # 3. çµ±è¨ˆæ›´æ–°
            new_cache_size = len(self.frame_cache)
            logger.info(f"Memory cleanup: cache {old_cache_size} â†’ {new_cache_size}")
            
        except Exception as e:
            logger.error(f"Error in memory cleanup: {e}")
```

=== ğŸ“Š ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

```python
# backend/src/core/memory_manager.py
class MemoryManager:
    def monitor_memory_usage(self) -> float:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            memory_mb = memory_info.rss / 1024 / 1024
            
            # ä½¿ç”¨ç‡è¨ˆç®—
            system_memory = psutil.virtual_memory()
            usage_percent = (memory_mb / (system_memory.total / 1024 / 1024)) * 100
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if usage_percent > self.gc_threshold:
                logger.warning(f"High memory usage: {usage_percent:.1f}%")
                if usage_percent > 90.0:  # ç·Šæ€¥æ™‚
                    self.cleanup_memory()
            
            # çµ±è¨ˆæ›´æ–°
            self.memory_usage_history.append({
                'timestamp': time.time(),
                'memory_mb': memory_mb,
                'usage_percent': usage_percent
            })
            
            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self.memory_usage_history) > 100:
                self.memory_usage_history.pop(0)
            
            return memory_mb
            
        except Exception as e:
            logger.error(f"Error monitoring memory usage: {e}")
            return 0.0
```

== ğŸ›ï¸ å‹•çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´

=== ğŸ“ˆ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ 

[mermaid]
....
flowchart TD
    MEASURE[æ€§èƒ½æ¸¬å®š] --> ANALYZE[æ€§èƒ½åˆ†æ]
    ANALYZE --> DECISION{èª¿æ•´åˆ¤å®š}
    
    DECISION -->|FPSä½ä¸‹| OPTIMIZE_FPS[FPSæœ€é©åŒ–]
    DECISION -->|ãƒ¡ãƒ¢ãƒªä¸è¶³| OPTIMIZE_MEM[ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–]
    DECISION -->|CPUéè² è·| OPTIMIZE_CPU[CPUæœ€é©åŒ–]
    DECISION -->|æ­£å¸¸| MAINTAIN[ç¾çŠ¶ç¶­æŒ]
    
    OPTIMIZE_FPS --> INCREASE_SKIP[ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆå¢—åŠ ]
    OPTIMIZE_FPS --> REDUCE_QUALITY[å“è³ªä½ä¸‹]
    OPTIMIZE_FPS --> BATCH_ENABLE[ãƒãƒƒãƒå‡¦ç†æœ‰åŠ¹]
    
    OPTIMIZE_MEM --> CLEAR_CACHE[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢]
    OPTIMIZE_MEM --> FORCE_GC[å¼·åˆ¶GC]
    OPTIMIZE_MEM --> REDUCE_BUFFER[ãƒãƒƒãƒ•ã‚¡å‰Šæ¸›]
    
    OPTIMIZE_CPU --> ASYNC_PROCESSING[éåŒæœŸå‡¦ç†]
    OPTIMIZE_CPU --> THREAD_LIMIT[ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™]
    OPTIMIZE_CPU --> PRIORITY_ADJUST[å„ªå…ˆåº¦èª¿æ•´]
    
    INCREASE_SKIP --> VALIDATE[åŠ¹æœæ¤œè¨¼]
    REDUCE_QUALITY --> VALIDATE
    BATCH_ENABLE --> VALIDATE
    CLEAR_CACHE --> VALIDATE
    FORCE_GC --> VALIDATE
    REDUCE_BUFFER --> VALIDATE
    ASYNC_PROCESSING --> VALIDATE
    THREAD_LIMIT --> VALIDATE
    PRIORITY_ADJUST --> VALIDATE
    MAINTAIN --> VALIDATE
    
    VALIDATE --> WAIT[å¾…æ©ŸæœŸé–“]
    WAIT --> MEASURE
    
    classDef measure fill:#e3f2fd
    classDef decision fill:#fff3e0
    classDef optimize fill:#e8f5e8
    classDef action fill:#f3e5f5
    classDef validate fill:#fce4ec
    
    class MEASURE,ANALYZE measure
    class DECISION decision
    class OPTIMIZE_FPS,OPTIMIZE_MEM,OPTIMIZE_CPU optimize
    class INCREASE_SKIP,REDUCE_QUALITY,BATCH_ENABLE,CLEAR_CACHE,FORCE_GC,REDUCE_BUFFER,ASYNC_PROCESSING,THREAD_LIMIT,PRIORITY_ADJUST,MAINTAIN action
    class VALIDATE,WAIT validate
....

=== âš™ï¸ è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£…

```python
# backend/src/core/performance_tuner.py
class PerformanceTuner:
    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.optimization_history = deque(maxlen=50)
        self.tuning_metrics = TuningMetrics()
        
    async def auto_tune_performance(self) -> TuningResult:
        """è‡ªå‹•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        try:
            # ç¾åœ¨ã®æ€§èƒ½æ¸¬å®š
            current_metrics = await self._collect_performance_metrics()
            
            # æœ€é©åŒ–ã®å¿…è¦æ€§ã‚’åˆ¤å®š
            optimization_needs = self._analyze_optimization_needs(current_metrics)
            
            if not optimization_needs:
                return TuningResult(status='optimal', changes=[])
            
            # æœ€é©åŒ–æˆ¦ç•¥ã®æ±ºå®š
            strategies = self._determine_optimization_strategies(optimization_needs)
            
            # æœ€é©åŒ–ã®å®Ÿè¡Œ
            applied_changes = []
            for strategy in strategies:
                change_result = await self._apply_optimization_strategy(strategy)
                applied_changes.append(change_result)
                
                # åŠ¹æœæ¤œè¨¼
                if await self._validate_optimization_effect(strategy, change_result):
                    logger.info(f"Optimization strategy '{strategy.name}' successful")
                else:
                    # åŠ¹æœãŒç„¡ã„å ´åˆã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    await self._rollback_optimization(change_result)
                    logger.warning(f"Optimization strategy '{strategy.name}' ineffective, rolled back")
            
            # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´ã®æ›´æ–°
            self._update_tuning_history(current_metrics, applied_changes)
            
            return TuningResult(
                status='optimized',
                changes=applied_changes,
                metrics=current_metrics
            )
            
        except Exception as e:
            logger.error(f"Error in auto-tuning performance: {e}")
            return TuningResult(status='error', error=str(e))
```

== ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–

=== ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

[mermaid]
....
classDiagram
    class DeviceManager {
        -available_devices: List[str]
        -current_device: str
        -device_capabilities: Dict
        -optimization_profiles: Dict
        
        +detect_optimal_device(): str
        +setup_device_optimization(device: str): void
        +get_device_performance(): DeviceMetrics
        +switch_device(device: str): bool
    }
    
    class CUDAOptimizer {
        -gpu_memory_limit: float
        -mixed_precision: bool
        -tensorrt_enabled: bool
        
        +optimize_gpu_memory(): void
        +enable_mixed_precision(): void
        +setup_tensorrt(): void
        +get_gpu_stats(): Dict
    }
    
    class MPSOptimizer {
        -mps_memory_fraction: float
        -graph_optimization: bool
        
        +optimize_mps_settings(): void
        +enable_graph_optimization(): void
        +get_mps_performance(): Dict
    }
    
    class CPUOptimizer {
        -thread_count: int
        -simd_optimization: bool
        -numa_affinity: bool
        
        +optimize_thread_count(): void
        +enable_simd(): void
        +setup_numa_affinity(): void
        +get_cpu_stats(): Dict
    }
    
    DeviceManager --> CUDAOptimizer
    DeviceManager --> MPSOptimizer
    DeviceManager --> CPUOptimizer
....

=== ğŸš€ GPUæœ€é©åŒ–æˆ¦ç•¥

```python
# backend/src/core/device_optimizer.py
class DeviceOptimizer:
    def optimize_for_gpu(self, device_type: str) -> OptimizationResult:
        """GPUå‘ã‘æœ€é©åŒ–è¨­å®š"""
        try:
            optimization_config = {}
            
            if device_type == 'cuda':
                # CUDAæœ€é©åŒ–
                optimization_config.update({
                    'memory_limit': 0.8,  # GPU ãƒ¡ãƒ¢ãƒªã®80%ä½¿ç”¨
                    'mixed_precision': True,  # åŠç²¾åº¦è¨ˆç®—
                    'tensorrt': True,  # TensorRTæœ€é©åŒ–
                    'batch_size': 8,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ 
                    'async_execution': True  # éåŒæœŸå®Ÿè¡Œ
                })
                
                # GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
                torch.cuda.empty_cache()
                torch.cuda.set_per_process_memory_fraction(0.8)
                
            elif device_type == 'mps':
                # Apple MPSæœ€é©åŒ–
                optimization_config.update({
                    'memory_fraction': 0.7,  # MPS ãƒ¡ãƒ¢ãƒªã®70%ä½¿ç”¨
                    'graph_optimization': True,  # è¨ˆç®—ã‚°ãƒ©ãƒ•æœ€é©åŒ–
                    'batch_size': 4,  # é©åº¦ãªãƒãƒƒãƒã‚µã‚¤ã‚º
                    'precision': 'fp16'  # åŠç²¾åº¦
                })
                
            # æœ€é©åŒ–è¨­å®šã‚’é©ç”¨
            self._apply_optimization_config(optimization_config)
            
            return OptimizationResult(
                success=True,
                device=device_type,
                config=optimization_config
            )
            
        except Exception as e:
            logger.error(f"GPU optimization failed for {device_type}: {e}")
            return OptimizationResult(success=False, error=str(e))
```

== ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

=== ğŸ“ˆ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

[mermaid]
....
graph TB
    subgraph "ğŸ“Š ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹"
        FPS[FPS Monitor<br/>15+ target]
        LATENCY[Inference Latency<br/><50ms target]
        MEMORY[Memory Usage<br/><2GB target]
        CPU[CPU Utilization<br/><80% target]
    end
    
    subgraph "ğŸ›ï¸ åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ "
        CONTROLLER[Performance Controller]
        ADJUSTER[Dynamic Adjuster]
        ALERTER[Alert Manager]
        LOGGER[Performance Logger]
    end
    
    subgraph "âš¡ æœ€é©åŒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"
        SKIP_ADJUST[Skip Rate Adjust]
        MEMORY_CLEAN[Memory Cleanup]
        DEVICE_SWITCH[Device Switch]
        BATCH_TOGGLE[Batch Toggle]
    end
    
    FPS --> CONTROLLER
    LATENCY --> CONTROLLER
    MEMORY --> CONTROLLER
    CPU --> CONTROLLER
    
    CONTROLLER --> ADJUSTER
    CONTROLLER --> ALERTER
    CONTROLLER --> LOGGER
    
    ADJUSTER --> SKIP_ADJUST
    ADJUSTER --> MEMORY_CLEAN
    ADJUSTER --> DEVICE_SWITCH
    ADJUSTER --> BATCH_TOGGLE
    
    classDef monitor fill:#e3f2fd
    classDef control fill:#e8f5e8
    classDef action fill:#fff3e0
    
    class FPS,LATENCY,MEMORY,CPU monitor
    class CONTROLLER,ADJUSTER,ALERTER,LOGGER control
    class SKIP_ADJUST,MEMORY_CLEAN,DEVICE_SWITCH,BATCH_TOGGLE action
....

=== ğŸ“Š æ€§èƒ½æŒ‡æ¨™ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
# backend/src/core/performance_dashboard.py
class PerformanceDashboard:
    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ã®å–å¾—"""
        try:
            current_stats = self.performance_monitor.get_stats()
            memory_stats = self.memory_manager.get_memory_stats()
            optimization_stats = self.ai_optimizer.get_performance_stats()
            
            # æ€§èƒ½è©•ä¾¡
            performance_score = self._calculate_performance_score(current_stats)
            health_status = self._determine_health_status(current_stats)
            
            summary = {
                'timestamp': datetime.utcnow().isoformat(),
                'performance_score': performance_score,
                'health_status': health_status,
                'metrics': {
                    'fps': {
                        'current': current_stats['fps'],
                        'target': 15.0,
                        'status': 'good' if current_stats['fps'] >= 15.0 else 'warning'
                    },
                    'latency': {
                        'current_ms': current_stats['avg_inference_ms'],
                        'target_ms': 50.0,
                        'status': 'good' if current_stats['avg_inference_ms'] <= 50.0 else 'warning'
                    },
                    'memory': {
                        'current_mb': memory_stats['current_usage_mb'],
                        'target_mb': 2048.0,
                        'status': 'good' if memory_stats['current_usage_mb'] <= 2048.0 else 'warning'
                    }
                },
                'optimizations': {
                    'skip_rate': optimization_stats['skip_rate'],
                    'cache_hit_rate': optimization_stats.get('cache_hit_rate', 0.0),
                    'batch_enabled': optimization_stats['batch_enabled'],
                    'device': optimization_stats.get('device', 'cpu')
                },
                'recommendations': self._generate_performance_recommendations(current_stats)
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating performance summary: {e}")
            return {'error': str(e)}
```

== ğŸ”§ è¨­å®šæœ€é©åŒ–

=== âš™ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# ä½æ€§èƒ½ç’°å¢ƒå‘ã‘è¨­å®š
performance_profiles:
  low_end:
    ai_optimization:
      target_fps: 8.0
      frame_optimization:
        auto_resize: true
        max_width: 320
        max_height: 240
        quality_vs_speed: 0.2
    yolo:
      device:
        preferred: "cpu"
      optimization:
        half_precision: false
    mediapipe:
      pose:
        model_complexity: 0
      hands:
        enabled: false
      face:
        enabled: false
    monitoring:
      interval_seconds: 5

  # é«˜æ€§èƒ½ç’°å¢ƒå‘ã‘è¨­å®š
  high_end:
    ai_optimization:
      target_fps: 30.0
      frame_optimization:
        max_width: 1280
        max_height: 720
        quality_vs_speed: 0.8
    yolo:
      device:
        preferred: "cuda"
      optimization:
        half_precision: true
        tensorrt: true
    mediapipe:
      pose:
        model_complexity: 2
      hands:
        enabled: true
      face:
        enabled: true
    ai_optimization:
      gpu:
        memory_limit: 0.9
      memory:
        cache_size_mb: 1024
```

=== ğŸ›ï¸ å‹•çš„è¨­å®šèª¿æ•´

```python
# backend/src/core/config_optimizer.py
class ConfigOptimizer:
    def optimize_config_for_environment(self) -> ConfigOptimizationResult:
        """ç’°å¢ƒã«å¿œã˜ãŸè¨­å®šæœ€é©åŒ–"""
        try:
            # ç’°å¢ƒæ¤œå‡º
            env_specs = self._detect_environment_specifications()
            
            # æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®é¸æŠ
            optimal_profile = self._select_optimal_profile(env_specs)
            
            # è¨­å®šã®å‹•çš„èª¿æ•´
            optimized_config = self._adjust_config_dynamically(optimal_profile, env_specs)
            
            # è¨­å®šã®é©ç”¨
            self.config_manager.update_config(optimized_config)
            
            logger.info(f"Config optimized for environment: {optimal_profile}")
            
            return ConfigOptimizationResult(
                success=True,
                profile=optimal_profile,
                config=optimized_config,
                environment=env_specs
            )
            
        except Exception as e:
            logger.error(f"Config optimization failed: {e}")
            return ConfigOptimizationResult(success=False, error=str(e))
    
    def _detect_environment_specifications(self) -> EnvironmentSpecs:
        """ç’°å¢ƒä»•æ§˜ã®æ¤œå‡º"""
        import psutil
        import torch
        
        # CPUä»•æ§˜
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq().max if psutil.cpu_freq() else 0
        
        # ãƒ¡ãƒ¢ãƒªä»•æ§˜
        memory_gb = psutil.virtual_memory().total / 1024 / 1024 / 1024
        
        # GPUä»•æ§˜
        gpu_available = torch.cuda.is_available()
        gpu_memory_gb = 0
        if gpu_available:
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
        
        # MPSä»•æ§˜ï¼ˆApple Siliconï¼‰
        mps_available = torch.backends.mps.is_available()
        
        return EnvironmentSpecs(
            cpu_count=cpu_count,
            cpu_freq_mhz=cpu_freq,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_memory_gb=gpu_memory_gb,
            mps_available=mps_available
        )
```

== ğŸ§ª ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

=== ğŸ”¬ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
# tests/test_performance_optimization.py
import pytest
import numpy as np
from core.ai_optimizer import AIOptimizer

class TestPerformanceOptimization:
    def test_frame_skipping_under_low_fps(self):
        """ä½FPSæ™‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        optimizer = AIOptimizer()
        
        # ä½FPSã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        low_fps = 5.0
        
        # è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã‚¹ã‚­ãƒƒãƒ—å‹•ä½œç¢ºèª
        skip_decisions = []
        for _ in range(10):
            should_process = optimizer.frame_skipper.should_process_frame(low_fps)
            skip_decisions.append(should_process)
        
        # ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ãƒˆãŒä¸Šæ˜‡ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert optimizer.frame_skipper.skip_rate > 1
        
        # å®Ÿéš›ã«ã‚¹ã‚­ãƒƒãƒ—ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        processed_count = sum(skip_decisions)
        assert processed_count < len(skip_decisions)
    
    def test_memory_cleanup_under_pressure(self):
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        from core.memory_manager import MemoryManager
        
        memory_manager = MemoryManager()
        
        # å¤§é‡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ãƒ¡ãƒ¢ãƒªåœ§è¿«ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        large_frame = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é™ç•Œã¾ã§è¿½åŠ 
        for i in range(memory_manager.cache_max_size + 5):
            memory_manager.cache_frame(f"frame_{i}", large_frame)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãŒåˆ¶é™å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert len(memory_manager.frame_cache) <= memory_manager.cache_max_size
    
    def test_ai_optimization_integration(self):
        """AIæœ€é©åŒ–ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
        optimizer = AIOptimizer()
        test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®åˆæœŸåŒ–
        initial_stats = optimizer.get_performance_stats()
        
        # æœ€é©åŒ–å‡¦ç†ã®å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
        mock_model = lambda frame, verbose=False: []
        
        results = []
        for _ in range(20):
            result = optimizer.optimize_yolo_inference(mock_model, test_frame)
            results.append(result)
            optimizer.update_performance_stats()
        
        # çµ±è¨ˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        final_stats = optimizer.get_performance_stats()
        assert final_stats['frame_count'] > initial_stats['frame_count']
```

== ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒªã‚½ãƒ¼ã‚¹

=== ğŸ“– å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

* **<<ai-ml-specifications>>**: AI/ML ã‚·ã‚¹ãƒ†ãƒ è©³ç´°
* **<<system-design>>**: ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆå…¨ä½“åƒ
* **<<configuration-guide>>**: è¨­å®šæœ€é©åŒ–ã‚¬ã‚¤ãƒ‰
* **<<troubleshooting-guide>>**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œå¯¾å¿œ

=== ğŸ”— å¤–éƒ¨ãƒªã‚½ãƒ¼ã‚¹

* **PyTorch Performance Tuning**: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
* **CUDA Optimization Guide**: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
* **Python Memory Profiling**: https://docs.python.org/3/library/tracemalloc.html
* **NumPy Performance**: https://numpy.org/doc/stable/user/basics.performance.html

=== ğŸ› ï¸ é–‹ç™ºæ”¯æ´ãƒ„ãƒ¼ãƒ«

```bash
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
python scripts/monitor_performance.py --duration 300

# ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
python -m memory_profiler backend/main.py

# AIæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
python scripts/test_optimization.py --profile low_end

# è¨­å®šæœ€é©åŒ–
python scripts/optimize_config.py --auto-detect
```

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: 2024-12-27 