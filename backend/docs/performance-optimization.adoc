= ⚡ パフォーマンス最適化システム - Performance Optimization System
:author: KanshiChan Development Team
:version: 1.0
:date: 2024-12-27
:target_audience: システムエンジニア、バックエンド開発者
:document_type: システム詳細仕様書
:toc: left
:toclevels: 4
:sectnums:
:source-highlighter: highlight.js

== 📋 概要

=== 📖 このドキュメントについて

本ドキュメントは、KanshiChanシステムのパフォーマンス最適化機能における詳細な技術仕様とアーキテクチャを定義します。

**対象読者**: システムエンジニア、バックエンド開発者 +
**前提知識**: Python最適化、AI/ML推論、メモリ管理 +
**関連ドキュメント**: <<ai-ml-specifications>>, <<system-design>>, <<configuration-guide>>

=== 🎯 システム目的

* **リアルタイム性能**: 15FPS以上の安定したフレーム処理
* **智慧的負荷調整**: 動的なパフォーマンス最適化
* **メモリ効率**: 最適化されたメモリ使用と管理
* **デバイス最適化**: CPU/GPU/MPS自動選択と最適化

=== 🏗️ 最適化アーキテクチャ

[mermaid]
....
graph TB
    subgraph "📊 パフォーマンス監視"
        PERF_MON[Performance Monitor]
        FPS_TRACKER[FPS Tracker]
        MEM_MONITOR[Memory Monitor]
        CPU_MONITOR[CPU Monitor]
    end
    
    subgraph "⚡ AI最適化エンジン"
        AI_OPT[AI Optimizer]
        FRAME_SKIP[Frame Skipper]
        BATCH_PROC[Batch Processor]
        DEVICE_MGR[Device Manager]
    end
    
    subgraph "💾 メモリ管理"
        MEM_MGR[Memory Manager]
        CACHE_MGR[Cache Manager]
        GC_OPT[GC Optimizer]
        LRU_CACHE[LRU Cache]
    end
    
    subgraph "🔧 動的最適化"
        DYN_ADJUSTER[Dynamic Adjuster]
        THRESHOLD_MGR[Threshold Manager]
        FEEDBACK_CTRL[Feedback Control]
        AUTO_TUNER[Auto Tuner]
    end
    
    subgraph "🎛️ 設定管理"
        CONFIG_MGR[Config Manager]
        PERF_PROFILE[Performance Profile]
        ENV_DETECTOR[Environment Detector]
        PRESET_MGR[Preset Manager]
    end
    
    %% データフロー
    PERF_MON --> FPS_TRACKER
    PERF_MON --> MEM_MONITOR
    PERF_MON --> CPU_MONITOR
    
    FPS_TRACKER --> DYN_ADJUSTER
    MEM_MONITOR --> MEM_MGR
    CPU_MONITOR --> AI_OPT
    
    AI_OPT --> FRAME_SKIP
    AI_OPT --> BATCH_PROC
    AI_OPT --> DEVICE_MGR
    
    MEM_MGR --> CACHE_MGR
    MEM_MGR --> GC_OPT
    MEM_MGR --> LRU_CACHE
    
    DYN_ADJUSTER --> THRESHOLD_MGR
    DYN_ADJUSTER --> FEEDBACK_CTRL
    DYN_ADJUSTER --> AUTO_TUNER
    
    CONFIG_MGR --> PERF_PROFILE
    CONFIG_MGR --> ENV_DETECTOR
    CONFIG_MGR --> PRESET_MGR
    
    FEEDBACK_CTRL --> AI_OPT
    AUTO_TUNER --> MEM_MGR
    ENV_DETECTOR --> DEVICE_MGR
    
    classDef monitoring fill:#e3f2fd
    classDef optimization fill:#e8f5e8
    classDef memory fill:#fff3e0
    classDef dynamic fill:#f3e5f5
    classDef config fill:#fce4ec
    
    class PERF_MON,FPS_TRACKER,MEM_MONITOR,CPU_MONITOR monitoring
    class AI_OPT,FRAME_SKIP,BATCH_PROC,DEVICE_MGR optimization
    class MEM_MGR,CACHE_MGR,GC_OPT,LRU_CACHE memory
    class DYN_ADJUSTER,THRESHOLD_MGR,FEEDBACK_CTRL,AUTO_TUNER dynamic
    class CONFIG_MGR,PERF_PROFILE,ENV_DETECTOR,PRESET_MGR config
....

== ⚡ AI推論最適化

=== 🧠 AI最適化エンジン

[mermaid]
....
classDiagram
    class AIOptimizer {
        -performance_monitor: PerformanceMonitor
        -frame_skipper: FrameSkipper
        -batch_processor: BatchProcessor
        -last_yolo_results: Any
        -last_yolo_results_age: int
        
        +optimize_yolo_inference(model, frame): Optional[Any]
        +optimize_mediapipe_pipeline(model, frame): Optional[Any]
        +update_performance_stats(): void
        +get_performance_stats(): Dict[str, Any]
        +log_performance_summary(): void
    }
    
    class PerformanceMonitor {
        -frame_times: deque
        -inference_times: deque
        -memory_usage: deque
        -last_frame_time: float
        
        +record_frame(): void
        +record_inference_time(time: float): void
        +get_current_fps(): float
        +get_avg_inference_time(): float
        +get_memory_usage(): float
        +get_stats(): Dict[str, float]
    }
    
    class FrameSkipper {
        -skip_rate: int
        -frame_counter: int
        -target_fps: float
        -min_fps: float
        -max_skip_rate: int
        
        +should_process_frame(current_fps: float): bool
        +_adjust_skip_rate(current_fps: float): void
    }
    
    class BatchProcessor {
        -batch_size: int
        -timeout_ms: int
        -frame_buffer: List
        -enabled: bool
        
        +add_frame(frame: ndarray): Optional[List[ndarray]]
    }
    
    AIOptimizer --> PerformanceMonitor
    AIOptimizer --> FrameSkipper
    AIOptimizer --> BatchProcessor
....

=== 🎯 YOLO最適化戦略

**結果キャッシュ付きYOLO推論**

```python
# backend/src/core/ai_optimizer.py
class AIOptimizer:
    def optimize_yolo_inference(self, model, frame: np.ndarray) -> Optional[Any]:
        """YOLO推論の最適化（描画継続性を考慮した改良版）"""
        try:
            inference_start = time.time()
            
            # キャッシュの年齢を更新
            self.last_yolo_results_age += 1
            
            # フレームスキップ判定
            current_fps = self.performance_monitor.get_current_fps()
            should_skip = not self.frame_skipper.should_process_frame(current_fps)
            
            if should_skip:
                # スキップ時も前回の検出結果を返すモードを追加
                if (self.last_yolo_results is not None and 
                    self.last_yolo_results_age <= self.max_cache_age):
                    # 前回結果を返して描画継続性を維持
                    logger.debug(f"Using cached YOLO results (age: {self.last_yolo_results_age} frames)")
                    return self.last_yolo_results
                else:
                    # キャッシュが古すぎる場合はNoneを返す
                    logger.debug(f"Cache too old or empty, returning None (age: {self.last_yolo_results_age})")
                return None
                
            # フレーム前処理の最適化
            optimized_frame = self._optimize_frame_preprocessing(frame)
            
            # YOLO推論実行
            with torch.no_grad():  # 勾配計算を無効化
                results = model(optimized_frame, verbose=False)
                
            # 成功した推論結果をキャッシュ
            self.last_yolo_results = results
            self.last_yolo_results_age = 0  # キャッシュをリフレッシュ
                
            inference_time = time.time() - inference_start
            self.performance_monitor.record_inference_time(inference_time)
            
            return results
            
        except Exception as e:
            model_error = wrap_exception(
                e, ModelError,
                "YOLO inference optimization failed",
                details={'fallback_to_standard': True}
            )
            logger.warning(f"YOLO optimization error: {model_error.to_dict()}")
            return None
```

=== 📊 動的フレームスキップ

[mermaid]
....
stateDiagram-v2
    [*] --> Monitoring
    Monitoring --> LowFPS : FPS < 5.0
    Monitoring --> HighFPS : FPS > 18.0
    Monitoring --> OptimalFPS : 5.0 ≤ FPS ≤ 18.0
    
    LowFPS --> IncreaseSkip : skip_rate += 1
    IncreaseSkip --> Monitoring : max 5x
    
    HighFPS --> DecreaseSkip : skip_rate -= 1
    DecreaseSkip --> Monitoring : min 1x
    
    OptimalFPS --> Monitoring : 維持
    
    note right of LowFPS
        スキップレート増加
        処理負荷軽減
    end note
    
    note left of HighFPS
        スキップレート減少
        品質向上
    end note
....

**フレームスキップ実装**

```python
# backend/src/core/ai_optimizer.py
class FrameSkipper:
    def should_process_frame(self, current_fps: float) -> bool:
        """フレームを処理すべきかどうかを判定"""
        self.frame_counter += 1
        
        # 動的スキップレート調整
        self._adjust_skip_rate(current_fps)
        
        # スキップレートに基づいて処理判定
        return (self.frame_counter % self.skip_rate) == 0
        
    def _adjust_skip_rate(self, current_fps: float) -> None:
        """現在のFPSに基づいてスキップレートを動的調整"""
        current_time = time.time()
        
        if current_time - self.last_adjustment < self.adjustment_interval:
            return
            
        self.last_adjustment = current_time
        
        if current_fps < self.min_fps:
            # FPSが低すぎる場合はスキップレートを上げる
            self.skip_rate = min(self.skip_rate + 1, self.max_skip_rate)
            logger.debug(f"Low FPS detected ({current_fps:.1f}), increasing skip rate to {self.skip_rate}")
        elif current_fps > self.target_fps * 1.2:
            # FPSが十分高い場合はスキップレートを下げる
            self.skip_rate = max(self.skip_rate - 1, 1)
            logger.debug(f"High FPS detected ({current_fps:.1f}), decreasing skip rate to {self.skip_rate}")
```

== 💾 メモリ管理システム

=== 🧹 メモリ管理アーキテクチャ

[mermaid]
....
classDiagram
    class MemoryManager {
        -frame_cache: Dict
        -frame_cache_stats: Dict
        -gc_threshold: float
        -max_memory_mb: float
        -monitor_interval: float
        -cache_max_size: int
        
        +cache_frame(key: str, frame: ndarray): bool
        +get_cached_frame(key: str): Optional[ndarray]
        +clear_frame_cache(): void
        +monitor_memory_usage(): float
        +cleanup_memory(): void
        +get_memory_stats(): Dict[str, Any]
    }
    
    class LRUCache {
        -max_size: int
        -cache: OrderedDict
        -access_order: deque
        
        +get(key: str): Optional[Any]
        +put(key: str, value: Any): void
        +evict_lru(): void
        +clear(): void
        +size(): int
    }
    
    class GCOptimizer {
        -gc_interval: float
        -memory_threshold: float
        -last_gc_time: float
        
        +should_trigger_gc(): bool
        +force_gc(): void
        +optimize_gc_settings(): void
        +get_gc_stats(): Dict[str, Any]
    }
    
    class CacheMonitor {
        -hit_rate: float
        -miss_rate: float
        -eviction_count: int
        
        +record_hit(): void
        +record_miss(): void
        +record_eviction(): void
        +get_cache_metrics(): Dict[str, float]
    }
    
    MemoryManager --> LRUCache
    MemoryManager --> GCOptimizer
    MemoryManager --> CacheMonitor
....

=== 🔄 LRUキャッシュ実装

```python
# backend/src/core/memory_manager.py
class MemoryManager:
    def cache_frame(self, key: str, frame: np.ndarray) -> bool:
        """フレームをキャッシュに保存"""
        try:
            if self.is_memory_usage_high():
                self.cleanup_memory()
                
            frame_size = frame.nbytes / 1024 / 1024  # MB
            
            if frame_size > self.max_frame_size_mb:
                logger.warning(f"Frame too large for cache: {frame_size:.1f}MB")
                return False
                
            # キャッシュサイズ制限チェック
            if len(self.frame_cache) >= self.cache_max_size:
                self._evict_oldest_frame()
                
            # フレームをキャッシュに保存
            self.frame_cache[key] = frame.copy()
            self.frame_cache_stats[key] = {
                'timestamp': time.time(),
                'size_mb': frame_size,
                'access_count': 0
            }
            
            logger.debug(f"Frame cached: {key} ({frame_size:.1f}MB)")
            return True
            
        except Exception as e:
            logger.error(f"Error caching frame {key}: {e}")
            return False
    
    def cleanup_memory(self) -> None:
        """メモリクリーンアップの実行"""
        try:
            # 1. フレームキャッシュのクリーンアップ
            old_cache_size = len(self.frame_cache)
            self._cleanup_old_cached_frames()
            
            # 2. 強制ガベージコレクション
            if self.is_memory_usage_critical():
                import gc
                collected = gc.collect()
                logger.info(f"Forced GC collected {collected} objects")
            
            # 3. 統計更新
            new_cache_size = len(self.frame_cache)
            logger.info(f"Memory cleanup: cache {old_cache_size} → {new_cache_size}")
            
        except Exception as e:
            logger.error(f"Error in memory cleanup: {e}")
```

=== 📊 メモリ監視システム

```python
# backend/src/core/memory_manager.py
class MemoryManager:
    def monitor_memory_usage(self) -> float:
        """システムメモリ使用量を監視"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            
            # メモリ使用量（MB）
            memory_mb = memory_info.rss / 1024 / 1024
            
            # 使用率計算
            system_memory = psutil.virtual_memory()
            usage_percent = (memory_mb / (system_memory.total / 1024 / 1024)) * 100
            
            # 閾値チェック
            if usage_percent > self.gc_threshold:
                logger.warning(f"High memory usage: {usage_percent:.1f}%")
                if usage_percent > 90.0:  # 緊急時
                    self.cleanup_memory()
            
            # 統計更新
            self.memory_usage_history.append({
                'timestamp': time.time(),
                'memory_mb': memory_mb,
                'usage_percent': usage_percent
            })
            
            # 履歴サイズ制限
            if len(self.memory_usage_history) > 100:
                self.memory_usage_history.pop(0)
            
            return memory_mb
            
        except Exception as e:
            logger.error(f"Error monitoring memory usage: {e}")
            return 0.0
```

== 🎛️ 動的パフォーマンス調整

=== 📈 フィードバック制御システム

[mermaid]
....
flowchart TD
    MEASURE[性能測定] --> ANALYZE[性能分析]
    ANALYZE --> DECISION{調整判定}
    
    DECISION -->|FPS低下| OPTIMIZE_FPS[FPS最適化]
    DECISION -->|メモリ不足| OPTIMIZE_MEM[メモリ最適化]
    DECISION -->|CPU過負荷| OPTIMIZE_CPU[CPU最適化]
    DECISION -->|正常| MAINTAIN[現状維持]
    
    OPTIMIZE_FPS --> INCREASE_SKIP[スキップレート増加]
    OPTIMIZE_FPS --> REDUCE_QUALITY[品質低下]
    OPTIMIZE_FPS --> BATCH_ENABLE[バッチ処理有効]
    
    OPTIMIZE_MEM --> CLEAR_CACHE[キャッシュクリア]
    OPTIMIZE_MEM --> FORCE_GC[強制GC]
    OPTIMIZE_MEM --> REDUCE_BUFFER[バッファ削減]
    
    OPTIMIZE_CPU --> ASYNC_PROCESSING[非同期処理]
    OPTIMIZE_CPU --> THREAD_LIMIT[スレッド制限]
    OPTIMIZE_CPU --> PRIORITY_ADJUST[優先度調整]
    
    INCREASE_SKIP --> VALIDATE[効果検証]
    REDUCE_QUALITY --> VALIDATE
    BATCH_ENABLE --> VALIDATE
    CLEAR_CACHE --> VALIDATE
    FORCE_GC --> VALIDATE
    REDUCE_BUFFER --> VALIDATE
    ASYNC_PROCESSING --> VALIDATE
    THREAD_LIMIT --> VALIDATE
    PRIORITY_ADJUST --> VALIDATE
    MAINTAIN --> VALIDATE
    
    VALIDATE --> WAIT[待機期間]
    WAIT --> MEASURE
    
    classDef measure fill:#e3f2fd
    classDef decision fill:#fff3e0
    classDef optimize fill:#e8f5e8
    classDef action fill:#f3e5f5
    classDef validate fill:#fce4ec
    
    class MEASURE,ANALYZE measure
    class DECISION decision
    class OPTIMIZE_FPS,OPTIMIZE_MEM,OPTIMIZE_CPU optimize
    class INCREASE_SKIP,REDUCE_QUALITY,BATCH_ENABLE,CLEAR_CACHE,FORCE_GC,REDUCE_BUFFER,ASYNC_PROCESSING,THREAD_LIMIT,PRIORITY_ADJUST,MAINTAIN action
    class VALIDATE,WAIT validate
....

=== ⚙️ 自動チューニング実装

```python
# backend/src/core/performance_tuner.py
class PerformanceTuner:
    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.optimization_history = deque(maxlen=50)
        self.tuning_metrics = TuningMetrics()
        
    async def auto_tune_performance(self) -> TuningResult:
        """自動パフォーマンスチューニング"""
        try:
            # 現在の性能測定
            current_metrics = await self._collect_performance_metrics()
            
            # 最適化の必要性を判定
            optimization_needs = self._analyze_optimization_needs(current_metrics)
            
            if not optimization_needs:
                return TuningResult(status='optimal', changes=[])
            
            # 最適化戦略の決定
            strategies = self._determine_optimization_strategies(optimization_needs)
            
            # 最適化の実行
            applied_changes = []
            for strategy in strategies:
                change_result = await self._apply_optimization_strategy(strategy)
                applied_changes.append(change_result)
                
                # 効果検証
                if await self._validate_optimization_effect(strategy, change_result):
                    logger.info(f"Optimization strategy '{strategy.name}' successful")
                else:
                    # 効果が無い場合はロールバック
                    await self._rollback_optimization(change_result)
                    logger.warning(f"Optimization strategy '{strategy.name}' ineffective, rolled back")
            
            # チューニング履歴の更新
            self._update_tuning_history(current_metrics, applied_changes)
            
            return TuningResult(
                status='optimized',
                changes=applied_changes,
                metrics=current_metrics
            )
            
        except Exception as e:
            logger.error(f"Error in auto-tuning performance: {e}")
            return TuningResult(status='error', error=str(e))
```

== 🖥️ デバイス最適化

=== 🔧 デバイス管理システム

[mermaid]
....
classDiagram
    class DeviceManager {
        -available_devices: List[str]
        -current_device: str
        -device_capabilities: Dict
        -optimization_profiles: Dict
        
        +detect_optimal_device(): str
        +setup_device_optimization(device: str): void
        +get_device_performance(): DeviceMetrics
        +switch_device(device: str): bool
    }
    
    class CUDAOptimizer {
        -gpu_memory_limit: float
        -mixed_precision: bool
        -tensorrt_enabled: bool
        
        +optimize_gpu_memory(): void
        +enable_mixed_precision(): void
        +setup_tensorrt(): void
        +get_gpu_stats(): Dict
    }
    
    class MPSOptimizer {
        -mps_memory_fraction: float
        -graph_optimization: bool
        
        +optimize_mps_settings(): void
        +enable_graph_optimization(): void
        +get_mps_performance(): Dict
    }
    
    class CPUOptimizer {
        -thread_count: int
        -simd_optimization: bool
        -numa_affinity: bool
        
        +optimize_thread_count(): void
        +enable_simd(): void
        +setup_numa_affinity(): void
        +get_cpu_stats(): Dict
    }
    
    DeviceManager --> CUDAOptimizer
    DeviceManager --> MPSOptimizer
    DeviceManager --> CPUOptimizer
....

=== 🚀 GPU最適化戦略

```python
# backend/src/core/device_optimizer.py
class DeviceOptimizer:
    def optimize_for_gpu(self, device_type: str) -> OptimizationResult:
        """GPU向け最適化設定"""
        try:
            optimization_config = {}
            
            if device_type == 'cuda':
                # CUDA最適化
                optimization_config.update({
                    'memory_limit': 0.8,  # GPU メモリの80%使用
                    'mixed_precision': True,  # 半精度計算
                    'tensorrt': True,  # TensorRT最適化
                    'batch_size': 8,  # バッチサイズ増加
                    'async_execution': True  # 非同期実行
                })
                
                # GPU メモリ最適化
                torch.cuda.empty_cache()
                torch.cuda.set_per_process_memory_fraction(0.8)
                
            elif device_type == 'mps':
                # Apple MPS最適化
                optimization_config.update({
                    'memory_fraction': 0.7,  # MPS メモリの70%使用
                    'graph_optimization': True,  # 計算グラフ最適化
                    'batch_size': 4,  # 適度なバッチサイズ
                    'precision': 'fp16'  # 半精度
                })
                
            # 最適化設定を適用
            self._apply_optimization_config(optimization_config)
            
            return OptimizationResult(
                success=True,
                device=device_type,
                config=optimization_config
            )
            
        except Exception as e:
            logger.error(f"GPU optimization failed for {device_type}: {e}")
            return OptimizationResult(success=False, error=str(e))
```

== 📊 パフォーマンス監視

=== 📈 リアルタイム監視システム

[mermaid]
....
graph TB
    subgraph "📊 監視メトリクス"
        FPS[FPS Monitor<br/>15+ target]
        LATENCY[Inference Latency<br/><50ms target]
        MEMORY[Memory Usage<br/><2GB target]
        CPU[CPU Utilization<br/><80% target]
    end
    
    subgraph "🎛️ 制御システム"
        CONTROLLER[Performance Controller]
        ADJUSTER[Dynamic Adjuster]
        ALERTER[Alert Manager]
        LOGGER[Performance Logger]
    end
    
    subgraph "⚡ 最適化アクション"
        SKIP_ADJUST[Skip Rate Adjust]
        MEMORY_CLEAN[Memory Cleanup]
        DEVICE_SWITCH[Device Switch]
        BATCH_TOGGLE[Batch Toggle]
    end
    
    FPS --> CONTROLLER
    LATENCY --> CONTROLLER
    MEMORY --> CONTROLLER
    CPU --> CONTROLLER
    
    CONTROLLER --> ADJUSTER
    CONTROLLER --> ALERTER
    CONTROLLER --> LOGGER
    
    ADJUSTER --> SKIP_ADJUST
    ADJUSTER --> MEMORY_CLEAN
    ADJUSTER --> DEVICE_SWITCH
    ADJUSTER --> BATCH_TOGGLE
    
    classDef monitor fill:#e3f2fd
    classDef control fill:#e8f5e8
    classDef action fill:#fff3e0
    
    class FPS,LATENCY,MEMORY,CPU monitor
    class CONTROLLER,ADJUSTER,ALERTER,LOGGER control
    class SKIP_ADJUST,MEMORY_CLEAN,DEVICE_SWITCH,BATCH_TOGGLE action
....

=== 📊 性能指標ダッシュボード

```python
# backend/src/core/performance_dashboard.py
class PerformanceDashboard:
    def get_performance_summary(self) -> Dict[str, Any]:
        """パフォーマンス要約の取得"""
        try:
            current_stats = self.performance_monitor.get_stats()
            memory_stats = self.memory_manager.get_memory_stats()
            optimization_stats = self.ai_optimizer.get_performance_stats()
            
            # 性能評価
            performance_score = self._calculate_performance_score(current_stats)
            health_status = self._determine_health_status(current_stats)
            
            summary = {
                'timestamp': datetime.utcnow().isoformat(),
                'performance_score': performance_score,
                'health_status': health_status,
                'metrics': {
                    'fps': {
                        'current': current_stats['fps'],
                        'target': 15.0,
                        'status': 'good' if current_stats['fps'] >= 15.0 else 'warning'
                    },
                    'latency': {
                        'current_ms': current_stats['avg_inference_ms'],
                        'target_ms': 50.0,
                        'status': 'good' if current_stats['avg_inference_ms'] <= 50.0 else 'warning'
                    },
                    'memory': {
                        'current_mb': memory_stats['current_usage_mb'],
                        'target_mb': 2048.0,
                        'status': 'good' if memory_stats['current_usage_mb'] <= 2048.0 else 'warning'
                    }
                },
                'optimizations': {
                    'skip_rate': optimization_stats['skip_rate'],
                    'cache_hit_rate': optimization_stats.get('cache_hit_rate', 0.0),
                    'batch_enabled': optimization_stats['batch_enabled'],
                    'device': optimization_stats.get('device', 'cpu')
                },
                'recommendations': self._generate_performance_recommendations(current_stats)
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating performance summary: {e}")
            return {'error': str(e)}
```

== 🔧 設定最適化

=== ⚙️ パフォーマンスプロファイル

```yaml
# 低性能環境向け設定
performance_profiles:
  low_end:
    ai_optimization:
      target_fps: 8.0
      frame_optimization:
        auto_resize: true
        max_width: 320
        max_height: 240
        quality_vs_speed: 0.2
    yolo:
      device:
        preferred: "cpu"
      optimization:
        half_precision: false
    mediapipe:
      pose:
        model_complexity: 0
      hands:
        enabled: false
      face:
        enabled: false
    monitoring:
      interval_seconds: 5

  # 高性能環境向け設定
  high_end:
    ai_optimization:
      target_fps: 30.0
      frame_optimization:
        max_width: 1280
        max_height: 720
        quality_vs_speed: 0.8
    yolo:
      device:
        preferred: "cuda"
      optimization:
        half_precision: true
        tensorrt: true
    mediapipe:
      pose:
        model_complexity: 2
      hands:
        enabled: true
      face:
        enabled: true
    ai_optimization:
      gpu:
        memory_limit: 0.9
      memory:
        cache_size_mb: 1024
```

=== 🎛️ 動的設定調整

```python
# backend/src/core/config_optimizer.py
class ConfigOptimizer:
    def optimize_config_for_environment(self) -> ConfigOptimizationResult:
        """環境に応じた設定最適化"""
        try:
            # 環境検出
            env_specs = self._detect_environment_specifications()
            
            # 最適化プロファイルの選択
            optimal_profile = self._select_optimal_profile(env_specs)
            
            # 設定の動的調整
            optimized_config = self._adjust_config_dynamically(optimal_profile, env_specs)
            
            # 設定の適用
            self.config_manager.update_config(optimized_config)
            
            logger.info(f"Config optimized for environment: {optimal_profile}")
            
            return ConfigOptimizationResult(
                success=True,
                profile=optimal_profile,
                config=optimized_config,
                environment=env_specs
            )
            
        except Exception as e:
            logger.error(f"Config optimization failed: {e}")
            return ConfigOptimizationResult(success=False, error=str(e))
    
    def _detect_environment_specifications(self) -> EnvironmentSpecs:
        """環境仕様の検出"""
        import psutil
        import torch
        
        # CPU仕様
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq().max if psutil.cpu_freq() else 0
        
        # メモリ仕様
        memory_gb = psutil.virtual_memory().total / 1024 / 1024 / 1024
        
        # GPU仕様
        gpu_available = torch.cuda.is_available()
        gpu_memory_gb = 0
        if gpu_available:
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
        
        # MPS仕様（Apple Silicon）
        mps_available = torch.backends.mps.is_available()
        
        return EnvironmentSpecs(
            cpu_count=cpu_count,
            cpu_freq_mhz=cpu_freq,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_memory_gb=gpu_memory_gb,
            mps_available=mps_available
        )
```

== 🧪 テスト戦略

=== 🔬 パフォーマンステスト

```python
# tests/test_performance_optimization.py
import pytest
import numpy as np
from core.ai_optimizer import AIOptimizer

class TestPerformanceOptimization:
    def test_frame_skipping_under_low_fps(self):
        """低FPS時のフレームスキップテスト"""
        optimizer = AIOptimizer()
        
        # 低FPSシミュレーション
        low_fps = 5.0
        
        # 複数フレームでスキップ動作確認
        skip_decisions = []
        for _ in range(10):
            should_process = optimizer.frame_skipper.should_process_frame(low_fps)
            skip_decisions.append(should_process)
        
        # スキップレートが上昇していることを確認
        assert optimizer.frame_skipper.skip_rate > 1
        
        # 実際にスキップが発生していることを確認
        processed_count = sum(skip_decisions)
        assert processed_count < len(skip_decisions)
    
    def test_memory_cleanup_under_pressure(self):
        """メモリ圧迫時のクリーンアップテスト"""
        from core.memory_manager import MemoryManager
        
        memory_manager = MemoryManager()
        
        # 大量フレームをキャッシュしてメモリ圧迫をシミュレート
        large_frame = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)
        
        # キャッシュ限界まで追加
        for i in range(memory_manager.cache_max_size + 5):
            memory_manager.cache_frame(f"frame_{i}", large_frame)
        
        # キャッシュサイズが制限内であることを確認
        assert len(memory_manager.frame_cache) <= memory_manager.cache_max_size
    
    def test_ai_optimization_integration(self):
        """AI最適化の統合テスト"""
        optimizer = AIOptimizer()
        test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # パフォーマンス統計の初期化
        initial_stats = optimizer.get_performance_stats()
        
        # 最適化処理の実行（モックモデル使用）
        mock_model = lambda frame, verbose=False: []
        
        results = []
        for _ in range(20):
            result = optimizer.optimize_yolo_inference(mock_model, test_frame)
            results.append(result)
            optimizer.update_performance_stats()
        
        # 統計が更新されていることを確認
        final_stats = optimizer.get_performance_stats()
        assert final_stats['frame_count'] > initial_stats['frame_count']
```

== 📚 関連ドキュメント・リソース

=== 📖 参照ドキュメント

* **<<ai-ml-specifications>>**: AI/ML システム詳細
* **<<system-design>>**: システム設計全体像
* **<<configuration-guide>>**: 設定最適化ガイド
* **<<troubleshooting-guide>>**: パフォーマンス問題対応

=== 🔗 外部リソース

* **PyTorch Performance Tuning**: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
* **CUDA Optimization Guide**: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
* **Python Memory Profiling**: https://docs.python.org/3/library/tracemalloc.html
* **NumPy Performance**: https://numpy.org/doc/stable/user/basics.performance.html

=== 🛠️ 開発支援ツール

```bash
# パフォーマンス監視
python scripts/monitor_performance.py --duration 300

# メモリプロファイリング
python -m memory_profiler backend/main.py

# AI最適化テスト
python scripts/test_optimization.py --profile low_end

# 設定最適化
python scripts/optimize_config.py --auto-detect
```

---

**📞 Contact**: team@kanshichan.dev +
**🔗 Repository**: https://github.com/kanshichan/backend +
**📅 Last Updated**: 2024-12-27 