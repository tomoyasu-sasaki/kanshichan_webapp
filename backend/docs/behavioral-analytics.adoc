= ğŸ§  ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) è¡Œå‹•åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 3
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:author: KanshiChan Development Team
:email: team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

[NOTE]
====
ğŸ“‹ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±**

* **ä½œæˆè€…**: KanshiChan Development Team
* **æœ€çµ‚æ›´æ–°æ—¥**: {docdate}
* **å¯¾è±¡èª­è€…**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã€AI/MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ
* **å‰æçŸ¥è­˜**: æ©Ÿæ¢°å­¦ç¿’ã€æ™‚ç³»åˆ—åˆ†æã€è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
* **é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: <<ai-ml-specifications.adoc>>, <<detection-system.adoc>>, <<behavior-analysis.adoc>>
====

== ğŸ“– æ¦‚è¦

ç›£è¦–ã¡ã‚ƒã‚“ï¼ˆKanshiChanï¼‰ã®è¡Œå‹•åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é›†ä¸­çŠ¶æ…‹ã€ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨è¡Œå‹•ã‚’å­¦ç¿’ãƒ»åˆ†æã—ã€
å€‹äººæœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨äºˆæ¸¬ã‚’æä¾›ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

=== ğŸ¯ åˆ†æç›®æ¨™

* **é›†ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’**: å€‹äººã®æœ€é©ãªä½œæ¥­ãƒªã‚ºãƒ ç™ºè¦‹
* **äºˆæ¸¬åˆ†æ**: é›†ä¸­åº¦ä½ä¸‹ãƒ»ä¸­æ–­ã®äº‹å‰äºˆæ¸¬
* **è¡Œå‹•æœ€é©åŒ–**: ãƒ‡ãƒ¼ã‚¿é§†å‹•ã«ã‚ˆã‚‹ç¿’æ…£æ”¹å–„ææ¡ˆ
* **ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³**: å€‹äººç‰¹æ€§ã«é©å¿œã—ãŸæ”¯æ´

== ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

=== ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ§‹æˆ

[mermaid]
....
graph TB
    subgraph "ãƒ‡ãƒ¼ã‚¿åé›†å±¤"
        DETECT[ç‰©ä½“æ¤œå‡º<br/>YOLO + MediaPipe]
        SENSOR[ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿<br/>ã‚«ãƒ¡ãƒ©ãƒ»éŸ³å£°]
        USER[ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›<br/>è¨­å®šãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯]
    end
    
    subgraph "å‰å‡¦ç†å±¤"
        CLEAN[ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°]
        FEATURE[ç‰¹å¾´é‡æŠ½å‡º]
        NORMAL[æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–]
    end
    
    subgraph "åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"
        PATTERN[ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ]
        PREDICT[äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«]
        CLUSTER[ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°]
        ANOMALY[ç•°å¸¸æ¤œçŸ¥]
    end
    
    subgraph "å­¦ç¿’å±¤"
        ONLINE[ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’]
        BATCH[ãƒãƒƒãƒå­¦ç¿’]
        TRANSFER[è»¢ç§»å­¦ç¿’]
    end
    
    subgraph "å‡ºåŠ›å±¤"
        INSIGHT[ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ]
        RECOMMEND[ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰]
        ALERT[ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ]
        VISUAL[å¯è¦–åŒ–]
    end
    
    subgraph "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯"
        FEEDBACK[ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯]
        ADAPT[é©å¿œå­¦ç¿’]
    end
    
    DETECT --> CLEAN
    SENSOR --> CLEAN
    USER --> CLEAN
    
    CLEAN --> FEATURE
    FEATURE --> NORMAL
    
    NORMAL --> PATTERN
    NORMAL --> PREDICT
    NORMAL --> CLUSTER
    NORMAL --> ANOMALY
    
    PATTERN --> ONLINE
    PREDICT --> BATCH
    CLUSTER --> TRANSFER
    
    ONLINE --> INSIGHT
    BATCH --> RECOMMEND
    TRANSFER --> ALERT
    ANOMALY --> VISUAL
    
    INSIGHT --> FEEDBACK
    RECOMMEND --> FEEDBACK
    ALERT --> FEEDBACK
    VISUAL --> FEEDBACK
    
    FEEDBACK --> ADAPT
    ADAPT --> ONLINE
....

=== ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**
```python
# src/services/analysis/realtime_pipeline.py
import asyncio
import numpy as np
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import pandas as pd

@dataclass
class BehaviorEvent:
    """è¡Œå‹•ã‚¤ãƒ™ãƒ³ãƒˆ"""
    timestamp: datetime
    event_type: str  # 'presence', 'smartphone_use', 'posture_change'
    confidence: float
    duration: Optional[float] = None
    metadata: Optional[Dict] = None

@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    timestamp: datetime
    focus_score: float  # 0-100
    stress_level: float  # 0-100
    productivity_index: float  # 0-100
    predicted_break_time: Optional[datetime]
    recommendations: List[str]

class RealtimeAnalyticsPipeline:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡Œå‹•åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.event_buffer: List[BehaviorEvent] = []
        self.buffer_size = 1000
        self.analysis_window = timedelta(minutes=10)
        
        # åˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.focus_analyzer = FocusAnalyzer()
        self.pattern_detector = PatternDetector()
        self.predictor = BehaviorPredictor()
        
    async def process_detection_event(self, detection_data: Dict) -> Optional[AnalysisResult]:
        """æ¤œå‡ºã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        # æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡Œå‹•ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ
        event = self._create_behavior_event(detection_data)
        
        # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        self._add_to_buffer(event)
        
        # åˆ†æå®Ÿè¡Œï¼ˆä¸€å®šé–“éš”ï¼‰
        if self._should_analyze():
            return await self._analyze_current_window()
            
        return None
        
    def _create_behavior_event(self, detection_data: Dict) -> BehaviorEvent:
        """æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡Œå‹•ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now()
        
        # äººç‰©æ¤œå‡ºçŠ¶æ…‹
        person_detected = detection_data.get('person_detected', False)
        smartphone_detected = detection_data.get('smartphone_detected', False)
        
        if not person_detected:
            return BehaviorEvent(
                timestamp=timestamp,
                event_type='absence',
                confidence=detection_data.get('confidence', 0.0)
            )
        elif smartphone_detected:
            return BehaviorEvent(
                timestamp=timestamp,
                event_type='smartphone_use',
                confidence=detection_data.get('smartphone_confidence', 0.0),
                metadata={'device_angle': detection_data.get('device_angle')}
            )
        else:
            return BehaviorEvent(
                timestamp=timestamp,
                event_type='focused_work',
                confidence=detection_data.get('confidence', 0.0),
                metadata={'posture': detection_data.get('posture_info')}
            )
            
    async def _analyze_current_window(self) -> AnalysisResult:
        """ç¾åœ¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
        window_events = self._get_analysis_window()
        
        # å„ç¨®åˆ†æã®ä¸¦è¡Œå®Ÿè¡Œ
        focus_task = asyncio.create_task(self.focus_analyzer.analyze(window_events))
        pattern_task = asyncio.create_task(self.pattern_detector.detect_patterns(window_events))
        prediction_task = asyncio.create_task(self.predictor.predict_next_behavior(window_events))
        
        focus_score = await focus_task
        patterns = await pattern_task
        predictions = await prediction_task
        
        return AnalysisResult(
            timestamp=datetime.now(),
            focus_score=focus_score,
            stress_level=self._calculate_stress_level(window_events, patterns),
            productivity_index=self._calculate_productivity(window_events),
            predicted_break_time=predictions.get('next_break'),
            recommendations=self._generate_recommendations(focus_score, patterns)
        )
```

== ğŸ” è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ

=== ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ**
```python
# src/services/analysis/pattern_detector.py
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

class PatternDetector:
    """è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.cluster_model = DBSCAN(eps=0.3, min_samples=5)
        self.pattern_history = []
        
    async def detect_patterns(self, events: List[BehaviorEvent]) -> Dict[str, Any]:
        """è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        if len(events) < 10:
            return {'patterns': [], 'confidence': 0.0}
            
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_temporal_features(events)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clusters = await self._cluster_behaviors(features)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è§£é‡ˆ
        interpreted_patterns = self._interpret_clusters(clusters, events)
        
        return {
            'patterns': interpreted_patterns,
            'confidence': self._calculate_pattern_confidence(interpreted_patterns),
            'dominant_pattern': self._find_dominant_pattern(interpreted_patterns)
        }
        
    def _extract_temporal_features(self, events: List[BehaviorEvent]) -> np.ndarray:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡æŠ½å‡º"""
        df = pd.DataFrame([{
            'timestamp': event.timestamp,
            'event_type': event.event_type,
            'confidence': event.confidence,
            'hour': event.timestamp.hour,
            'minute': event.timestamp.minute,
            'weekday': event.timestamp.weekday()
        } for event in events])
        
        # æ™‚é–“ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡
        features = []
        
        for _, event in df.iterrows():
            # åŸºæœ¬ç‰¹å¾´é‡
            base_features = [
                event['hour'] / 24.0,  # æ™‚åˆ»æ­£è¦åŒ–
                event['minute'] / 60.0,  # åˆ†æ­£è¦åŒ–
                event['weekday'] / 7.0,  # æ›œæ—¥æ­£è¦åŒ–
                event['confidence']
            ]
            
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—One-Hot
            event_one_hot = [
                1.0 if event['event_type'] == 'focused_work' else 0.0,
                1.0 if event['event_type'] == 'smartphone_use' else 0.0,
                1.0 if event['event_type'] == 'absence' else 0.0
            ]
            
            features.append(base_features + event_one_hot)
            
        return np.array(features)
        
    async def _cluster_behaviors(self, features: np.ndarray) -> np.ndarray:
        """è¡Œå‹•ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        # æ­£è¦åŒ–
        features_scaled = self.scaler.fit_transform(features)
        
        # DBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_labels = self.cluster_model.fit_predict(features_scaled)
        
        return cluster_labels
        
    def _interpret_clusters(self, clusters: np.ndarray, events: List[BehaviorEvent]) -> List[Dict]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è§£é‡ˆ"""
        unique_clusters = np.unique(clusters)
        interpreted_patterns = []
        
        for cluster_id in unique_clusters:
            if cluster_id == -1:  # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
                continue
                
            cluster_events = [events[i] for i, c in enumerate(clusters) if c == cluster_id]
            
            pattern = self._analyze_cluster_pattern(cluster_events)
            interpreted_patterns.append({
                'cluster_id': int(cluster_id),
                'pattern_type': pattern['type'],
                'description': pattern['description'],
                'frequency': len(cluster_events),
                'confidence': pattern['confidence'],
                'time_range': pattern['time_range'],
                'characteristics': pattern['characteristics']
            })
            
        return interpreted_patterns
        
    def _analyze_cluster_pattern(self, cluster_events: List[BehaviorEvent]) -> Dict:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if not cluster_events:
            return {'type': 'unknown', 'description': 'No events', 'confidence': 0.0}
            
        # æ™‚é–“åˆ†æ
        hours = [event.timestamp.hour for event in cluster_events]
        hour_std = np.std(hours)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ†æ
        event_types = [event.event_type for event in cluster_events]
        dominant_type = max(set(event_types), key=event_types.count)
        type_ratio = event_types.count(dominant_type) / len(event_types)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ¤å®š
        if hour_std < 2 and type_ratio > 0.8:
            pattern_type = 'routine'
            description = f"Regular {dominant_type} around {np.mean(hours):.1f}:00"
        elif dominant_type == 'smartphone_use' and type_ratio > 0.6:
            pattern_type = 'distraction_pattern'
            description = "Frequent smartphone usage pattern"
        elif dominant_type == 'focused_work' and type_ratio > 0.7:
            pattern_type = 'focus_session'
            description = "Extended focus work session"
        else:
            pattern_type = 'mixed'
            description = "Mixed behavior pattern"
            
        return {
            'type': pattern_type,
            'description': description,
            'confidence': type_ratio,
            'time_range': (min(hours), max(hours)),
            'characteristics': {
                'dominant_event': dominant_type,
                'time_consistency': 1.0 - (hour_std / 12.0),
                'event_consistency': type_ratio
            }
        }
```

=== é›†ä¸­åº¦åˆ†æ

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†ä¸­åº¦è¨ˆç®—**
```python
# src/services/analysis/focus_analyzer.py
import numpy as np
from typing import List, Dict, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class FocusMetrics:
    """é›†ä¸­åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    raw_score: float
    smoothed_score: float
    trend: str  # 'improving', 'declining', 'stable'
    confidence: float

class FocusAnalyzer:
    """é›†ä¸­åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.focus_history: List[float] = []
        self.smoothing_window = 10
        self.trend_threshold = 5.0
        
    async def analyze(self, events: List[BehaviorEvent]) -> float:
        """é›†ä¸­åº¦åˆ†æãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        if not events:
            return 0.0
            
        # åŸºæœ¬é›†ä¸­åº¦è¨ˆç®—
        raw_focus = self._calculate_raw_focus(events)
        
        # å±¥æ­´ãƒ™ãƒ¼ã‚¹å¹³æ»‘åŒ–
        smoothed_focus = self._apply_smoothing(raw_focus)
        
        # é›†ä¸­åº¦å±¥æ­´æ›´æ–°
        self.focus_history.append(smoothed_focus)
        if len(self.focus_history) > 100:
            self.focus_history.pop(0)
            
        return smoothed_focus
        
    def _calculate_raw_focus(self, events: List[BehaviorEvent]) -> float:
        """ç”Ÿã®é›†ä¸­åº¦è¨ˆç®—"""
        if not events:
            return 0.0
            
        # ã‚¤ãƒ™ãƒ³ãƒˆé‡ã¿å®šç¾©
        event_weights = {
            'focused_work': 1.0,
            'smartphone_use': -0.8,
            'absence': -0.3,
            'posture_change': -0.1
        }
        
        total_weight = 0.0
        total_time = 0.0
        
        # æ™‚é–“é‡ã¿ä»˜ã‘é›†ä¸­åº¦è¨ˆç®—
        for i, event in enumerate(events):
            weight = event_weights.get(event.event_type, 0.0)
            confidence = event.confidence
            
            # æ™‚é–“ã«ã‚ˆã‚‹æ¸›è¡°ï¼ˆæœ€æ–°ã»ã©é‡è¦ï¼‰
            time_decay = self._calculate_time_decay(event.timestamp)
            
            # ç¶™ç¶šæ€§ãƒœãƒ¼ãƒŠã‚¹
            continuity_bonus = self._calculate_continuity_bonus(events, i)
            
            adjusted_weight = weight * confidence * time_decay * continuity_bonus
            total_weight += adjusted_weight
            total_time += time_decay
            
        if total_time == 0:
            return 0.0
            
        # æ­£è¦åŒ– (0-100ã‚¹ã‚±ãƒ¼ãƒ«)
        raw_score = (total_weight / total_time + 1.0) * 50.0
        return max(0.0, min(100.0, raw_score))
        
    def _calculate_time_decay(self, timestamp: datetime) -> float:
        """æ™‚é–“æ¸›è¡°ä¿‚æ•°è¨ˆç®—"""
        now = datetime.now()
        age_minutes = (now - timestamp).total_seconds() / 60.0
        
        # æŒ‡æ•°æ¸›è¡°ï¼ˆåŠæ¸›æœŸ: 30åˆ†ï¼‰
        decay = np.exp(-age_minutes / 30.0)
        return decay
        
    def _calculate_continuity_bonus(self, events: List[BehaviorEvent], index: int) -> float:
        """ç¶™ç¶šæ€§ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—"""
        if index == 0:
            return 1.0
            
        current_event = events[index]
        previous_event = events[index - 1]
        
        # åŒç¨®ã‚¤ãƒ™ãƒ³ãƒˆã®ç¶™ç¶š
        if current_event.event_type == previous_event.event_type:
            # é›†ä¸­ä½œæ¥­ã®ç¶™ç¶šã«ã¯å¤§ããªãƒœãƒ¼ãƒŠã‚¹
            if current_event.event_type == 'focused_work':
                return 1.3
            # ãã®ä»–ã®ç¶™ç¶šã«ã¯å°ã•ãªãƒšãƒŠãƒ«ãƒ†ã‚£
            else:
                return 0.9
                
        return 1.0
        
    def _apply_smoothing(self, raw_focus: float) -> float:
        """å±¥æ­´ãƒ™ãƒ¼ã‚¹å¹³æ»‘åŒ–"""
        if len(self.focus_history) < self.smoothing_window:
            # å±¥æ­´ä¸è¶³æ™‚ã¯é‡ã¿ä»˜ã‘å¹³å‡
            weights = np.linspace(0.1, 1.0, len(self.focus_history) + 1)
            values = self.focus_history + [raw_focus]
            return np.average(values, weights=weights)
        else:
            # ç§»å‹•å¹³å‡
            recent_history = self.focus_history[-self.smoothing_window:]
            weights = np.linspace(0.5, 1.0, self.smoothing_window + 1)
            values = recent_history + [raw_focus]
            return np.average(values, weights=weights)
            
    def get_focus_metrics(self) -> FocusMetrics:
        """è©³ç´°é›†ä¸­åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        if len(self.focus_history) < 3:
            return FocusMetrics(0.0, 0.0, 'stable', 0.0)
            
        current_score = self.focus_history[-1]
        raw_score = current_score  # ç°¡ç•¥åŒ–
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trend = self._analyze_trend()
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_confidence()
        
        return FocusMetrics(
            raw_score=raw_score,
            smoothed_score=current_score,
            trend=trend,
            confidence=confidence
        )
        
    def _analyze_trend(self) -> str:
        """é›†ä¸­åº¦ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        if len(self.focus_history) < 5:
            return 'stable'
            
        recent_values = self.focus_history[-5:]
        
        # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å‚¾ãè¨ˆç®—
        x = np.arange(len(recent_values))
        slope = np.polyfit(x, recent_values, 1)[0]
        
        if slope > self.trend_threshold:
            return 'improving'
        elif slope < -self.trend_threshold:
            return 'declining'
        else:
            return 'stable'
```

== ğŸ”® äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«

=== è¡Œå‹•äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³

**LSTM ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**
```python
# src/services/analysis/behavior_predictor.py
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

class BehaviorLSTM(nn.Module):
    """è¡Œå‹•äºˆæ¸¬LSTM ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 50, num_layers: int = 2, output_size: int = 3):
        super(BehaviorLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM ãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        
        # å…¨çµåˆå±¤
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_size),
            nn.Softmax(dim=1)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """é †ä¼æ’­"""
        batch_size = x.size(0)
        
        # éš ã‚ŒçŠ¶æ…‹åˆæœŸåŒ–
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        
        # LSTM é€šé
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # æœ€å¾Œã®å‡ºåŠ›ã®ã¿ä½¿ç”¨
        output = self.fc(lstm_out[:, -1, :])
        
        return output

class BehaviorPredictor:
    """è¡Œå‹•äºˆæ¸¬å™¨"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = BehaviorLSTM().to(self.device)
        self.scaler = MinMaxScaler()
        self.sequence_length = 20
        self.prediction_classes = ['focused_work', 'smartphone_use', 'absence']
        
        if model_path:
            self.load_model(model_path)
            
    async def predict_next_behavior(self, events: List[BehaviorEvent]) -> Dict[str, Any]:
        """æ¬¡ã®è¡Œå‹•äºˆæ¸¬"""
        if len(events) < self.sequence_length:
            return self._default_prediction()
            
        # ç‰¹å¾´é‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        feature_sequence = self._create_feature_sequence(events)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = await self._predict_probabilities(feature_sequence)
        
        # çµæœè§£é‡ˆ
        return self._interpret_predictions(predictions, events[-1].timestamp)
        
    def _create_feature_sequence(self, events: List[BehaviorEvent]) -> np.ndarray:
        """äºˆæ¸¬ç”¨ç‰¹å¾´é‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ"""
        # æœ€æ–°ã®sequence_lengthå€‹ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä½¿ç”¨
        recent_events = events[-self.sequence_length:]
        
        features = []
        for event in recent_events:
            feature_vector = [
                event.timestamp.hour / 24.0,  # æ™‚åˆ»
                event.timestamp.minute / 60.0,  # åˆ†
                event.timestamp.weekday() / 7.0,  # æ›œæ—¥
                event.confidence,  # ä¿¡é ¼åº¦
                1.0 if event.event_type == 'focused_work' else 0.0,
                1.0 if event.event_type == 'smartphone_use' else 0.0,
                1.0 if event.event_type == 'absence' else 0.0,
                self._calculate_session_duration(recent_events, event),
                self._calculate_transition_frequency(recent_events),
                self._calculate_focus_momentum(recent_events[:recent_events.index(event)+1])
            ]
            features.append(feature_vector)
            
        return np.array(features).reshape(1, self.sequence_length, -1)
        
    async def _predict_probabilities(self, feature_sequence: np.ndarray) -> torch.Tensor:
        """ç¢ºç‡äºˆæ¸¬å®Ÿè¡Œ"""
        self.model.eval()
        
        with torch.no_grad():
            # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
            input_tensor = torch.FloatTensor(feature_sequence).to(self.device)
            
            # äºˆæ¸¬å®Ÿè¡Œ
            probabilities = self.model(input_tensor)
            
        return probabilities.cpu()
        
    def _interpret_predictions(self, predictions: torch.Tensor, last_timestamp: datetime) -> Dict[str, Any]:
        """äºˆæ¸¬çµæœè§£é‡ˆ"""
        probs = predictions.numpy()[0]
        
        # æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„è¡Œå‹•
        predicted_class_idx = np.argmax(probs)
        predicted_class = self.prediction_classes[predicted_class_idx]
        confidence = float(probs[predicted_class_idx])
        
        # æ¬¡ã®äºˆæƒ³æ™‚åˆ»
        next_timestamp = last_timestamp + timedelta(minutes=5)
        
        # ç‰¹åˆ¥ãªäºˆæ¸¬ï¼ˆä¼‘æ†©æ™‚åˆ»ç­‰ï¼‰
        special_predictions = self._generate_special_predictions(probs, last_timestamp)
        
        return {
            'next_behavior': predicted_class,
            'confidence': confidence,
            'probabilities': {
                class_name: float(prob) 
                for class_name, prob in zip(self.prediction_classes, probs)
            },
            'predicted_time': next_timestamp,
            **special_predictions
        }
        
    def _generate_special_predictions(self, probs: np.ndarray, last_timestamp: datetime) -> Dict:
        """ç‰¹åˆ¥ãªäºˆæ¸¬ç”Ÿæˆ"""
        focus_prob = probs[0]  # focused_workç¢ºç‡
        
        # ä¼‘æ†©æ¨å¥¨æ™‚åˆ»äºˆæ¸¬
        if focus_prob < 0.3:  # é›†ä¸­åº¦ä½ä¸‹äºˆæ¸¬
            next_break = last_timestamp + timedelta(minutes=15)
        else:
            next_break = last_timestamp + timedelta(minutes=45)
            
        # æ³¨æ„åŠ›ä½ä¸‹ã‚¢ãƒ©ãƒ¼ãƒˆ
        distraction_risk = probs[1]  # smartphone_useç¢ºç‡
        alert_needed = distraction_risk > 0.6
        
        return {
            'next_break': next_break,
            'break_recommendation': 'short' if focus_prob < 0.4 else 'none',
            'distraction_alert': alert_needed,
            'distraction_risk': float(distraction_risk)
        }
```

== ğŸ“Š å€‹äººåŒ–å­¦ç¿’

=== ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰

**å‹•çš„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’**
```python
# src/services/personalization/user_profile_builder.py
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import json

@dataclass
class UserProfile:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    user_id: str
    optimal_focus_hours: List[int]  # æœ€é©é›†ä¸­æ™‚é–“å¸¯
    average_session_length: float  # å¹³å‡ä½œæ¥­ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·
    break_preferences: Dict[str, float]  # ä¼‘æ†©ãƒ‘ã‚¿ãƒ¼ãƒ³
    distraction_triggers: List[str]  # æ³¨æ„æ•£æ¼«è¦å› 
    productivity_patterns: Dict[str, Any]  # ç”Ÿç”£æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
    adaptation_rate: float  # å­¦ç¿’é©å¿œé€Ÿåº¦
    confidence_level: float  # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¿¡é ¼åº¦
    last_updated: datetime

class UserProfileBuilder:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰å™¨"""
    
    def __init__(self, config_manager):
        self.config_manager = config_manager
        self.user_profiles: Dict[str, UserProfile] = {}
        self.learning_rate = 0.1
        self.min_data_points = 50
        
    async def update_profile(self, user_id: str, events: List[BehaviorEvent], 
                           feedback: Optional[Dict] = None) -> UserProfile:
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°"""
        
        # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã¾ãŸã¯æ–°è¦ä½œæˆ
        profile = self.user_profiles.get(user_id) or self._create_default_profile(user_id)
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿è“„ç©
        learning_data = self._extract_learning_data(events)
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å­¦ç¿’
        updated_profile = await self._learn_from_data(profile, learning_data, feedback)
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        self.user_profiles[user_id] = updated_profile
        await self._save_profile(updated_profile)
        
        return updated_profile
        
    def _create_default_profile(self, user_id: str) -> UserProfile:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        return UserProfile(
            user_id=user_id,
            optimal_focus_hours=[9, 10, 11, 14, 15, 16],  # ä¸€èˆ¬çš„ãªé›†ä¸­æ™‚é–“
            average_session_length=45.0,  # 45åˆ†
            break_preferences={
                'short_break_frequency': 3,  # 3å›ä½œæ¥­å¾Œã«çŸ­ã„ä¼‘æ†©
                'long_break_duration': 15,   # 15åˆ†ã®é•·ã„ä¼‘æ†©
                'preferred_break_activities': []
            },
            distraction_triggers=[],
            productivity_patterns={
                'morning_productivity': 0.7,
                'afternoon_productivity': 0.6,
                'weekly_pattern': [0.6, 0.8, 0.8, 0.7, 0.6, 0.3, 0.3]  # æœˆ-æ—¥
            },
            adaptation_rate=0.1,
            confidence_level=0.1,
            last_updated=datetime.now()
        )
        
    async def _learn_from_data(self, profile: UserProfile, learning_data: Dict, 
                             feedback: Optional[Dict]) -> UserProfile:
        """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’"""
        
        # é›†ä¸­æ™‚é–“å¸¯å­¦ç¿’
        profile.optimal_focus_hours = await self._learn_optimal_hours(
            profile.optimal_focus_hours, learning_data['hourly_focus']
        )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·å­¦ç¿’
        profile.average_session_length = self._learn_session_length(
            profile.average_session_length, learning_data['session_lengths']
        )
        
        # ç”Ÿç”£æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
        profile.productivity_patterns = await self._learn_productivity_patterns(
            profile.productivity_patterns, learning_data['productivity_data']
        )
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é©ç”¨
        if feedback:
            profile = self._apply_feedback(profile, feedback)
            
        # ä¿¡é ¼åº¦æ›´æ–°
        profile.confidence_level = self._update_confidence(profile, learning_data)
        profile.last_updated = datetime.now()
        
        return profile
        
    async def _learn_optimal_hours(self, current_hours: List[int], 
                                 hourly_focus: Dict[int, float]) -> List[int]:
        """æœ€é©æ™‚é–“å¸¯å­¦ç¿’"""
        if not hourly_focus:
            return current_hours
            
        # æ™‚é–“å¸¯åˆ¥é›†ä¸­åº¦ã®çµ±è¨ˆ
        hour_scores = []
        for hour in range(24):
            scores = hourly_focus.get(hour, [])
            avg_score = np.mean(scores) if scores else 0.0
            hour_scores.append((hour, avg_score))
            
        # ä¸Šä½6æ™‚é–“ã‚’é¸æŠ
        sorted_hours = sorted(hour_scores, key=lambda x: x[1], reverse=True)
        new_optimal_hours = [hour for hour, _ in sorted_hours[:6]]
        
        # ç¾åœ¨ã®è¨­å®šã¨æ–°ã—ã„å­¦ç¿’çµæœã‚’ãƒ–ãƒ¬ãƒ³ãƒ‰
        blended_hours = self._blend_hour_preferences(current_hours, new_optimal_hours)
        
        return sorted(blended_hours)
        
    def _blend_hour_preferences(self, current: List[int], learned: List[int]) -> List[int]:
        """æ™‚é–“è¨­å®šã®ãƒ–ãƒ¬ãƒ³ãƒ‰"""
        # é‡ã¿ä»˜ã‘å¹³å‡çš„ãªè€ƒãˆæ–¹ã§ãƒ–ãƒ¬ãƒ³ãƒ‰
        all_hours = set(current + learned)
        
        hour_scores = {}
        for hour in all_hours:
            current_weight = 1.0 if hour in current else 0.0
            learned_weight = 1.0 if hour in learned else 0.0
            
            # å­¦ç¿’çµæœã«é‡ãã‚’ç½®ã
            blended_score = (current_weight * 0.3) + (learned_weight * 0.7)
            hour_scores[hour] = blended_score
            
        # ä¸Šä½6æ™‚é–“ã‚’é¸æŠ
        top_hours = sorted(hour_scores.items(), key=lambda x: x[1], reverse=True)[:6]
        return [hour for hour, _ in top_hours]
        
    def generate_personalized_recommendations(self, user_id: str, 
                                            current_context: Dict) -> List[str]:
        """å€‹äººåŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        profile = self.user_profiles.get(user_id)
        if not profile:
            return self._default_recommendations()
            
        recommendations = []
        current_hour = datetime.now().hour
        
        # æ™‚é–“å¸¯ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if current_hour in profile.optimal_focus_hours:
            recommendations.append("This is your optimal focus time. Consider tackling complex tasks.")
        else:
            recommendations.append("Consider lighter tasks or take a break during this time.")
            
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        current_session_length = current_context.get('session_length', 0)
        if current_session_length > profile.average_session_length * 1.2:
            recommendations.append("You've been working longer than usual. Consider a break.")
            
        # å€‹äººãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        weekday = datetime.now().weekday()
        expected_productivity = profile.productivity_patterns['weekly_pattern'][weekday]
        
        if expected_productivity < 0.5:
            recommendations.append("Today typically shows lower productivity. Set realistic goals.")
        elif expected_productivity > 0.8:
            recommendations.append("Today is typically a high-productivity day. Consider challenging tasks.")
            
        return recommendations
```

== ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã¨æ´å¯Ÿ

=== ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

**åˆ†æçµæœå¯è¦–åŒ–**
```python
# src/services/analysis/insight_generator.py
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import base64
import io

class InsightGenerator:
    """ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.insight_templates = self._load_insight_templates()
        
    async def generate_daily_insights(self, user_id: str, 
                                    analysis_data: Dict) -> Dict[str, Any]:
        """æ—¥æ¬¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""
        
        insights = {
            'summary': await self._generate_summary(analysis_data),
            'focus_analysis': self._analyze_focus_patterns(analysis_data),
            'productivity_insights': self._generate_productivity_insights(analysis_data),
            'recommendations': self._generate_smart_recommendations(analysis_data),
            'visualizations': await self._create_visualizations(analysis_data)
        }
        
        return insights
        
    async def _generate_summary(self, data: Dict) -> Dict[str, Any]:
        """è¦ç´„ç”Ÿæˆ"""
        total_events = len(data.get('events', []))
        focus_events = [e for e in data.get('events', []) if e.event_type == 'focused_work']
        distraction_events = [e for e in data.get('events', []) if e.event_type == 'smartphone_use']
        
        focus_time = len(focus_events) * 5  # 5åˆ†é–“éš”æƒ³å®š
        distraction_time = len(distraction_events) * 5
        
        focus_percentage = (focus_time / (focus_time + distraction_time + 1)) * 100
        
        return {
            'total_monitoring_time': total_events * 5,
            'focus_time_minutes': focus_time,
            'distraction_time_minutes': distraction_time,
            'focus_percentage': round(focus_percentage, 1),
            'productivity_score': self._calculate_productivity_score(data),
            'key_insight': self._generate_key_insight(focus_percentage, data)
        }
        
    def _analyze_focus_patterns(self, data: Dict) -> Dict[str, Any]:
        """é›†ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        events = data.get('events', [])
        if not events:
            return {'patterns': [], 'peak_hours': []}
            
        # æ™‚é–“å¸¯åˆ¥é›†ä¸­åº¦
        hourly_focus = {}
        for event in events:
            hour = event.timestamp.hour
            if hour not in hourly_focus:
                hourly_focus[hour] = []
                
            focus_score = 100 if event.event_type == 'focused_work' else 0
            hourly_focus[hour].append(focus_score)
            
        # å¹³å‡é›†ä¸­åº¦è¨ˆç®—
        hourly_avg = {hour: np.mean(scores) for hour, scores in hourly_focus.items()}
        
        # ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯ç‰¹å®š
        peak_hours = [hour for hour, score in hourly_avg.items() if score > 70]
        
        return {
            'hourly_focus': hourly_avg,
            'peak_hours': sorted(peak_hours),
            'focus_consistency': self._calculate_consistency(hourly_avg),
            'improvement_hours': [hour for hour, score in hourly_avg.items() if score < 30]
        }
        
    async def _create_visualizations(self, data: Dict) -> Dict[str, str]:
        """å¯è¦–åŒ–ä½œæˆ"""
        visualizations = {}
        
        # é›†ä¸­åº¦æ¨ç§»ã‚°ãƒ©ãƒ•
        focus_chart = await self._create_focus_timeline(data)
        visualizations['focus_timeline'] = focus_chart
        
        # æ™‚é–“å¸¯åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        heatmap = await self._create_hourly_heatmap(data)
        visualizations['hourly_heatmap'] = heatmap
        
        # ç”Ÿç”£æ€§ã‚¹ã‚³ã‚¢æ¨ç§»
        productivity_chart = await self._create_productivity_chart(data)
        visualizations['productivity_trend'] = productivity_chart
        
        return visualizations
        
    async def _create_focus_timeline(self, data: Dict) -> str:
        """é›†ä¸­åº¦ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        events = data.get('events', [])
        if not events:
            return ""
            
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        timestamps = [event.timestamp for event in events]
        focus_scores = [
            100 if event.event_type == 'focused_work' 
            else 0 if event.event_type == 'smartphone_use'
            else 50 for event in events
        ]
        
        # ã‚°ãƒ©ãƒ•ä½œæˆ
        plt.figure(figsize=(12, 6))
        plt.plot(timestamps, focus_scores, linewidth=2, alpha=0.8)
        plt.fill_between(timestamps, focus_scores, alpha=0.3)
        
        plt.title('Focus Score Timeline', fontsize=16, fontweight='bold')
        plt.xlabel('Time', fontsize=12)
        plt.ylabel('Focus Score', fontsize=12)
        plt.ylim(0, 100)
        plt.grid(True, alpha=0.3)
        
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', bbox_inches='tight', dpi=150)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.read()).decode()
        plt.close()
        
        return f"data:image/png;base64,{image_base64}"
        
    def _generate_smart_recommendations(self, data: Dict) -> List[Dict[str, str]]:
        """ã‚¹ãƒãƒ¼ãƒˆæ¨å¥¨ç”Ÿæˆ"""
        recommendations = []
        
        # åˆ†æçµæœãƒ™ãƒ¼ã‚¹æ¨å¥¨
        focus_analysis = self._analyze_focus_patterns(data)
        
        # æ”¹å–„æ™‚é–“å¸¯æ¨å¥¨
        if focus_analysis['improvement_hours']:
            recommendations.append({
                'type': 'schedule_optimization',
                'title': 'Schedule Optimization',
                'description': f"Consider lighter tasks during {', '.join(map(str, focus_analysis['improvement_hours']))}:00",
                'priority': 'medium'
            })
            
        # ãƒ”ãƒ¼ã‚¯æ™‚é–“æ´»ç”¨æ¨å¥¨
        if focus_analysis['peak_hours']:
            recommendations.append({
                'type': 'peak_utilization',
                'title': 'Peak Performance',
                'description': f"Your peak focus hours are {', '.join(map(str, focus_analysis['peak_hours']))}:00. Schedule important tasks then.",
                'priority': 'high'
            })
            
        # ä¸€è²«æ€§æ”¹å–„æ¨å¥¨
        consistency = focus_analysis.get('focus_consistency', 0)
        if consistency < 0.6:
            recommendations.append({
                'type': 'consistency_improvement',
                'title': 'Focus Consistency',
                'description': 'Your focus varies significantly throughout the day. Try establishing regular work patterns.',
                'priority': 'medium'
            })
            
        return recommendations
```

== ğŸ¯ ã¾ã¨ã‚

ç›£è¦–ã¡ã‚ƒã‚“ã®è¡Œå‹•åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¯ä»¥ä¸‹ã®å…ˆé€²çš„æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š

=== ä¸»è¦å®Ÿè£…æ©Ÿèƒ½

* âœ… **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ**: æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å³åº§ã«è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
* âœ… **äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**: LSTM ãƒ™ãƒ¼ã‚¹ã®æ¬¡è¡Œå‹•ãƒ»é›†ä¸­åº¦äºˆæ¸¬
* âœ… **å€‹äººåŒ–å­¦ç¿’**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ãƒ»é©å¿œ
* âœ… **ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ**: ãƒ‡ãƒ¼ã‚¿é§†å‹•ã«ã‚ˆã‚‹æ”¹å–„ææ¡ˆ
* âœ… **å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ç›´æ„Ÿçš„ãªåˆ†æçµæœè¡¨ç¤º

=== åˆ†æç²¾åº¦

[cols="2,2,2", options="header"]
|===
|åˆ†æé …ç›® |ç›®æ¨™ç²¾åº¦ |å®Ÿè£…æŠ€è¡“
|é›†ä¸­åº¦åˆ¤å®š |85%+ |é‡ã¿ä»˜ã‘æ™‚ç³»åˆ—åˆ†æ
|è¡Œå‹•äºˆæ¸¬ |80%+ |LSTM + ç‰¹å¾´å·¥å­¦
|ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º |90%+ |DBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
|å€‹äººåŒ–é©å¿œ |ç¶™ç¶šæ”¹å–„ |ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’
|===

=== ä»Šå¾Œã®æ‹¡å¼µ

1. **æ·±å±¤å­¦ç¿’å¼·åŒ–**: Transformer ãƒ¢ãƒ‡ãƒ«å°å…¥
2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æ**: éŸ³å£°ãƒ»è¦–ç·šãƒ‡ãƒ¼ã‚¿çµ±åˆ
3. **é•·æœŸè¡Œå‹•äºˆæ¸¬**: é€±ãƒ»æœˆå˜ä½ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬
4. **ãƒãƒ¼ãƒ åˆ†æ**: çµ„ç¹”ãƒ¬ãƒ™ãƒ«ã§ã®ç”Ÿç”£æ€§åˆ†æ

---

**ğŸ“ Contact**: team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber} 