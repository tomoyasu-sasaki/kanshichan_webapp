= ğŸ§ª ç›£è¦–ã¡ã‚ƒã‚“(KanshiChan) ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ä»•æ§˜æ›¸
:toc: left
:toc-title: ç›®æ¬¡
:toclevels: 4
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:version: 1.0.0
:author: KanshiChan QA Team
:email: qa-team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

== ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

[cols="1,3", options="header"]
|===
|é …ç›® |è©³ç´°
|**ä½œæˆè€…** |KanshiChan QAãƒ»é–‹ç™ºãƒãƒ¼ãƒ 
|**æœ€çµ‚æ›´æ–°æ—¥** |{docdate}
|**å¯¾è±¡èª­è€…** |ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…ã€QAã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€DevOpsã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
|**æ–‡æ›¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³** |{revnumber}
|**ã‚·ã‚¹ãƒ†ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³** |KanshiChan v2.0
|**ãƒ¬ãƒ“ãƒ¥ãƒ¼çŠ¶æ³** |åˆç‰ˆä½œæˆå®Œäº†
|===

[IMPORTANT]
====
ğŸ¯ **ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç›®çš„**

ç›£è¦–ã¡ã‚ƒã‚“ï¼ˆKanshiChanï¼‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®ç·åˆçš„ãªãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã«ã¤ã„ã¦ã€å“è³ªä¿è¨¼ã®æ–¹é‡ã€ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã€è‡ªå‹•åŒ–æˆ¦ç•¥ã€ãŠã‚ˆã³ç¶™ç¶šçš„ãªå“è³ªæ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½“ç³»çš„ã«è§£èª¬ã—ã¾ã™ã€‚

**æƒ³å®šèª­è€…**: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºè€…ã€QAã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€DevOpsã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ
====

== ğŸŒŸ ãƒ†ã‚¹ãƒˆæˆ¦ç•¥æ¦‚è¦

=== ğŸ“– å“è³ªä¿è¨¼ã®åŸºæœ¬æ–¹é‡

ç›£è¦–ã¡ã‚ƒã‚“ã®ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã¯ã€AI/MLã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸå¤šå±¤é˜²å¾¡å‹å“è³ªä¿è¨¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

[mermaid]
....
graph TB
    subgraph "å“è³ªä¿è¨¼ãƒ¬ã‚¤ãƒ¤ãƒ¼"
        A[ã‚³ãƒ¼ãƒ‰å“è³ª<br/>é™çš„è§£æãƒ»å‹ãƒã‚§ãƒƒã‚¯]
        B[å˜ä½“ãƒ†ã‚¹ãƒˆ<br/>é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹å˜ä½]
        C[çµ±åˆãƒ†ã‚¹ãƒˆ<br/>ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“é€£æº]
        D[ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ<br/>ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰]
        E[ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ<br/>è² è·ãƒ»æ€§èƒ½]
        F[ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ<br/>è„†å¼±æ€§æ¤œæŸ»]
    end
    
    subgraph "ç¶™ç¶šçš„å“è³ªç®¡ç†"
        G[CI/CDè‡ªå‹•åŒ–]
        H[å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–]
        I[å›å¸°ãƒ†ã‚¹ãƒˆ]
        J[ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ]
    end
    
    A --> B --> C --> D --> E --> F
    B --> G
    C --> G
    D --> H
    E --> H
    F --> I
    G --> J
    
    classDef qualityLayer fill:#e1f5fe
    classDef continuousLayer fill:#f3e5f5
    
    class A,B,C,D,E,F qualityLayer
    class G,H,I,J continuousLayer
....

=== ğŸ¯ å“è³ªç›®æ¨™

[cols="2,2,2,2", options="header"]
|===
|å“è³ªæŒ‡æ¨™ |ç¾åœ¨å€¤ |ç›®æ¨™å€¤ |æ¸¬å®šæ–¹æ³•
|**ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸** |79.13% |90% |pytest-cov
|**å‹ãƒã‚§ãƒƒã‚¯é©åˆç‡** |95% |100% |mypy
|**è„†å¼±æ€§ä»¶æ•°** |0ä»¶ |0ä»¶ |bandit
|**å¹³å‡å¿œç­”æ™‚é–“** |<100ms |<50ms |è² è·ãƒ†ã‚¹ãƒˆ
|**ã‚·ã‚¹ãƒ†ãƒ å¯ç”¨æ€§** |99.5% |99.9% |ç›£è¦–ãƒ„ãƒ¼ãƒ«
|===

== ğŸ—ï¸ ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰æ§‹é€ 

=== ğŸ“Š éšå±¤åˆ¥ãƒ†ã‚¹ãƒˆé…åˆ†

[mermaid]
....
graph TB
    subgraph "ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰"
        E2E[E2Eãƒ†ã‚¹ãƒˆ<br/>ğŸ” çµ±åˆã‚·ãƒŠãƒªã‚ª<br/>å°‘æ•°ãƒ»é‡è¦æ©Ÿèƒ½ã®ã¿<br/>5%]
        INTEGRATION[çµ±åˆãƒ†ã‚¹ãƒˆ<br/>ğŸ”— ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“é€£æº<br/>APIãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹<br/>25%]
        UNIT[å˜ä½“ãƒ†ã‚¹ãƒˆ<br/>âš¡ å€‹åˆ¥é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹<br/>é«˜é€Ÿãƒ»å¤§é‡ãƒ»è©³ç´°<br/>70%]
    end
    
    E2E --> INTEGRATION
    INTEGRATION --> UNIT
    
    classDef e2e fill:#ffebee
    classDef integration fill:#f3e5f5
    classDef unit fill:#e8f5e8
    
    class E2E e2e
    class INTEGRATION integration
    class UNIT unit
....

=== ğŸ§© ãƒ†ã‚¹ãƒˆç¨®åˆ¥è©³ç´°

==== ğŸ”¬ å˜ä½“ãƒ†ã‚¹ãƒˆ (70%)

**å¯¾è±¡ç¯„å›²**: å€‹åˆ¥é–¢æ•°ã€ã‚¯ãƒ©ã‚¹ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

[source,python]
----
# ä¾‹: AIæœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ
import pytest
import numpy as np
from unittest.mock import Mock, patch
from src.core.ai_optimizer import AIOptimizer

class TestAIOptimizer:
    """AIOptimizerå˜ä½“ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def mock_config(self):
        """ãƒ¢ãƒƒã‚¯è¨­å®š"""
        config = Mock()
        config.get.side_effect = lambda key, default=None: {
            'ai_optimization.enabled': True,
            'ai_optimization.target_fps': 15.0,
            'ai_optimization.skip_threshold': 0.8
        }.get(key, default)
        return config
    
    @pytest.fixture
    def ai_optimizer(self, mock_config):
        """AIOptimizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        return AIOptimizer(mock_config)
    
    def test_initialize_with_valid_config(self, ai_optimizer):
        """æœ‰åŠ¹ãªè¨­å®šã§ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert ai_optimizer.enabled is True
        assert ai_optimizer.target_fps == 15.0
    
    def test_frame_skip_decision_high_load(self, ai_optimizer):
        """é«˜è² è·æ™‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š"""
        # é«˜è² è·çŠ¶æ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        ai_optimizer.current_fps = 8.0
        ai_optimizer.cpu_usage = 0.9
        
        should_skip = ai_optimizer.should_skip_frame()
        assert should_skip is True
    
    def test_yolo_optimization_with_caching(self, ai_optimizer):
        """YOLOæœ€é©åŒ–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½"""
        mock_model = Mock()
        mock_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
        result1 = ai_optimizer.optimize_yolo_inference(mock_model, mock_frame)
        
        # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰
        result2 = ai_optimizer.optimize_yolo_inference(mock_model, mock_frame)
        
        assert result1 is not None
        assert result2 is not None
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç¢ºèª
        assert ai_optimizer.cache_hit_count > 0
----

==== ğŸ”— çµ±åˆãƒ†ã‚¹ãƒˆ (25%)

**å¯¾è±¡ç¯„å›²**: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“é€£æºã€APIã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹

[source,python]
----
class TestAPIIntegration:
    """APIçµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def client(self):
        """ãƒ†ã‚¹ãƒˆç”¨Flaskã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
        app = create_app(testing=True)
        with app.test_client() as client:
            yield client
    
    def test_analysis_api_workflow(self, client):
        """åˆ†æAPIã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # 1. åŸºæœ¬åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = client.get('/api/analysis/basic?hours=1')
        assert response.status_code == 200
        data = response.get_json()
        assert 'status' in data
        assert 'analysis_result' in data
        
        # 2. é«˜åº¦åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = client.get('/api/analysis/advanced-patterns?timeframe=daily')
        assert response.status_code == 200
        advanced_data = response.get_json()
        assert 'timeseries_analysis' in advanced_data['data']
        
        # 3. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
        assert advanced_data['data']['total_logs'] >= 0
    
    def test_websocket_integration(self, client):
        """WebSocketçµ±åˆãƒ†ã‚¹ãƒˆ"""
        with patch('socketio.SocketIO') as mock_socketio:
            # WebSocketæ¥ç¶šã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            mock_socketio.emit.return_value = True
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿é…ä¿¡ãƒ†ã‚¹ãƒˆ
            response = client.post('/api/stream/start')
            assert response.status_code == 200
            
            # WebSocket emissionç¢ºèª
            mock_socketio.emit.assert_called()
----

==== ğŸŒ E2Eãƒ†ã‚¹ãƒˆ (5%)

**å¯¾è±¡ç¯„å›²**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼å…¨ä½“ã€é‡è¦æ¥­å‹™ã‚·ãƒŠãƒªã‚ª

[source,python]
----
class TestCompleteUserJourney:
    """å®Œå…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‹ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    
    def test_monitoring_session_lifecycle(self):
        """ç›£è¦–ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œå…¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«"""
        # 1. ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
        session_id = self._start_monitoring_session()
        assert session_id is not None
        
        # 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º
        detection_results = self._simulate_detection_sequence()
        assert len(detection_results) > 0
        
        # 3. è¡Œå‹•åˆ†æå®Ÿè¡Œ
        analysis_results = self._perform_behavior_analysis(session_id)
        assert analysis_results['overall_score'] > 0
        
        # 4. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        recommendations = self._generate_recommendations(analysis_results)
        assert len(recommendations) > 0
        
        # 5. ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†
        session_summary = self._end_monitoring_session(session_id)
        assert session_summary['session_completed'] is True
----

== ğŸ¤– AI/MLç‰¹åŒ–ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

=== ğŸ¯ AI/MLã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç‰¹åˆ¥ãªè€ƒæ…®äº‹é …

AI/MLã‚·ã‚¹ãƒ†ãƒ ã¯éæ±ºå®šçš„ãªå‹•ä½œãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€å°‚ç”¨ã®ãƒ†ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚

[mermaid]
....
graph LR
    subgraph "AI/MLãƒ†ã‚¹ãƒˆé ˜åŸŸ"
        A[ãƒ¢ãƒ‡ãƒ«å“è³ªãƒ†ã‚¹ãƒˆ<br/>ç²¾åº¦ãƒ»å†ç¾æ€§]
        B[ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ<br/>å‰å‡¦ç†ãƒ»æ¤œè¨¼]
        C[æ¨è«–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ<br/>ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ]
        D[ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ<br/>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“]
    end
    
    subgraph "ãƒ†ã‚¹ãƒˆæ‰‹æ³•"
        E[çµ±è¨ˆçš„ãƒ†ã‚¹ãƒˆ]
        F[å›å¸°ãƒ†ã‚¹ãƒˆ]
        G[ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ]
        H[ã‚«ãƒŠãƒªã‚¢ãƒ†ã‚¹ãƒˆ]
    end
    
    A --> E
    B --> F
    C --> G
    D --> H
....

=== ğŸ” YOLOv8æ¤œå‡ºå™¨ãƒ†ã‚¹ãƒˆ

[source,python]
----
class TestYOLODetection:
    """YOLOv8æ¤œå‡ºå™¨å°‚ç”¨ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def test_images(self):
        """ãƒ†ã‚¹ãƒˆç”¨ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
        return {
            'person_present': np.load('tests/fixtures/person_frame.npy'),
            'smartphone_present': np.load('tests/fixtures/smartphone_frame.npy'),
            'empty_scene': np.load('tests/fixtures/empty_frame.npy'),
            'low_quality': np.random.randint(0, 50, (240, 320, 3), dtype=np.uint8)
        }
    
    def test_person_detection_accuracy(self, detector, test_images):
        """äººç‰©æ¤œå‡ºç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        # äººç‰©ãŒå­˜åœ¨ã™ã‚‹ç”»åƒã§ã®æ¤œå‡º
        result = detector.detect_objects(test_images['person_present'])
        
        # æœŸå¾…ã•ã‚Œã‚‹æ¤œå‡ºçµæœ
        assert any(obj['class'] == 'person' for obj in result['detections'])
        assert result['detections'][0]['confidence'] > 0.5
    
    def test_detection_consistency(self, detector, test_images):
        """æ¤œå‡ºä¸€è²«æ€§ãƒ†ã‚¹ãƒˆï¼ˆåŒä¸€ç”»åƒã®è¤‡æ•°å›æ¤œå‡ºï¼‰"""
        image = test_images['person_present']
        results = []
        
        # åŒä¸€ç”»åƒã‚’10å›æ¤œå‡º
        for _ in range(10):
            result = detector.detect_objects(image)
            results.append(len(result['detections']))
        
        # çµæœã®ä¸€è²«æ€§ç¢ºèªï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ã“ã¨ï¼‰
        import statistics
        assert statistics.stdev(results) < 1.0
    
    def test_edge_case_handling(self, detector, test_images):
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        # ä½å“è³ªç”»åƒ
        result = detector.detect_objects(test_images['low_quality'])
        assert 'error' not in result  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ã“ã¨
        
        # ç©ºã®ç”»åƒ
        empty_frame = np.zeros((100, 100, 3), dtype=np.uint8)
        result = detector.detect_objects(empty_frame)
        assert result['detections'] == []
----

=== ğŸ“Š MediaPipeå§¿å‹¢æ¤œå‡ºãƒ†ã‚¹ãƒˆ

[source,python]
----
class TestMediaPipePose:
    """MediaPipeå§¿å‹¢æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    
    def test_pose_landmark_extraction(self, pose_detector, test_images):
        """å§¿å‹¢ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æŠ½å‡ºãƒ†ã‚¹ãƒˆ"""
        result = pose_detector.detect_pose(test_images['person_present'])
        
        # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ãŒæ­£ã—ãæŠ½å‡ºã•ã‚Œã‚‹ã“ã¨
        assert 'landmarks' in result
        assert len(result['landmarks']) == 33  # MediaPipeã®æ¨™æº–ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ•°
        
        # ä¿¡é ¼åº¦ãƒã‚§ãƒƒã‚¯
        assert all(lm['visibility'] >= 0 for lm in result['landmarks'])
    
    def test_focus_score_calculation(self, pose_detector, test_images):
        """é›†ä¸­åº¦ã‚¹ã‚³ã‚¢ç®—å‡ºãƒ†ã‚¹ãƒˆ"""
        # æ­£é¢å‘ãç”»åƒã§ã®é›†ä¸­åº¦
        frontal_result = pose_detector.calculate_focus_score(test_images['person_present'])
        
        # æ¨ªå‘ãç”»åƒã§ã®é›†ä¸­åº¦
        side_result = pose_detector.calculate_focus_score(test_images['side_view'])
        
        # æ­£é¢å‘ãã®æ–¹ãŒé«˜ã„é›†ä¸­åº¦ã‚’ç¤ºã™ã“ã¨
        assert frontal_result > side_result
        assert 0.0 <= frontal_result <= 1.0
        assert 0.0 <= side_result <= 1.0
----

== ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

=== âš¡ æ€§èƒ½è¦ä»¶å®šç¾©

[cols="2,2,2,2", options="header"]
|===
|é …ç›® |ç›®æ¨™å€¤ |è¨±å®¹å€¤ |æ¸¬å®šæ¡ä»¶
|**APIå¿œç­”æ™‚é–“** |<50ms |<100ms |é€šå¸¸è² è·
|**æ¤œå‡ºå‡¦ç†æ™‚é–“** |<66ms |<100ms |640x480ç”»åƒ
|**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** |<2GB |<4GB |é€£ç¶š24æ™‚é–“ç¨¼åƒ
|**CPUä½¿ç”¨ç‡** |<70% |<90% |ãƒ”ãƒ¼ã‚¯æ™‚è² è·
|===

=== ğŸ“ˆ è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª

[source,python]
----
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class TestPerformance:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    async def test_concurrent_api_requests(self):
        """åŒæ™‚APIãƒªã‚¯ã‚¨ã‚¹ãƒˆè² è·ãƒ†ã‚¹ãƒˆ"""
        base_url = "http://localhost:8000"
        concurrent_users = 50
        requests_per_user = 10
        
        async def make_request(session, user_id):
            """å€‹åˆ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
            start_time = time.time()
            async with session.get(f"{base_url}/api/analysis/basic") as response:
                await response.json()
                response_time = time.time() - start_time
                return response_time
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for user_id in range(concurrent_users):
                for _ in range(requests_per_user):
                    task = make_request(session, user_id)
                    tasks.append(task)
            
            # å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆåŒæ™‚å®Ÿè¡Œ
            response_times = await asyncio.gather(*tasks)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        avg_response_time = sum(response_times) / len(response_times)
        max_response_time = max(response_times)
        
        assert avg_response_time < 0.1  # 100msæœªæº€
        assert max_response_time < 0.5  # 500msæœªæº€
        
        print(f"Average response time: {avg_response_time:.3f}s")
        print(f"Max response time: {max_response_time:.3f}s")
    
    def test_memory_leak_detection(self):
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        # å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        for i in range(1000):
            # æ¤œå‡ºå‡¦ç†ã®å®Ÿè¡Œ
            self._simulate_detection_processing()
            
            # å®šæœŸçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            if i % 100 == 0:
                gc.collect()
                current_memory = process.memory_info().rss
                memory_growth = current_memory - initial_memory
                
                # ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡ãŒè¨±å®¹ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨
                assert memory_growth < 100 * 1024 * 1024  # 100MBæœªæº€
    
    def test_ai_processing_performance(self):
        """AIå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        from src.core.object_detector import ObjectDetector
        
        detector = ObjectDetector()
        test_frame = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)
        
        # é€£ç¶šå‡¦ç†æ™‚é–“æ¸¬å®š
        processing_times = []
        for _ in range(100):
            start_time = time.time()
            detector.detect_objects(test_frame)
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
        
        avg_time = sum(processing_times) / len(processing_times)
        assert avg_time < 0.066  # 15FPSç›¸å½“ï¼ˆ66msæœªæº€ï¼‰
----

== ğŸ”§ ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ãƒ»CI/CDçµ±åˆ

=== ğŸš€ GitHub Actionsè¨­å®š

[source,yaml]
----
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install -r backend/requirements-test.txt
    
    - name: Lint with flake8
      run: |
        flake8 backend/src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 backend/src --count --exit-zero --max-complexity=10 --max-line-length=88
    
    - name: Type check with mypy
      run: |
        mypy backend/src --strict
    
    - name: Security check with bandit
      run: |
        bandit -r backend/src -f json -o bandit-report.json
    
    - name: Test with pytest
      run: |
        cd backend
        pytest tests/ \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --junitxml=pytest-report.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
    
    - name: Performance regression test
      run: |
        cd backend
        pytest tests/test_performance.py --benchmark-only
----

=== ğŸ“Š å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š

[source,python]
----
# quality_gate.py - å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
import json
import sys
import subprocess

def check_quality_gates():
    """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
    quality_checks = {
        'test_coverage': check_test_coverage(),
        'type_check': check_type_coverage(),
        'security_scan': check_security_issues(),
        'performance': check_performance_regression()
    }
    
    failed_checks = []
    for check_name, passed in quality_checks.items():
        if not passed:
            failed_checks.append(check_name)
    
    if failed_checks:
        print(f"âŒ Quality gate failed: {', '.join(failed_checks)}")
        sys.exit(1)
    else:
        print("âœ… All quality gates passed")

def check_test_coverage():
    """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
    try:
        result = subprocess.run(['pytest', '--cov=src', '--cov-report=json'], 
                              capture_output=True, text=True)
        with open('coverage.json') as f:
            coverage_data = json.load(f)
            coverage_percent = coverage_data['totals']['percent_covered']
            return coverage_percent >= 80.0
    except:
        return False

def check_type_coverage():
    """å‹ãƒã‚§ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸"""
    try:
        result = subprocess.run(['mypy', 'src/', '--strict'], 
                              capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

if __name__ == "__main__":
    check_quality_gates()
----

== ğŸ” ãƒ¢ãƒƒã‚­ãƒ³ã‚°æˆ¦ç•¥

=== ğŸ­ å¤–éƒ¨ä¾å­˜ãƒ¢ãƒƒã‚­ãƒ³ã‚°

[source,python]
----
# conftest.py - å…±é€šãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
import pytest
import numpy as np
from unittest.mock import Mock, patch, MagicMock

@pytest.fixture
def mock_yolo_model():
    """YOLOãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ãƒƒã‚¯"""
    mock_model = Mock()
    mock_model.predict.return_value = [
        Mock(boxes=Mock(data=np.array([[100, 100, 200, 200, 0.9, 0]])))
    ]
    return mock_model

@pytest.fixture
def mock_mediapipe():
    """MediaPipeã®ãƒ¢ãƒƒã‚¯"""
    with patch('mediapipe.solutions.pose.Pose') as mock_pose:
        mock_pose_instance = Mock()
        mock_pose_instance.process.return_value = Mock(
            pose_landmarks=Mock(landmark=[
                Mock(x=0.5, y=0.3, z=0.1, visibility=0.9)
            ])
        )
        mock_pose.return_value = mock_pose_instance
        yield mock_pose_instance

@pytest.fixture
def mock_config_manager():
    """ConfigManagerã®ãƒ¢ãƒƒã‚¯"""
    mock_config = Mock()
    mock_config.get.side_effect = lambda key, default=None: {
        'yolo.model_path': '/mock/path/yolo.pt',
        'yolo.confidence_threshold': 0.5,
        'mediapipe.min_detection_confidence': 0.5,
        'ai_optimization.enabled': True
    }.get(key, default)
    return mock_config

@pytest.fixture
def mock_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒƒã‚¯"""
    with patch('models.db.session') as mock_session:
        mock_session.add = Mock()
        mock_session.commit = Mock()
        mock_session.query = Mock()
        yield mock_session
----

=== ğŸª æ™‚é–“ãƒ»ç’°å¢ƒãƒ¢ãƒƒã‚­ãƒ³ã‚°

[source,python]
----
import pytest
from unittest.mock import patch
from freezegun import freeze_time
from datetime import datetime

class TestTimeDependent:
    """æ™‚é–“ä¾å­˜ãƒ†ã‚¹ãƒˆ"""
    
    @freeze_time("2024-01-15 10:00:00")
    def test_daily_analysis_with_fixed_time(self):
        """å›ºå®šæ™‚é–“ã§ã®æ—¥æ¬¡åˆ†æãƒ†ã‚¹ãƒˆ"""
        from src.services.analysis.behavior_analyzer import BehaviorAnalyzer
        
        analyzer = BehaviorAnalyzer(config)
        
        # å›ºå®šæ™‚é–“ã§ã®åˆ†æå®Ÿè¡Œ
        result = analyzer.analyze_daily_patterns()
        
        # æœŸå¾…ã•ã‚Œã‚‹æ™‚é–“ç¯„å›²ã§ã®çµæœç¢ºèª
        assert result['analysis_date'] == "2024-01-15"
        assert result['analysis_hour'] == 10

@pytest.fixture
def mock_environment():
    """ç’°å¢ƒå¤‰æ•°ãƒ¢ãƒƒã‚­ãƒ³ã‚°"""
    with patch.dict('os.environ', {
        'KANSHICHAN_ENV': 'test',
        'DATABASE_URL': 'sqlite:///:memory:',
        'REDIS_URL': 'redis://localhost:6379/1'
    }):
        yield
----

== ğŸ“ˆ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ç›£è¦–

=== ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

[source,python]
----
# test_metrics.py - ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
import json
import subprocess
import time
from typing import Dict, Any

class TestMetricsCollector:
    """ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å™¨"""
    
    def collect_all_metrics(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        return {
            'coverage': self.get_coverage_metrics(),
            'performance': self.get_performance_metrics(),
            'quality': self.get_quality_metrics(),
            'timestamp': time.time()
        }
    
    def get_coverage_metrics(self) -> Dict[str, float]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        try:
            result = subprocess.run(
                ['pytest', '--cov=src', '--cov-report=json', '--quiet'],
                capture_output=True, text=True, check=True
            )
            
            with open('coverage.json') as f:
                coverage_data = json.load(f)
                
            return {
                'line_coverage': coverage_data['totals']['percent_covered'],
                'branch_coverage': coverage_data['totals'].get('percent_covered_display', 0),
                'missing_lines': coverage_data['totals']['missing_lines']
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        try:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                ['pytest', 'tests/test_performance.py', '--benchmark-json=benchmark.json'],
                capture_output=True, text=True
            )
            
            with open('benchmark.json') as f:
                benchmark_data = json.load(f)
            
            return {
                'avg_detection_time': self._extract_benchmark_stat(benchmark_data, 'test_detection_speed'),
                'avg_analysis_time': self._extract_benchmark_stat(benchmark_data, 'test_analysis_speed'),
                'memory_usage_peak': self._get_memory_peak()
            }
        except Exception as e:
            return {'error': str(e)}
----

=== ğŸ“‹ å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

[source,python]
----
# quality_report.py - å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
import jinja2
from datetime import datetime

class QualityReportGenerator:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def generate_html_report(self, metrics: Dict[str, Any]) -> str:
        """HTMLå“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        template = jinja2.Template("""
        <!DOCTYPE html>
        <html>
        <head>
            <title>KanshiChan Quality Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric { margin: 10px 0; padding: 10px; border-left: 4px solid #007acc; }
                .pass { border-color: #28a745; }
                .fail { border-color: #dc3545; }
                .warning { border-color: #ffc107; }
            </style>
        </head>
        <body>
            <h1>KanshiChan Quality Report</h1>
            <p>Generated: {{ timestamp }}</p>
            
            <h2>Test Coverage</h2>
            <div class="metric {{ 'pass' if metrics.coverage.line_coverage >= 80 else 'fail' }}">
                <strong>Line Coverage:</strong> {{ "%.1f"|format(metrics.coverage.line_coverage) }}%
            </div>
            
            <h2>Performance Metrics</h2>
            <div class="metric {{ 'pass' if metrics.performance.avg_detection_time < 0.1 else 'warning' }}">
                <strong>Detection Speed:</strong> {{ "%.3f"|format(metrics.performance.avg_detection_time) }}s
            </div>
            
            <h2>Quality Gates</h2>
            {% for gate, status in quality_gates.items() %}
            <div class="metric {{ 'pass' if status else 'fail' }}">
                <strong>{{ gate }}:</strong> {{ 'PASS' if status else 'FAIL' }}
            </div>
            {% endfor %}
        </body>
        </html>
        """)
        
        quality_gates = {
            'Test Coverage >= 80%': metrics['coverage']['line_coverage'] >= 80,
            'Type Check Clean': metrics['quality']['type_check_errors'] == 0,
            'Security Issues = 0': metrics['quality']['security_issues'] == 0,
            'Performance Targets Met': metrics['performance']['avg_detection_time'] < 0.1
        }
        
        return template.render(
            metrics=metrics,
            quality_gates=quality_gates,
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
----

== ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ

=== ğŸ›¡ï¸ è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³

[source,python]
----
class TestSecurity:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    
    def test_api_authentication(self, client):
        """APIèªè¨¼ãƒ†ã‚¹ãƒˆ"""
        # èªè¨¼ãªã—ã§ã®ã‚¢ã‚¯ã‚»ã‚¹
        response = client.get('/api/admin/config')
        assert response.status_code == 401
        
        # ç„¡åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã§ã®ã‚¢ã‚¯ã‚»ã‚¹
        headers = {'Authorization': 'Bearer invalid_token'}
        response = client.get('/api/admin/config', headers=headers)
        assert response.status_code == 403
    
    def test_input_validation(self, client):
        """å…¥åŠ›å€¤æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è©¦è¡Œ
        malicious_input = "'; DROP TABLE behavior_logs; --"
        response = client.post('/api/analysis/search', 
                              json={'query': malicious_input})
        assert response.status_code == 400
        
        # XSSè©¦è¡Œ
        xss_input = "<script>alert('xss')</script>"
        response = client.post('/api/analysis/comment',
                              json={'comment': xss_input})
        assert response.status_code == 400
    
    def test_data_privacy(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # å€‹äººè­˜åˆ¥æƒ…å ±ã®é©åˆ‡ãªåŒ¿ååŒ–ç¢ºèª
        from src.services.privacy.anonymizer import DataAnonymizer
        
        anonymizer = DataAnonymizer()
        sensitive_data = {
            'face_landmarks': [[0.1, 0.2], [0.3, 0.4]],
            'user_id': 'user_12345',
            'session_id': 'session_abcdef'
        }
        
        anonymized = anonymizer.anonymize_behavior_data(sensitive_data)
        
        # å…ƒã®IDãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨
        assert 'user_12345' not in str(anonymized)
        assert 'session_abcdef' not in str(anonymized)
        
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ä¿æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨
        assert 'face_landmarks' in anonymized
        assert len(anonymized['face_landmarks']) == 2
----

== ğŸ“š ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

=== â— ã‚ˆãã‚ã‚‹ãƒ†ã‚¹ãƒˆå•é¡Œ

[cols="2,3,3", options="header"]
|===
|å•é¡Œ |åŸå›  |è§£æ±ºç­–
|**ãƒ†ã‚¹ãƒˆãŒé–“æ¬ çš„ã«å¤±æ•—** |éæ±ºå®šçš„ãªAIå‡¦ç† |é–¾å€¤ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã€çµ±è¨ˆçš„æ¤œè¨¼
|**ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼** |å¤§é‡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ |ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ã€ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
|**ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“éé•·** |é‡ã„AIå‡¦ç† |ãƒ¢ãƒƒã‚¯æ´»ç”¨ã€è»½é‡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
|**ã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³** |è¤‡é›‘ãªæ¡ä»¶åˆ†å² |ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆè¿½åŠ 
|===

=== ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

[source,python]
----
# test_debug.py - ãƒ†ã‚¹ãƒˆãƒ‡ãƒãƒƒã‚°ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
import pytest
import logging

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
logging.basicConfig(level=logging.DEBUG)

def pytest_runtest_protocol(item, nextitem):
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ãƒ•ãƒƒã‚¯"""
    print(f"\nğŸ§ª Running: {item.name}")
    return None

@pytest.fixture
def debug_mode():
    """ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–"""
    import os
    os.environ['KANSHICHAN_DEBUG'] = '1'
    yield
    os.environ.pop('KANSHICHAN_DEBUG', None)

# ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã®è©³ç´°æƒ…å ±å‡ºåŠ›
def pytest_runtest_makereport(item, call):
    """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    if call.when == "call" and call.excinfo is not None:
        print(f"\nâŒ Test failed: {item.name}")
        print(f"Error: {call.excinfo.value}")
        
        # AI/MLãƒ†ã‚¹ãƒˆã®å ´åˆã¯è¿½åŠ æƒ…å ±å‡ºåŠ›
        if 'ai' in item.name.lower() or 'ml' in item.name.lower():
            print("ğŸ¤– AI/ML Test Failure - Check model state and data quality")
----

== ğŸ“– å‚è€ƒè³‡æ–™

=== ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

* <<development-guide.adoc#,é–‹ç™ºã‚¬ã‚¤ãƒ‰>>
* <<backend-architecture.adoc#,ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£>>
* <<ai-ml-specifications.adoc#,AI/MLæŠ€è¡“ä»•æ§˜>>
* <<configuration-guide.adoc#,è¨­å®šã‚¬ã‚¤ãƒ‰>>

=== ğŸ“š æŠ€è¡“å‚è€ƒæ–‡çŒ®

* pytestå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://docs.pytest.org/
* unittest.mock: https://docs.python.org/3/library/unittest.mock.html
* Property-based testing: https://hypothesis.readthedocs.io/
* AI/MLãƒ†ã‚¹ãƒˆæ‰‹æ³•: https://madewithml.com/courses/mlops/testing/

=== ğŸ¤ é–‹ç™ºãƒãƒ¼ãƒ ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ

[cols="2,3", options="header"]
|===
|å½¹å‰² |é€£çµ¡å…ˆ
|**QAãƒªãƒ¼ãƒ‰** |qa-lead@kanshichan.dev
|**ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢** |test-automation@kanshichan.dev
|**æŠ€è¡“ã‚µãƒãƒ¼ãƒˆ** |tech-support@kanshichan.dev
|===

---

**ğŸ“ Contact**: qa-team@kanshichan.dev +
**ğŸ”— Repository**: https://github.com/kanshichan/backend +
**ğŸ“… Last Updated**: {docdate} +
**ğŸ“ Document Version**: {revnumber} 