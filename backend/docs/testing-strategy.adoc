= 🧪 監視ちゃん(KanshiChan) テスト戦略仕様書
:toc: left
:toc-title: 目次
:toclevels: 4
:numbered:
:source-highlighter: highlight.js
:icons: font
:doctype: book
:version: 1.0.0
:author: KanshiChan QA Team
:email: qa-team@kanshichan.dev
:revnumber: 1.0
:revdate: {docdate}
:experimental:

== 📋 ドキュメント情報

[cols="1,3", options="header"]
|===
|項目 |詳細
|**作成者** |KanshiChan QA・開発チーム
|**最終更新日** |{docdate}
|**対象読者** |バックエンド開発者、QAエンジニア、DevOpsエンジニア
|**文書バージョン** |{revnumber}
|**システムバージョン** |KanshiChan v2.0
|**レビュー状況** |初版作成完了
|===

[IMPORTANT]
====
🎯 **このドキュメントの目的**

監視ちゃん（KanshiChan）バックエンドシステムの総合的なテスト戦略について、品質保証の方針、テスト手法、自動化戦略、および継続的な品質改善プロセスを体系的に解説します。

**想定読者**: バックエンド開発者、QAエンジニア、DevOpsエンジニア、システムアーキテクト
====

== 🌟 テスト戦略概要

=== 📖 品質保証の基本方針

監視ちゃんのテスト戦略は、AI/MLシステムの特性を考慮した多層防御型品質保証アプローチを採用しています。

[mermaid]
....
graph TB
    subgraph "品質保証レイヤー"
        A[コード品質<br/>静的解析・型チェック]
        B[単体テスト<br/>関数・クラス単位]
        C[統合テスト<br/>コンポーネント間連携]
        D[システムテスト<br/>エンドツーエンド]
        E[パフォーマンステスト<br/>負荷・性能]
        F[セキュリティテスト<br/>脆弱性検査]
    end
    
    subgraph "継続的品質管理"
        G[CI/CD自動化]
        H[品質メトリクス監視]
        I[回帰テスト]
        J[リスクベーステスト]
    end
    
    A --> B --> C --> D --> E --> F
    B --> G
    C --> G
    D --> H
    E --> H
    F --> I
    G --> J
    
    classDef qualityLayer fill:#e1f5fe
    classDef continuousLayer fill:#f3e5f5
    
    class A,B,C,D,E,F qualityLayer
    class G,H,I,J continuousLayer
....

=== 🎯 品質目標

[cols="2,2,2,2", options="header"]
|===
|品質指標 |現在値 |目標値 |測定方法
|**テストカバレッジ** |79.13% |90% |pytest-cov
|**型チェック適合率** |95% |100% |mypy
|**脆弱性件数** |0件 |0件 |bandit
|**平均応答時間** |<100ms |<50ms |負荷テスト
|**システム可用性** |99.5% |99.9% |監視ツール
|===

== 🏗️ テストピラミッド構造

=== 📊 階層別テスト配分

[mermaid]
....
graph TB
    subgraph "テストピラミッド"
        E2E[E2Eテスト<br/>🔍 統合シナリオ<br/>少数・重要機能のみ<br/>5%]
        INTEGRATION[統合テスト<br/>🔗 コンポーネント間連携<br/>API・データベース・外部サービス<br/>25%]
        UNIT[単体テスト<br/>⚡ 個別関数・クラス<br/>高速・大量・詳細<br/>70%]
    end
    
    E2E --> INTEGRATION
    INTEGRATION --> UNIT
    
    classDef e2e fill:#ffebee
    classDef integration fill:#f3e5f5
    classDef unit fill:#e8f5e8
    
    class E2E e2e
    class INTEGRATION integration
    class UNIT unit
....

=== 🧩 テスト種別詳細

==== 🔬 単体テスト (70%)

**対象範囲**: 個別関数、クラス、モジュール

[source,python]
----
# 例: AI最適化コンポーネントのテスト
import pytest
import numpy as np
from unittest.mock import Mock, patch
from src.core.ai_optimizer import AIOptimizer

class TestAIOptimizer:
    """AIOptimizer単体テスト"""
    
    @pytest.fixture
    def mock_config(self):
        """モック設定"""
        config = Mock()
        config.get.side_effect = lambda key, default=None: {
            'ai_optimization.enabled': True,
            'ai_optimization.target_fps': 15.0,
            'ai_optimization.skip_threshold': 0.8
        }.get(key, default)
        return config
    
    @pytest.fixture
    def ai_optimizer(self, mock_config):
        """AIOptimizerインスタンス"""
        return AIOptimizer(mock_config)
    
    def test_initialize_with_valid_config(self, ai_optimizer):
        """有効な設定での初期化テスト"""
        assert ai_optimizer.enabled is True
        assert ai_optimizer.target_fps == 15.0
    
    def test_frame_skip_decision_high_load(self, ai_optimizer):
        """高負荷時のフレームスキップ判定"""
        # 高負荷状況をシミュレート
        ai_optimizer.current_fps = 8.0
        ai_optimizer.cpu_usage = 0.9
        
        should_skip = ai_optimizer.should_skip_frame()
        assert should_skip is True
    
    def test_yolo_optimization_with_caching(self, ai_optimizer):
        """YOLO最適化とキャッシュ機能"""
        mock_model = Mock()
        mock_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # 初回実行（キャッシュなし）
        result1 = ai_optimizer.optimize_yolo_inference(mock_model, mock_frame)
        
        # 2回目実行（キャッシュ利用）
        result2 = ai_optimizer.optimize_yolo_inference(mock_model, mock_frame)
        
        assert result1 is not None
        assert result2 is not None
        # キャッシュヒット確認
        assert ai_optimizer.cache_hit_count > 0
----

==== 🔗 統合テスト (25%)

**対象範囲**: コンポーネント間連携、API、データベース

[source,python]
----
class TestAPIIntegration:
    """API統合テスト"""
    
    @pytest.fixture
    def client(self):
        """テスト用Flaskクライアント"""
        app = create_app(testing=True)
        with app.test_client() as client:
            yield client
    
    def test_analysis_api_workflow(self, client):
        """分析APIのワークフローテスト"""
        # 1. 基本分析リクエスト
        response = client.get('/api/analysis/basic?hours=1')
        assert response.status_code == 200
        data = response.get_json()
        assert 'status' in data
        assert 'analysis_result' in data
        
        # 2. 高度分析リクエスト
        response = client.get('/api/analysis/advanced-patterns?timeframe=daily')
        assert response.status_code == 200
        advanced_data = response.get_json()
        assert 'timeseries_analysis' in advanced_data['data']
        
        # 3. データ整合性確認
        assert advanced_data['data']['total_logs'] >= 0
    
    def test_websocket_integration(self, client):
        """WebSocket統合テスト"""
        with patch('socketio.SocketIO') as mock_socketio:
            # WebSocket接続シミュレート
            mock_socketio.emit.return_value = True
            
            # リアルタイムデータ配信テスト
            response = client.post('/api/stream/start')
            assert response.status_code == 200
            
            # WebSocket emission確認
            mock_socketio.emit.assert_called()
----

==== 🌐 E2Eテスト (5%)

**対象範囲**: ユーザージャーニー全体、重要業務シナリオ

[source,python]
----
class TestCompleteUserJourney:
    """完全ユーザージャーニーテスト"""
    
    def test_monitoring_session_lifecycle(self):
        """監視セッション完全ライフサイクル"""
        # 1. セッション開始
        session_id = self._start_monitoring_session()
        assert session_id is not None
        
        # 2. リアルタイム検出
        detection_results = self._simulate_detection_sequence()
        assert len(detection_results) > 0
        
        # 3. 行動分析実行
        analysis_results = self._perform_behavior_analysis(session_id)
        assert analysis_results['overall_score'] > 0
        
        # 4. 推奨アクション生成
        recommendations = self._generate_recommendations(analysis_results)
        assert len(recommendations) > 0
        
        # 5. セッション終了
        session_summary = self._end_monitoring_session(session_id)
        assert session_summary['session_completed'] is True
----

== 🤖 AI/ML特化テスト戦略

=== 🎯 AI/MLコンポーネントの特別な考慮事項

AI/MLシステムは非決定的な動作が含まれるため、専用のテストアプローチが必要です。

[mermaid]
....
graph LR
    subgraph "AI/MLテスト領域"
        A[モデル品質テスト<br/>精度・再現性]
        B[データ品質テスト<br/>前処理・検証]
        C[推論パフォーマンステスト<br/>レイテンシ・スループット]
        D[モデル統合テスト<br/>パイプライン全体]
    end
    
    subgraph "テスト手法"
        E[統計的テスト]
        F[回帰テスト]
        G[プロパティベーステスト]
        H[カナリアテスト]
    end
    
    A --> E
    B --> F
    C --> G
    D --> H
....

=== 🔍 YOLOv8検出器テスト

[source,python]
----
class TestYOLODetection:
    """YOLOv8検出器専用テスト"""
    
    @pytest.fixture
    def test_images(self):
        """テスト用画像データセット"""
        return {
            'person_present': np.load('tests/fixtures/person_frame.npy'),
            'smartphone_present': np.load('tests/fixtures/smartphone_frame.npy'),
            'empty_scene': np.load('tests/fixtures/empty_frame.npy'),
            'low_quality': np.random.randint(0, 50, (240, 320, 3), dtype=np.uint8)
        }
    
    def test_person_detection_accuracy(self, detector, test_images):
        """人物検出精度テスト"""
        # 人物が存在する画像での検出
        result = detector.detect_objects(test_images['person_present'])
        
        # 期待される検出結果
        assert any(obj['class'] == 'person' for obj in result['detections'])
        assert result['detections'][0]['confidence'] > 0.5
    
    def test_detection_consistency(self, detector, test_images):
        """検出一貫性テスト（同一画像の複数回検出）"""
        image = test_images['person_present']
        results = []
        
        # 同一画像を10回検出
        for _ in range(10):
            result = detector.detect_objects(image)
            results.append(len(result['detections']))
        
        # 結果の一貫性確認（標準偏差が小さいこと）
        import statistics
        assert statistics.stdev(results) < 1.0
    
    def test_edge_case_handling(self, detector, test_images):
        """エッジケース処理テスト"""
        # 低品質画像
        result = detector.detect_objects(test_images['low_quality'])
        assert 'error' not in result  # エラーが発生しないこと
        
        # 空の画像
        empty_frame = np.zeros((100, 100, 3), dtype=np.uint8)
        result = detector.detect_objects(empty_frame)
        assert result['detections'] == []
----

=== 📊 MediaPipe姿勢検出テスト

[source,python]
----
class TestMediaPipePose:
    """MediaPipe姿勢検出テスト"""
    
    def test_pose_landmark_extraction(self, pose_detector, test_images):
        """姿勢ランドマーク抽出テスト"""
        result = pose_detector.detect_pose(test_images['person_present'])
        
        # ランドマークが正しく抽出されること
        assert 'landmarks' in result
        assert len(result['landmarks']) == 33  # MediaPipeの標準ランドマーク数
        
        # 信頼度チェック
        assert all(lm['visibility'] >= 0 for lm in result['landmarks'])
    
    def test_focus_score_calculation(self, pose_detector, test_images):
        """集中度スコア算出テスト"""
        # 正面向き画像での集中度
        frontal_result = pose_detector.calculate_focus_score(test_images['person_present'])
        
        # 横向き画像での集中度
        side_result = pose_detector.calculate_focus_score(test_images['side_view'])
        
        # 正面向きの方が高い集中度を示すこと
        assert frontal_result > side_result
        assert 0.0 <= frontal_result <= 1.0
        assert 0.0 <= side_result <= 1.0
----

== 🚀 パフォーマンステスト戦略

=== ⚡ 性能要件定義

[cols="2,2,2,2", options="header"]
|===
|項目 |目標値 |許容値 |測定条件
|**API応答時間** |<50ms |<100ms |通常負荷
|**検出処理時間** |<66ms |<100ms |640x480画像
|**メモリ使用量** |<2GB |<4GB |連続24時間稼働
|**CPU使用率** |<70% |<90% |ピーク時負荷
|===

=== 📈 負荷テストシナリオ

[source,python]
----
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class TestPerformance:
    """パフォーマンステスト"""
    
    async def test_concurrent_api_requests(self):
        """同時APIリクエスト負荷テスト"""
        base_url = "http://localhost:8000"
        concurrent_users = 50
        requests_per_user = 10
        
        async def make_request(session, user_id):
            """個別リクエスト実行"""
            start_time = time.time()
            async with session.get(f"{base_url}/api/analysis/basic") as response:
                await response.json()
                response_time = time.time() - start_time
                return response_time
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for user_id in range(concurrent_users):
                for _ in range(requests_per_user):
                    task = make_request(session, user_id)
                    tasks.append(task)
            
            # 全リクエスト同時実行
            response_times = await asyncio.gather(*tasks)
        
        # パフォーマンス評価
        avg_response_time = sum(response_times) / len(response_times)
        max_response_time = max(response_times)
        
        assert avg_response_time < 0.1  # 100ms未満
        assert max_response_time < 0.5  # 500ms未満
        
        print(f"Average response time: {avg_response_time:.3f}s")
        print(f"Max response time: {max_response_time:.3f}s")
    
    def test_memory_leak_detection(self):
        """メモリリーク検出テスト"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        # 大量のデータ処理をシミュレート
        for i in range(1000):
            # 検出処理の実行
            self._simulate_detection_processing()
            
            # 定期的なガベージコレクション
            if i % 100 == 0:
                gc.collect()
                current_memory = process.memory_info().rss
                memory_growth = current_memory - initial_memory
                
                # メモリ増加量が許容範囲内であること
                assert memory_growth < 100 * 1024 * 1024  # 100MB未満
    
    def test_ai_processing_performance(self):
        """AI処理パフォーマンステスト"""
        from src.core.object_detector import ObjectDetector
        
        detector = ObjectDetector()
        test_frame = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)
        
        # 連続処理時間測定
        processing_times = []
        for _ in range(100):
            start_time = time.time()
            detector.detect_objects(test_frame)
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
        
        avg_time = sum(processing_times) / len(processing_times)
        assert avg_time < 0.066  # 15FPS相当（66ms未満）
----

== 🔧 テスト自動化・CI/CD統合

=== 🚀 GitHub Actions設定

[source,yaml]
----
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install -r backend/requirements-test.txt
    
    - name: Lint with flake8
      run: |
        flake8 backend/src --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 backend/src --count --exit-zero --max-complexity=10 --max-line-length=88
    
    - name: Type check with mypy
      run: |
        mypy backend/src --strict
    
    - name: Security check with bandit
      run: |
        bandit -r backend/src -f json -o bandit-report.json
    
    - name: Test with pytest
      run: |
        cd backend
        pytest tests/ \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --junitxml=pytest-report.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
    
    - name: Performance regression test
      run: |
        cd backend
        pytest tests/test_performance.py --benchmark-only
----

=== 📊 品質ゲート設定

[source,python]
----
# quality_gate.py - 品質ゲートチェック
import json
import sys
import subprocess

def check_quality_gates():
    """品質ゲートチェック実行"""
    quality_checks = {
        'test_coverage': check_test_coverage(),
        'type_check': check_type_coverage(),
        'security_scan': check_security_issues(),
        'performance': check_performance_regression()
    }
    
    failed_checks = []
    for check_name, passed in quality_checks.items():
        if not passed:
            failed_checks.append(check_name)
    
    if failed_checks:
        print(f"❌ Quality gate failed: {', '.join(failed_checks)}")
        sys.exit(1)
    else:
        print("✅ All quality gates passed")

def check_test_coverage():
    """テストカバレッジチェック"""
    try:
        result = subprocess.run(['pytest', '--cov=src', '--cov-report=json'], 
                              capture_output=True, text=True)
        with open('coverage.json') as f:
            coverage_data = json.load(f)
            coverage_percent = coverage_data['totals']['percent_covered']
            return coverage_percent >= 80.0
    except:
        return False

def check_type_coverage():
    """型チェックカバレッジ"""
    try:
        result = subprocess.run(['mypy', 'src/', '--strict'], 
                              capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

if __name__ == "__main__":
    check_quality_gates()
----

== 🔍 モッキング戦略

=== 🎭 外部依存モッキング

[source,python]
----
# conftest.py - 共通テストフィクスチャ
import pytest
import numpy as np
from unittest.mock import Mock, patch, MagicMock

@pytest.fixture
def mock_yolo_model():
    """YOLOモデルのモック"""
    mock_model = Mock()
    mock_model.predict.return_value = [
        Mock(boxes=Mock(data=np.array([[100, 100, 200, 200, 0.9, 0]])))
    ]
    return mock_model

@pytest.fixture
def mock_mediapipe():
    """MediaPipeのモック"""
    with patch('mediapipe.solutions.pose.Pose') as mock_pose:
        mock_pose_instance = Mock()
        mock_pose_instance.process.return_value = Mock(
            pose_landmarks=Mock(landmark=[
                Mock(x=0.5, y=0.3, z=0.1, visibility=0.9)
            ])
        )
        mock_pose.return_value = mock_pose_instance
        yield mock_pose_instance

@pytest.fixture
def mock_config_manager():
    """ConfigManagerのモック"""
    mock_config = Mock()
    mock_config.get.side_effect = lambda key, default=None: {
        'yolo.model_path': '/mock/path/yolo.pt',
        'yolo.confidence_threshold': 0.5,
        'mediapipe.min_detection_confidence': 0.5,
        'ai_optimization.enabled': True
    }.get(key, default)
    return mock_config

@pytest.fixture
def mock_database():
    """データベースのモック"""
    with patch('models.db.session') as mock_session:
        mock_session.add = Mock()
        mock_session.commit = Mock()
        mock_session.query = Mock()
        yield mock_session
----

=== 🎪 時間・環境モッキング

[source,python]
----
import pytest
from unittest.mock import patch
from freezegun import freeze_time
from datetime import datetime

class TestTimeDependent:
    """時間依存テスト"""
    
    @freeze_time("2024-01-15 10:00:00")
    def test_daily_analysis_with_fixed_time(self):
        """固定時間での日次分析テスト"""
        from src.services.analysis.behavior_analyzer import BehaviorAnalyzer
        
        analyzer = BehaviorAnalyzer(config)
        
        # 固定時間での分析実行
        result = analyzer.analyze_daily_patterns()
        
        # 期待される時間範囲での結果確認
        assert result['analysis_date'] == "2024-01-15"
        assert result['analysis_hour'] == 10

@pytest.fixture
def mock_environment():
    """環境変数モッキング"""
    with patch.dict('os.environ', {
        'KANSHICHAN_ENV': 'test',
        'DATABASE_URL': 'sqlite:///:memory:',
        'REDIS_URL': 'redis://localhost:6379/1'
    }):
        yield
----

== 📈 品質メトリクス・監視

=== 📊 テストメトリクス収集

[source,python]
----
# test_metrics.py - テストメトリクス収集
import json
import subprocess
import time
from typing import Dict, Any

class TestMetricsCollector:
    """テストメトリクス収集器"""
    
    def collect_all_metrics(self) -> Dict[str, Any]:
        """全テストメトリクスを収集"""
        return {
            'coverage': self.get_coverage_metrics(),
            'performance': self.get_performance_metrics(),
            'quality': self.get_quality_metrics(),
            'timestamp': time.time()
        }
    
    def get_coverage_metrics(self) -> Dict[str, float]:
        """カバレッジメトリクス取得"""
        try:
            result = subprocess.run(
                ['pytest', '--cov=src', '--cov-report=json', '--quiet'],
                capture_output=True, text=True, check=True
            )
            
            with open('coverage.json') as f:
                coverage_data = json.load(f)
                
            return {
                'line_coverage': coverage_data['totals']['percent_covered'],
                'branch_coverage': coverage_data['totals'].get('percent_covered_display', 0),
                'missing_lines': coverage_data['totals']['missing_lines']
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """パフォーマンスメトリクス取得"""
        try:
            # ベンチマークテスト実行
            result = subprocess.run(
                ['pytest', 'tests/test_performance.py', '--benchmark-json=benchmark.json'],
                capture_output=True, text=True
            )
            
            with open('benchmark.json') as f:
                benchmark_data = json.load(f)
            
            return {
                'avg_detection_time': self._extract_benchmark_stat(benchmark_data, 'test_detection_speed'),
                'avg_analysis_time': self._extract_benchmark_stat(benchmark_data, 'test_analysis_speed'),
                'memory_usage_peak': self._get_memory_peak()
            }
        except Exception as e:
            return {'error': str(e)}
----

=== 📋 品質レポート生成

[source,python]
----
# quality_report.py - 品質レポート生成
import jinja2
from datetime import datetime

class QualityReportGenerator:
    """品質レポート生成器"""
    
    def generate_html_report(self, metrics: Dict[str, Any]) -> str:
        """HTML品質レポート生成"""
        template = jinja2.Template("""
        <!DOCTYPE html>
        <html>
        <head>
            <title>KanshiChan Quality Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .metric { margin: 10px 0; padding: 10px; border-left: 4px solid #007acc; }
                .pass { border-color: #28a745; }
                .fail { border-color: #dc3545; }
                .warning { border-color: #ffc107; }
            </style>
        </head>
        <body>
            <h1>KanshiChan Quality Report</h1>
            <p>Generated: {{ timestamp }}</p>
            
            <h2>Test Coverage</h2>
            <div class="metric {{ 'pass' if metrics.coverage.line_coverage >= 80 else 'fail' }}">
                <strong>Line Coverage:</strong> {{ "%.1f"|format(metrics.coverage.line_coverage) }}%
            </div>
            
            <h2>Performance Metrics</h2>
            <div class="metric {{ 'pass' if metrics.performance.avg_detection_time < 0.1 else 'warning' }}">
                <strong>Detection Speed:</strong> {{ "%.3f"|format(metrics.performance.avg_detection_time) }}s
            </div>
            
            <h2>Quality Gates</h2>
            {% for gate, status in quality_gates.items() %}
            <div class="metric {{ 'pass' if status else 'fail' }}">
                <strong>{{ gate }}:</strong> {{ 'PASS' if status else 'FAIL' }}
            </div>
            {% endfor %}
        </body>
        </html>
        """)
        
        quality_gates = {
            'Test Coverage >= 80%': metrics['coverage']['line_coverage'] >= 80,
            'Type Check Clean': metrics['quality']['type_check_errors'] == 0,
            'Security Issues = 0': metrics['quality']['security_issues'] == 0,
            'Performance Targets Met': metrics['performance']['avg_detection_time'] < 0.1
        }
        
        return template.render(
            metrics=metrics,
            quality_gates=quality_gates,
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
----

== 🔒 セキュリティテスト

=== 🛡️ 脆弱性スキャン

[source,python]
----
class TestSecurity:
    """セキュリティテスト"""
    
    def test_api_authentication(self, client):
        """API認証テスト"""
        # 認証なしでのアクセス
        response = client.get('/api/admin/config')
        assert response.status_code == 401
        
        # 無効なトークンでのアクセス
        headers = {'Authorization': 'Bearer invalid_token'}
        response = client.get('/api/admin/config', headers=headers)
        assert response.status_code == 403
    
    def test_input_validation(self, client):
        """入力値検証テスト"""
        # SQLインジェクション試行
        malicious_input = "'; DROP TABLE behavior_logs; --"
        response = client.post('/api/analysis/search', 
                              json={'query': malicious_input})
        assert response.status_code == 400
        
        # XSS試行
        xss_input = "<script>alert('xss')</script>"
        response = client.post('/api/analysis/comment',
                              json={'comment': xss_input})
        assert response.status_code == 400
    
    def test_data_privacy(self):
        """データプライバシーテスト"""
        # 個人識別情報の適切な匿名化確認
        from src.services.privacy.anonymizer import DataAnonymizer
        
        anonymizer = DataAnonymizer()
        sensitive_data = {
            'face_landmarks': [[0.1, 0.2], [0.3, 0.4]],
            'user_id': 'user_12345',
            'session_id': 'session_abcdef'
        }
        
        anonymized = anonymizer.anonymize_behavior_data(sensitive_data)
        
        # 元のIDが含まれていないこと
        assert 'user_12345' not in str(anonymized)
        assert 'session_abcdef' not in str(anonymized)
        
        # データ構造は保持されていること
        assert 'face_landmarks' in anonymized
        assert len(anonymized['face_landmarks']) == 2
----

== 📚 トラブルシューティング

=== ❗ よくあるテスト問題

[cols="2,3,3", options="header"]
|===
|問題 |原因 |解決策
|**テストが間欠的に失敗** |非決定的なAI処理 |閾値ベーステスト、統計的検証
|**メモリ不足エラー** |大量テストデータ |バッチサイズ調整、リソース管理
|**テスト実行時間過長** |重いAI処理 |モック活用、軽量テストデータ
|**カバレッジ不足** |複雑な条件分岐 |エッジケーステスト追加
|===

=== 🔧 デバッグテクニック

[source,python]
----
# test_debug.py - テストデバッグ用ユーティリティ
import pytest
import logging

# テスト実行時の詳細ログ出力
logging.basicConfig(level=logging.DEBUG)

def pytest_runtest_protocol(item, nextitem):
    """テスト実行時フック"""
    print(f"\n🧪 Running: {item.name}")
    return None

@pytest.fixture
def debug_mode():
    """デバッグモード有効化"""
    import os
    os.environ['KANSHICHAN_DEBUG'] = '1'
    yield
    os.environ.pop('KANSHICHAN_DEBUG', None)

# テスト失敗時の詳細情報出力
def pytest_runtest_makereport(item, call):
    """テスト結果レポート生成"""
    if call.when == "call" and call.excinfo is not None:
        print(f"\n❌ Test failed: {item.name}")
        print(f"Error: {call.excinfo.value}")
        
        # AI/MLテストの場合は追加情報出力
        if 'ai' in item.name.lower() or 'ml' in item.name.lower():
            print("🤖 AI/ML Test Failure - Check model state and data quality")
----

== 📖 参考資料

=== 🔗 関連ドキュメント

* <<development-guide.adoc#,開発ガイド>>
* <<backend-architecture.adoc#,システムアーキテクチャ>>
* <<ai-ml-specifications.adoc#,AI/ML技術仕様>>
* <<configuration-guide.adoc#,設定ガイド>>

=== 📚 技術参考文献

* pytest公式ドキュメント: https://docs.pytest.org/
* unittest.mock: https://docs.python.org/3/library/unittest.mock.html
* Property-based testing: https://hypothesis.readthedocs.io/
* AI/MLテスト手法: https://madewithml.com/courses/mlops/testing/

=== 🤝 開発チームコンタクト

[cols="2,3", options="header"]
|===
|役割 |連絡先
|**QAリード** |qa-lead@kanshichan.dev
|**テスト自動化エンジニア** |test-automation@kanshichan.dev
|**技術サポート** |tech-support@kanshichan.dev
|===

---

**📞 Contact**: qa-team@kanshichan.dev +
**🔗 Repository**: https://github.com/kanshichan/backend +
**📅 Last Updated**: {docdate} +
**📝 Document Version**: {revnumber} 