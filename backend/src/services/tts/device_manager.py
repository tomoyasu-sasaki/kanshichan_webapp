"""
Device Manager - ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã¨MPSæœ€é©åŒ–

PyTorchãƒ‡ãƒã‚¤ã‚¹é¸æŠã€MPSæœ€é©åŒ–ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’ç®¡ç†
"""

import os
import torch
from typing import Dict, Any, Optional
from utils.logger import setup_logger
from utils.exceptions import ServiceUnavailableError, wrap_exception

logger = setup_logger(__name__)


class DeviceManager:
    """ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹
    
    PyTorchãƒ‡ãƒã‚¤ã‚¹é¸æŠã€MPSæœ€é©åŒ–ã€GPU/CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
    """
    
    def __init__(self, tts_config: 'TTSConfig'):
        """ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†åˆæœŸåŒ–
        
        Args:
            tts_config: TTSè¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.tts_config = tts_config
        self.device = self._get_optimal_device()
        self.original_device = self.device
        
        logger.info(f"Device initialized: {self.device}")
    
    def _get_optimal_device(self) -> str:
        """æœ€é©ãªå®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ã‚’æ±ºå®š
        
        Returns:
            str: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ ('cuda', 'mps', 'cpu')
        """
        if torch.cuda.is_available():
            logger.info("ğŸš€ CUDA GPU detected")
            return 'cuda'
        
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            if self.tts_config.enable_mps:
                logger.info("ğŸ Apple Silicon MPS detected - attempting acceleration")
                return 'mps'
            else:
                logger.info("MPS available but disabled by configuration, using CPU")
                return 'cpu'
        else:
            logger.info("Using CPU device")
            return 'cpu'
    
    def configure_device_optimizations(self) -> None:
        """ãƒ‡ãƒã‚¤ã‚¹å›ºæœ‰ã®æœ€é©åŒ–è¨­å®š"""
        if self.device == 'mps':
            self._configure_mps_optimizations()
        elif self.device == 'cuda':
            self._configure_cuda_optimizations()
        else:
            self._configure_cpu_optimizations()
    
    def _configure_mps_optimizations(self) -> None:
        """MPSæœ€é©åŒ–è¨­å®š"""
        try:
            # MPSç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
            os.environ['MPS_GRAPH_CACHE_DEPTH'] = '5'
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.75'
            os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.70'
            os.environ['PYTORCH_MPS_PREFER_CPU_FALLBACK'] = '1'
            os.environ['PYTORCH_ENABLE_MPS_CPU_FALLBACK'] = '1'
            
            # ãƒ‡ãƒãƒƒã‚°è¨­å®š
            if self.tts_config.debug_mps:
                os.environ['PYTORCH_VERBOSE'] = '1'
                os.environ['PYTORCH_MPS_LOG_LEVEL'] = '1'
            
            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                torch.mps.set_per_process_memory_fraction(self.tts_config.mps_memory_fraction)
                logger.info(f"ğŸ§  MPS memory fraction: {self.tts_config.mps_memory_fraction}")
            
            # æœ€é©åŒ–ãƒ•ãƒ©ã‚°
            if hasattr(torch.backends.mps, 'is_available'):
                torch.backends.mps.enable_all_optimizations = True
            
            logger.info("ğŸ MPS optimizations configured with CPU fallback")
            
        except Exception as e:
            logger.warning(f"MPS optimization failed: {e}")
    
    def _configure_cuda_optimizations(self) -> None:
        """CUDAæœ€é©åŒ–è¨­å®š"""
        try:
            # cuDNNæœ€é©åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            if self.tts_config.gpu_memory_optimization:
                torch.cuda.empty_cache()
            
            logger.info("ğŸš€ CUDA optimizations configured")
            
        except Exception as e:
            logger.warning(f"CUDA optimization failed: {e}")
    
    def _configure_cpu_optimizations(self) -> None:
        """CPUæœ€é©åŒ–è¨­å®š"""
        try:
            # CPUæœ€é©åŒ–
            torch.set_num_threads(self.tts_config.max_worker_threads)
            
            # æ¨è«–æœ€é©åŒ–
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            
            logger.info("ğŸ’» CPU optimizations configured")
            
        except Exception as e:
            logger.warning(f"CPU optimization failed: {e}")
    
    def optimize_model_for_device(self, model: Any) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ç”¨ã«æœ€é©åŒ–
        
        Args:
            model: æœ€é©åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
            
        Returns:
            Any: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        """
        try:
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            model = model.to(self.device)
            
            # ãƒ‡ãƒã‚¤ã‚¹å›ºæœ‰ã®æœ€é©åŒ–
            if self.device == 'mps':
                model = self._optimize_model_for_mps(model)
            elif self.device == 'cuda':
                model = self._optimize_model_for_cuda(model)
            else:
                model = self._optimize_model_for_cpu(model)
            
            # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.eval()
            for param in model.parameters():
                param.requires_grad = False
            
            logger.info(f"Model optimized for {self.device.upper()}")
            return model
            
        except Exception as e:
            logger.error(f"Model optimization failed: {e}")
            return model
    
    def _optimize_model_for_mps(self, model: Any) -> Any:
        """MPSç”¨ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
        try:
            # åŠç²¾åº¦è¨­å®š
            if self.tts_config.mps_half_precision and hasattr(model, 'half'):
                try:
                    model = model.half()
                    logger.info("ğŸ”¢ MPS half precision enabled")
                except Exception as half_error:
                    logger.warning(f"MPS half precision failed: {half_error}")
            
            # æ¨è«–æœ€é©åŒ–
            try:
                model = torch.jit.optimize_for_inference(model)
                logger.info("ğŸ”§ MPS inference optimization applied")
            except Exception as opt_error:
                logger.debug(f"MPS inference optimization skipped: {opt_error}")
            
            return model
            
        except Exception as e:
            logger.warning(f"MPS model optimization failed: {e}")
            return model
    
    def _optimize_model_for_cuda(self, model: Any) -> Any:
        """CUDAç”¨ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
        try:
            # GPUæœ€é©åŒ–
            if hasattr(model, 'half'):
                try:
                    model = model.half()
                    logger.info("ğŸ”¢ CUDA half precision enabled")
                except Exception:
                    logger.debug("CUDA half precision not supported")
            
            return model
            
        except Exception as e:
            logger.warning(f"CUDA model optimization failed: {e}")
            return model
    
    def _optimize_model_for_cpu(self, model: Any) -> Any:
        """CPUç”¨ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
        try:
            # CPUæ¨è«–æœ€é©åŒ–
            logger.info("ğŸ’» CPU model optimization applied")
            return model
            
        except Exception as e:
            logger.warning(f"CPU model optimization failed: {e}")
            return model
    
    def handle_device_error(self, error: Exception, model: Any = None) -> tuple[str, Any]:
        """ãƒ‡ãƒã‚¤ã‚¹ã‚¨ãƒ©ãƒ¼ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        Args:
            error: ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼
            model: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            tuple[str, Any]: (æ–°ã—ã„ãƒ‡ãƒã‚¤ã‚¹, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾Œã®ãƒ¢ãƒ‡ãƒ«)
        """
        error_str = str(error).lower()
        
        # MPS ã‚¨ãƒ©ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.device == 'mps' and ('scatter_reduce' in error_str or 'aten::' in error_str or 'mps' in error_str):
            logger.warning(f"ğŸ”§ MPS error detected, falling back to CPU: {error}")
            
            # ç‰¹å®šã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã®è©³ç´°ãƒ­ã‚°
            if 'scatter_reduce' in error_str:
                logger.info("ğŸ“ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç®‡æ‰€: Zonos sampling.py ã® repetition penalty å‡¦ç†ä¸­")
                logger.info("ğŸ“ å…·ä½“çš„æ“ä½œ: torch.scatter_reduce() ãŒMPSã§æœªå®Ÿè£…")
                logger.info("ğŸ“ ç™ºç”Ÿã‚¿ã‚¤ãƒŸãƒ³ã‚°: model.generate() å®Ÿè¡Œä¸­ã®éŸ³å£°ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå‡¦ç†")
            
            # å®Œå…¨ãªCPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            fallback_model = self._perform_complete_cpu_fallback(model)
            
            return self.device, fallback_model
        
        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿ
        raise error
    
    def _perform_complete_cpu_fallback(self, model: Any = None) -> Any:
        """å®Œå…¨ãªCPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        
        MPSã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ç¢ºå®Ÿã«CPUã«ç§»è¡Œ
        
        Args:
            model: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
            
        Returns:
            Any: CPUç§»è¡Œå¾Œã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå‚ç…§æ›´æ–°ã®ãŸã‚ï¼‰
        """
        try:
            logger.info("ğŸ”„ Starting complete CPU fallback process...")
            
            # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šå¤‰æ›´
            self.device = 'cpu'
            self._configure_cpu_optimizations()
            
            # ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨CPUç§»è¡Œ
            if model is not None:
                # 1. ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’CPUã«ç§»å‹•
                logger.debug("Moving main model to CPU...")
                model = model.to('cpu')
                
                # 2. Autoencoderéƒ¨åˆ†ã®å®‰å…¨ãªç§»è¡Œ
                if hasattr(model, 'autoencoder') and model.autoencoder is not None:
                    logger.debug("Processing autoencoder components...")
                    
                    # autoencoderã®to()ãƒ¡ã‚½ãƒƒãƒ‰æœ‰ç„¡ã‚’ç¢ºèª
                    if hasattr(model.autoencoder, 'to'):
                        logger.debug("Moving autoencoder to CPU...")
                        model.autoencoder = model.autoencoder.to('cpu')
                    
                    # 3. DACéƒ¨åˆ†ã®å®‰å…¨ãªç§»è¡Œ
                    if hasattr(model.autoencoder, 'dac') and model.autoencoder.dac is not None:
                        logger.debug("Processing DAC components...")
                        
                        # DACã®to()ãƒ¡ã‚½ãƒƒãƒ‰æœ‰ç„¡ã‚’ç¢ºèª
                        if hasattr(model.autoencoder.dac, 'to'):
                            logger.debug("Moving DAC to CPU...")
                            model.autoencoder.dac = model.autoencoder.dac.to('cpu')
                        
                        # 4. Quantizeréƒ¨åˆ†ã®ç§»è¡Œ
                        if hasattr(model.autoencoder.dac, 'quantizer'):
                            logger.debug("Processing quantizer...")
                            if hasattr(model.autoencoder.dac.quantizer, 'to'):
                                logger.debug("Moving quantizer to CPU...")
                                model.autoencoder.dac.quantizer = model.autoencoder.dac.quantizer.to('cpu')
                            
                            # å€‹åˆ¥ã®quantizeréƒ¨å“ã‚’ç§»è¡Œ
                            if hasattr(model.autoencoder.dac.quantizer, 'quantizers'):
                                logger.debug("Moving individual quantizers...")
                                for i, quantizer in enumerate(model.autoencoder.dac.quantizer.quantizers):
                                    if hasattr(quantizer, 'to'):
                                        model.autoencoder.dac.quantizer.quantizers[i] = quantizer.to('cpu')
                        
                        # 5. Encoder/Decoderéƒ¨åˆ†ã®ç§»è¡Œ
                        if hasattr(model.autoencoder.dac, 'encoder') and hasattr(model.autoencoder.dac.encoder, 'to'):
                            logger.debug("Moving DAC encoder to CPU...")
                            model.autoencoder.dac.encoder = model.autoencoder.dac.encoder.to('cpu')
                        
                        if hasattr(model.autoencoder.dac, 'decoder') and hasattr(model.autoencoder.dac.decoder, 'to'):
                            logger.debug("Moving DAC decoder to CPU...")
                            model.autoencoder.dac.decoder = model.autoencoder.dac.decoder.to('cpu')
                
                # 6. CPUæœ€é©åŒ–ã‚’é©ç”¨
                model = self._optimize_model_for_cpu(model)
                
                # 7. MPSé–¢é€£ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                
                # 8. CPUå°‚ç”¨è¨­å®šã‚’é©ç”¨
                if hasattr(model, 'eval'):
                    model.eval()
                
                # 9. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèªã¨ãƒ‡ãƒã‚¤ã‚¹ç§»è¡Œ
                logger.debug("Verifying all parameters are on CPU...")
                try:
                    for name, param in model.named_parameters():
                        if param.device.type != 'cpu':
                            logger.debug(f"Moving parameter {name} to CPU...")
                            param.data = param.data.cpu()
                except Exception as param_error:
                    logger.warning(f"Parameter verification failed: {param_error}")
                
                logger.info("âœ… Complete CPU fallback successful")
                
            else:
                logger.warning("No model provided for CPU fallback")
                
            return model
                
        except Exception as fallback_error:
            logger.error(f"âŒ Complete CPU fallback failed: {fallback_error}")
            logger.error(f"Fallback error details: {type(fallback_error).__name__}: {str(fallback_error)}")
            
            # æœ€å¾Œã®æ‰‹æ®µ: ãƒ¢ãƒ‡ãƒ«å†åˆæœŸåŒ–ã‚’æ¨å¥¨
            logger.info("ğŸ’¡ Recommendation: Reinitialize model directly on CPU")
            raise ServiceUnavailableError(f"Device fallback failed - model reinitialiation required: {fallback_error}")
    
    def ensure_tensor_device_consistency(self, tensor: torch.Tensor, model: Any = None) -> torch.Tensor:
        """ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ä¸€è²«æ€§ã‚’ç¢ºä¿
        
        Args:
            tensor: å‡¦ç†ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«
            model: å‚ç…§ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            torch.Tensor: ãƒ‡ãƒã‚¤ã‚¹ä¸€è²«æ€§ãŒç¢ºä¿ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
            if model is not None:
                model_device = next(model.parameters()).device
            else:
                model_device = torch.device(self.device)
            
            # ãƒ‡ãƒã‚¤ã‚¹ãŒç•°ãªã‚‹å ´åˆã¯ç§»å‹•
            if tensor.device != model_device:
                logger.debug(f"Moving tensor from {tensor.device} to {model_device}")
                
                if model_device.type == 'mps':
                    try:
                        tensor = tensor.to(model_device)
                        # MPSãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        if hasattr(torch.mps, 'empty_cache'):
                            torch.mps.empty_cache()
                    except Exception as mps_error:
                        logger.warning(f"MPS tensor move failed: {mps_error}, using CPU")
                        tensor = tensor.to('cpu')
                        self.device = 'cpu'
                else:
                    tensor = tensor.to(model_device)
            
            return tensor
            
        except Exception as e:
            logger.warning(f"Tensor device consistency failed: {e}")
            return tensor
    
    def cleanup_device_memory(self) -> None:
        """ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒ¢ãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.device == 'cuda' and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.debug("CUDA memory cleaned")
            
            elif self.device == 'mps' and hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
                logger.debug("MPS memory cleaned")
                
        except Exception as e:
            logger.warning(f"Device memory cleanup failed: {e}")
    
    def get_device_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict[str, Any]: ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
        """
        device_info = {
            'current_device': self.device,
            'original_device': self.original_device,
            'cuda_available': torch.cuda.is_available(),
            'mps_available': hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),
            'mps_enabled_in_config': self.tts_config.enable_mps,
            'mps_memory_fraction': self.tts_config.mps_memory_fraction,
            'mps_half_precision': self.tts_config.mps_half_precision
        }
        
        # MPSå›ºæœ‰æƒ…å ±
        if self.device == 'mps':
            device_info.update({
                'mps_optimizations_active': True,
                'mps_fallback_enabled': os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK') == '1',
                'mps_cache_depth': os.environ.get('MPS_GRAPH_CACHE_DEPTH', 'not_set')
            })
        
        # CUDAå›ºæœ‰æƒ…å ±
        if self.device == 'cuda' and torch.cuda.is_available():
            device_info.update({
                'cuda_device_count': torch.cuda.device_count(),
                'cuda_current_device': torch.cuda.current_device(),
                'cuda_memory_allocated': torch.cuda.memory_allocated() / (1024**3),
                'cuda_memory_reserved': torch.cuda.memory_reserved() / (1024**3)
            })
        
        return device_info 